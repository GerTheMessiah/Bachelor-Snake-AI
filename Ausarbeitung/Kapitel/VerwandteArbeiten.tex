\chapter{Verwandte Arbeiten}
In diesem Kapitel soll es thematisch über den momentanen Stand der bereits durchgeführten Forschung gehen. Dabei sollen drei ausgewählte Arbeiten vorgestellt und diskutiert werden.

\section{Autonomous Agents in Snake Game via Deep Reinforcement Learning}
In der Arbeit Autonomous Agents in Snake Game via Deep Reinforcement Learning wurden mehrere Optimierungen an einem DQN Agenten durchgeführt, um mit diesen eine größere Performance beim Sammeln von Äpfeln im Spiel Snake zu erzielen. Die Arbeitet wurde von Zhepei Wei et al. an verschiedenen Universitäten und Forschungsinstituten, wie z.B. College of Computer Science and Technology Jilin University (Changchun, China), Science and Engineering Nanyang Technological University (Singapore) verfasst und im Jahr 2018 veröffentlicht.

\subsection{Vorstellung}
Thematisch werden zwei Optimierungen in diesem Paper vorgestellt, welche auf einen Baseline DQN (Referenz DQN) angewendet worden sind.
\begin{itemize}
	\item optimierte Reward-Funktion
	\item Dual Experience Replay Method 
\end{itemize} 
In der optimierten Rewardfunktion kommen drei Faktoren zum tragen. Der erste Faktor ist die Distanz. Um so weiter sich die Snake von dem Apfel entfernt, um so größer (negativ) wird der Reward. Nährt sich hingegen die Snake dem Apfen an, so wird der Reward immer größer (positiv).\\
Der zweite Faktor der Reward-Funktion ist der sogenannte Training Gap. Dieser stellt Erfahrungen dar, welche im Verlauf des Spiels nicht erlernt werden soll. Diese Erfahrungen, welche direkt nach dem Essen eines Apfels generiert wurden, werden nicht zum lernen herangezogen. Der Agent würde durch diese Erfahrungen lernen, wie das Spiel die Äpfel zufallsbasiert neu verteilt, was nicht dem Trainingsziel entspricht. Durch das Entfernen dieser Erfahrungen wird das Pathfinding des Agenten verbessert.\\
Der dritte Faktor der Reward-Funktion ist die Timeout Strategy. Diese sorgt für eine Bestrafung, wenn der Agent über eine gewissen Anzahl an Schritten $P$ keinen Apfel gefressen hat. Dies dient dazu den Agenten so zu trainieren, dass er den möglichst optimalsten Weg findet und um etwaige DeadLocks, wie das Gehen im Kreis, zu verhindern. Dabei werden die Rewards der letzen $P$ Schritte (Erfahrungen) mit einem Malus belegt, der sich nach der Länge der Snake richtet.
Insgesamt setzt sich die Reward-Funktion nun folgendermaßen zusammen:\\
Autonormal gilt der Distance Reward. Sollte sich das Spiel im Training Gap befinden, so wird der Reward des Training Gap gewählt. Überschreitet der Agent jedoch die Granze von $P$ Schritten ohne einen Apfel gefressen zu haben, wird der Timeout Strategy Reward Malus auf die letzen $P$ Schritte aufaddiert.\\
\\ Die Dual Experience Replay Method ist die zweite vorgestellt Optimierung dieses Papers. Hierbei wird der Experience Replay Buffer in zwei Hälften geteilt. in $MP_{1}$ und $MP_{2}$. Diese beiden Buffer speichern beide die Erfahrungen aus den gespielten Spielen. Der Unterschied zwischen beiden besteht jedoch darin, dass in $MP_{1}$ nur reward-basiert gute Erfahrungen ($r > 0.5$) gespeichert werden. Alle Erfahrungen mit Rewards $r < 0.5$ werden in $MP_{2}$ gespeichert. Ähnlich der $\epsilon$-greedy Strategie, wird das Verhältnis zwischen den Erfahrungen von $MP_{1}$ und $MP_{2}$ mit einem Linearen Zerfall gesteuert. Zu Beginn wird werden durchschnittlich 80\% Erfahrungen aus $MP_{1}$ und 20\% aus $MP_{2}$ gewählt. Durch stetiges decrementieren fällt dieses Verhältnis gegen Zerfallsende auf 50\% $MP_{1}$ und 50\% $MP_{2}$.\\
Durch diese Aufteilung soll ein schnellerer Lernerfolg gerade zu Beginn erzielt werden.

\subsection{Diskussion}
Die vorliegende Arbeit besitzt bereits viele gute Optimierungsstrategien, welche auf in dieser Bachelorarbeit Verwendung finden sollen. So ist die Idee der Anpassung der Reward-Funktion auf im Vorfeld der Bearbeitung der Verwandten Arbeiten als möglicher Optimierungsansatz infrage gekommen. Jedoch sehe ich davon ab, die Reward-Funktion genau so zu verwenden. Besonders der Timeout Strategy Reward Malus stellt ergibt keinen Sinn, da das Snake Env. so konzipiert wurde, dass die Spiel endet, sofern die Snake in $P$ Schritten keine Apfel frisst.\\
Dennoch ist der Ansatz der distanzbasierten Reward-Funktion sowie die Einbeziehung der Training Gap wertvolle Erweiterungen, welche in die Optimierungsphase mit eingebunden werden. Ähnlich verhält es sich mit der Dual Experience Replay Method.\\
Kritisch hingegen ist die Auswahl des Agenten zu betrachten. Das paper bleibt einer Begründung für die Nutzung des DQN-Agenten, welchen die Verfasser selbst erstellt haben, schuldig. Auch wurde nicht tiefergehend auf die Wahl der Netzsturktur, wie auch auf die Observation, eingegangen. Vielmehr erklärt das Paper nur welche Komponenten das Netzwerk besitzt, mit einer kurzen Erwähnung der Funktion der Komponente. Bei der Observation setzen die Verfasser auf ein 240x240 Pixel große Screenshots der Spieloberfälche. Diese Method sorgt zwar für eine vollständige Erfassung des Spielgeschehens seitens des Agenten, jedoch auch für eine große rechnerische Belastung. Es ist zu beführchten, dass das Lernen deutlichlagsamen wird. Des weitern könnte durch den Einsatz von Convolutionalen Layern wichtige Informationen verloren gehen. Verstärkt wird dieser Verdacht zudem nochdurch die tatsache, dass die Verfasser Convolutionalen Layer mit großen Filtern und Strides verwenden.\\
Diese Tatsachen sollen bei dem Design der netzstruktur beachtet werden, damit kein Informationsverlust eintreten kann. Ein begründete Wahl der Netzsturktur sowie eine auf diese netzstruktur angepasste Observation sollen die Folge dieser Kritik sein. Abschließend werden solgende Elemente aus diesem Paper in dieser Bachelorarbeit Verwendung oder Beachtung finden:
\begin{itemize}
	\item optimierte Reward-Funktion
	\item Dual Experience Replay Method 
	\item differenzierte Wahl der Netzsturktur
	\item differenzierte Wahl der Observation
\end{itemize}

\section{Deep Reinforcement Learning for Snake}
Die Arbeit Deep Reinforcement Learning for Snake wurde von Anton Finnson und Victor Molnö verfasst. Auf der inhaltlichen Ebene beschäftigt sich das Paper mit dem Verfahren, der Optimierung von Q-Learning Agenten durch successives anpassen einzelner Hyperparameter.\\
Nachdem zunächst ein leichteres Spiel herangezogen wurde, um die Agenten zu testen, haben die Autoren mit dem Spiel Snake begonnen. Aufgrund schlechter Leistungen wurde der Hauptfokus auf einen DDQN gelegt. Dieser wurde zuerst mit einen fest vordefinierten Set an Hyperparametern getestet.\\
Ziel des Vorgehens ist es einen Agenten mit den optimalsten Hyperparametern zu bestimmen. Dafür wurden eine Reihe an Standardparametern festgelegt, welche als Basis für die folgenden einzelnen Anpassungen dienen. Basierend auf den Basisparametern wird nun immer ein Parameter verändert. Daraunter fallen $\epsilon_{\text{min}}$ \ref{Q-Learning} die Netzsturktur $\epsilon_{\text{dec}}$ der Replay Memory Buffer usw. Am Ende wurden die besten besten Parameter genutzt einen optimierten Agenten zu definieren, welche eine bessere Leistung als der Agent vorwies als der Standard Ageten.

\section{UAV Autonomous Target Search Based onDeep Reinforcement Learning in Complex Disaster Scene}

