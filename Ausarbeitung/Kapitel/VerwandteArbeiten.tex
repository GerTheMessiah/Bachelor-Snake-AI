\chapter{Verwandte Arbeiten} \label{chap:Verwandte_Arbeiten}
In diesem Kapitel soll es thematisch über den momentanen Stand der bereits durchgeführten Forschung gehen. Dabei sollen die Arbeiten gezielt nach den folgenden Aspekten durchsucht und diese anschließend diskutiert werden. Die Arbeiten wurde aufgrund ihres thematischen Hintergrundes zum Spiel Snake, in Verbindung mit dem RL, ausgewählt. Zu den Aspekten gehören:
\begin{itemize}
	\item Optimierungsstrategien
	\item Reward Function
	\item Evaluationskriterien
\end{itemize}

\section{Autonomous Agents in Snake Game via Deep Reinforcement Learning} \label{sec:Paper_1}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{Autonomous_Agents_in_Snake_Game_via_DRL} bezogen.
In der Arbeit "Autonomous Agents in Snake Game via Deep Reinforcement Learning" wurden mehrere Optimierungen an einem DQN Agenten durchgeführt, um eine größere Performance im Spiel Snake zu erzielen. Sie wurde von Zhepei Wei et al., welche an verschiedenen Universitäten und Forschungsinstituten, wie z.B. College of Computer Science and Technology Jilin University (Changchun, China) und die Science and Engineering Nanyang Technological University (Singapore) arbeiten, verfasst und im Jahr 2018 veröffentlicht.\\
\\Thematisch werden in diesem Paper drei Optimierungsstrategien verwendet, welche auf einen Baseline DQN (Referenz DQN) angewendet worden sind. Bei diesen Strategien handelt es sich um den Training Gap, die Timeout Strategy und den Dual Experience Replay.\\
Der Dual Experience Replay (Splited Memory) besteht aus zwei Sub-Memories, in welchen Erfahrungen belohnungsbasiert sortiert und gespeichert werden. Mem1 besteht nur aus Erfahrungen, welche einen Reward größer als ein vordefinierter Grenzwert besitzen. Die restlichen Erfahrungen wandern in Mem2. Beim Lernen wird, zu beginn, eine überproportional größere Menge an Erfahrungen aus Mem1 ausgewählt, um den Lernerfolg zu beschleunigen. Im weiteren Lernverlauf wird dann das Entnahmeverhältnis normalisiert (Mem1: 50\% \& Mem2: 50\%).\\
Der Training Gap stellt die Erfahrungen dar, welche der Agent, zum Zwecke des performanteren Lernens, nicht verarbeiten soll. Zu diesen zählt die Erfahrung direkt nach dem Konsum eines Apfel, sodass der Agent die Neuplatzierung eines Apfels erlernt. Da der Agent auf diesen Prozess keinen Einfluss hat, könnte die Verarbeitung dieser Daten den Lernerfolg mindern, weshalb die Training Gap Strategie etwaige Erfahrungen löscht.\\
Die Timeout Strategy sorgt für eine Bestrafung, wenn der Agent über eine vordefinierte Anzahl an Schritten $P$ keinen Apfel mehr gefressen hat. Dabei werden die Rewards mit der letzten $P$ Erfahrungen mit einem Malus verrechnet, was den Agenten dazu anhält die schnellste Route zum Apfel zu finden. Die Höhe des Malus ist antiproportional zur Länge der Snake (geringe Länge $\rightarrow$ großer Malus; große Länge $\rightarrow$ geringer Malus).\\
Die optimierte Reward Function, welche das Paper verwendet, besteht damit aus dem Standard Distanz Reward ohne Training Gap Erfahrungen. Letzterer bestimmt sich aus der Distanz zwischen Kopf und Apfel und der Länge der Snake, wobei der Factor der Länge wieder antiproportional den Reward beeinflusst. Sollte die Timeout Strategy auslösen, so werden die letzten $P$ Erfahrungen entsprechend angepasst und in Mem2 verschoben.\\
Als maßgebliche Kriterien zur Evaluation der Leistung des DQN wurde die Performance, daher die Anzahl an gefressenen Äpfeln und die steps survived, also die überlebten Schritte herangezogen.

\subsection{Diskussion} \label{sec:Paper_1_Diskussion}
Sowohl die Training Gap also auch Dual Experience Replay und Timeout Strategy stellen vielversprechende Optimierungen dar, welche, auf experimentellen Resultaten basierend, gute Ergebnisse erzielen konnten. Eine Verwendung dieser Optimierungen im Kapitel \ref{chap:Optimierungen} bietet sich daher an. Dennoch müssen, für die Anwendung der Optimierungen einige Anpassungen durchgeführt werden.\\
Der Dual Experience Replay ist zwar für DQN Agenten ohne weitere Einschränkungen einsetzbar, jedoch verhindert die Logik des PPOs eine direkte Anwendung. Alternativ könnte nicht mehr nach einzelnen Erfahrungen sortieren werden, sondern nach abgeschlossenen Spielepisoden. Der Mittelwert über alle erhaltenden Rewards wäre dann der Sortierungsfaktor.\\
Fraglich ist, ob die Agenten, insbesondere der PPO, damit bessere Resultate erzielen können.\\
Auch beim Training Gap könnte der PPO Probleme bereiten, da nicht einfach Erfahrungen aus dem Memory entfernt werden sollten. Fraglich ist, ob die Entnahme dieser nicht lernenswerten Erfahrungen die Performance erhöhen oder senken.\\
Die Timeout Strategy bedarf ebenfalls einer Anpassung, da der verwendete DQN nach jedem Schritt lernt. Somit ist das editieren der Rewards im Nachhinein nicht möglich. Alternativ könnte der Reward direkt und stetig während des Spiels gesenkt werden, um die Funktionalität für den DQN herzustellen.\\
Erwähnenswert sind ebenfalls noch die Evaluationskriterien, welche sich auf die Performance und Spielzeit (time alive) beschränken. Eine weitere Betrachtung, beispielsweise der Robustheit oder Trainingszeit, wird vernachlässigt. Dies soll in dieser Ausarbeitung jedoch geschehen, da diese Faktoren ebenfalls wichtig für die voll umfassende Bewertung des Agenten sind, besonders in Real World Applications \ref{sec:Motivation}.


\section{UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene} \label{sec:Paper_2}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{UAV} bezogen.
Die Arbeit UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene wurde von Chunxue Wu et al. verfasst und am 5. August 2019 veröffentlicht. Dabei wird das Spiel Snake als ein dynamisches Pathfinding Problem interpretiert, auf dessen Basis unbemannte Drohnen in Katastrophensituationen zum Einsatz kommen sollen. Auch in diesem Paper verwenden die Autoren Optimierungen, um den Lernerfolg zu steigern.\\
\\ Eine dieser Optimierungen wurde des olfaktorischen Sinnes konzipiert. Der Odor Effect erzeugt um den Apfel drei aneinanderliegende Kreise, in welchen ein größerer Reward zurückgegeben wird als außerhalb der Kreise. Dabei unterscheiden sich die Kreise in der Höhe des Rewards, sodass der dritte den geringsten und der erste den größten Reward von allen zurückgibt ($r_1 > r_2 > r_3$, wobei $r_x$ der Reward des x-ten Kreises darstellt). Dieser Zonen stellen den zunehmenden Duft von Nahrung dar, wobei der Geruch immer stärker wird um so näher man sich der Quelle nährt.\\
Eine weitere Optimierungsstrategie basiert auf dem Loop Storm Effect, welcher das Verhalten des Laufens um den Apfel beschreibt. Die Verfasser haben festgestellt, dass dieser Effekt zu einem sehr schlechten Lernerfolg und zu langen Trainingsdauern führt. Darum haben Wu et al. einen dynamischen Positionsspeicher konzipiert, welcher Loops erkennt und diese durch das Zurückgeben einer Zufallsaktion, welche nicht auf dem Loop liegt, unterbricht. Experimentelle Ergebnisse des Papers zeigen, dass der Loop Storm Effect kaum mehr präsent ist.
\\ Auf Basis einer Standard Reward Function, welche für das Essen eines Apfels +100 und für das Sterben -100 zurückgibt, wurden ein Versuch durchgeführt. Dem Agenten war es nicht möglich gegen die optimale Lösung zu konvergieren, aufgrund von Loop Storms und einer unzureichende Reward Function. Hingegen war es dem Agenten mit dem Odor Effect und mit der Breakout Loop Storm Strategy möglich eine Konvergenz on 8 Millionen Epochs (hier Trainingsdurchläufe) zu verzeichne.\\
Auch in diesem Paper bleibt die Performance maßgeblicher Evaluationsfaktor, wobei der Fokus auf den erreichten Reward gelegt wurde, welcher stark mit der Performance korreliert.

\subsection{Diskussion}\label{sec:Paper_2_Diskussion}
Loop Storm Effect und Odor Effect stellen nach den experimentellen Ergebnissen gute Ergänzungen für das Kapitel Optimierungen \ref{chap:Optimierungen} dar. Dennoch ist fraglich, ob der Odor Effekt bessere Resultate als der Standard Distanz Reward aus Paper eins \ref{sec:Paper_1} erzielen kann. Auch bleibt die Frage offen, ob der Loop Storm Effect wirklich ein solches Problem darstellt. Eventuell ist es möglich diesen mit einer geschickten Belegung der Kreis-Rewards zu umgehen. Auch wirft die Präventionsstrategie des Loop Storm Effects Fragen auf. Was ist beispielsweise, wenn der Loop zwischen mehreren Kreisen (Loop Kreise nicht Odor Kreise) alterniert. Die Loop Detection würde durch ein solches Verhalten erschwert werden. Alternativ wäre es möglich, Zahl an gelaufenen Schritten um den Apfel zu zählen und ab einem Grenzwert eine Zufallsaktion zu wählen. Diese Methode ist zwar nicht so präzise aber allgemeiner anwendbar. Statistische Versuche sollen zeigen ob der Odor Effect und die Breakout Loop Storm Strategy die Performance und weitere Faktoren, welche im Paper nicht behandelt wurden, verbessern können, im Vergleich zu einem Referenz Modell.

\section{Zusammenfassung}
Sowohl ''Autonomous Agents in Snake Game via Deep Reinforcement Learning'' \ref{sec:Paper_1} als auch ''UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene'' \ref{sec:Paper_2} bieten einige Optimierungsstrategien, welche im weiteren Verlauf dieser Ausarbeitung angewendet werden sollen. Damit die Optimierungen auf beide Agenten Arten (DQN und PPO) angewendet werden können, wurden Anpassungen Diskutiert, welche sich im statistischen Vergleichen bewähren müssen. Diese Vergleiche werden im Kapitel Optimierungen \ref{chap:Optimierungen}, durch das Erklären der derselbigen, vorbereiten und die Resultate im Kapitel \ref{chap:Evaluation} dargestellt.\\
Auch wurde ein näherer Blick auf die Reward Function gelegt, um Anregungen für die eigene Reward Function zu erhalten. Ähnlich verhält es sich auch mit den Evaluationskriterien, die jedoch aus Sicht dieser Ausarbeitung für unzureichende erklärt werden müssen.
