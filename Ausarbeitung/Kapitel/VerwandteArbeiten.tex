\chapter{Verwandte Arbeiten} \label{chap:Verwandte_Arbeiten}
In diesem Kapitel soll es thematisch über den momentanen Stand der bereits durchgeführten Forschung gehen. Dabei sollen die Arbeiten gezielt nach den folgenden Aspekten durchsucht werden. Anschließend folgt eine Diskussion. 
Ausgewählt wurde die Arbeiten aufgrund ihres thematischen Hintergrundes zum Spiel Snake, in Verbindung mit dem RL.
Zu den oben erwähnten Aspekten gehören:
\begin{itemize}
	\item Optimierungsstrategien
	\item Reward Funktion
	\item Evaluationskriterien
\end{itemize}

\section{Autonomous Agents in Snake Game via Deep Reinforcement Learning} \label{sec:Paper_1}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{Autonomous_Agents_in_Snake_Game_via_DRL} bezogen.
In der Arbeit "`Autonomous Agents in Snake Game via Deep Reinforcement Learning"' wurden mehrere Optimierungen an einem DQN Agenten durchgeführt, um eine größere Performance im Spiel Snake zu erzielen. Sie wurde von Zhepei Wei et al. verfasst und im Jahr 2018 veröffentlicht.\\
\\Thematisch wurden in diesem Paper drei Optimierungsstrategien vorgestellt, welche auf einen Baseline DQN (Referenz DQN) angewendet worden sind. Bei diesen Strategien handelt es sich um den Training Gap, die Timeout Strategy und den Dual Experience Replay.\\
Der Dual Experience Replay (Splited Memory) besteht aus zwei Sub-Memories, in welchen Erfahrungen belohnungsbasiert einsortiert und gespeichert werden. Mem1 besteht dabei nur aus Erfahrungen, welche einen Reward aufweisen, der größer als ein vordefinierter Grenzwert ist. Die restlichen Erfahrungen werden in Mem2 eingepflegt. 
Zu Beginn des Lernens werden 80\% Erfahrungen aus Mem1 ausgewählt und 20\% Erfahrungen aus Mem2 entnommen, um den Lernerfolg zu beschleunigen. Im weiteren Lernverlauf wird dieses Verhältnis normalisiert (Mem1: 50\% and Mem2: 50\%).\\
Der Training Gap beschreibt die Erfahrungen, welche der Agent, zum Zweck des performanteren Lernens, nicht verarbeiten soll. Zu diesen zählen die Erfahrungen direkt nach dem Konsum eines Apfels, sodass der Agent die Neuplatzierung dieses erlernen würde. 
Da der Agent auf diesen Prozess jedoch keinen Einfluss hat, könnte die Verarbeitung dieser Daten den Lernerfolg mindern, weshalb die Training Gap Strategie etwaige Erfahrungen nicht speichert und das Lernen während dieser Periode verhindert.\\
Die Timeout Strategy sorgt für eine Bestrafung, wenn der Agent über eine vordefinierte Anzahl an Schritten $P$ keinen Apfel mehr gefressen hat. Dabei werden die Rewards der letzten $P$ Erfahrungen mit einem Malus verrechnet, was den Agenten dazu anhält die schnellste Route zum Apfel zu finden. Die Höhe des Malus ist antiproportional zur Länge der Snake (geringe Länge $\rightarrow$ großer Malus; große Länge $\rightarrow$ geringer Malus).\\
Die optimierte Reward Funktion, welche das Paper verwendet, besteht damit aus dem Distanz Reward, welcher sich aus der Addition des vorherigen Rewards mit $\Delta r$ ergibt.
\begin{align}
	r_{res} = r_t + \Delta r
\end{align}
Zusätzlich wird der resultierende Reward zwischen 1 und -1 geclipt $r_{res} = clip(r_{res}, -1, 1)$.\\
$\Delta r$ ist dabei wie folgt definiert:
\begin{align}
	\Delta r(L_t, D_t,D_{t+1}) = \log_{L_t}\frac{L_t + D_t}{L_t + D_{t + 1}}
\end{align}
Wobei $t$ dem vorherigen, $t+1$ dem aktuellen Zeitpunkt darstellt. $L_t$ ist die Läge der Snake zum vorherigen Zeitpunkt und $D_t$ und $D_{t+1}$ stellen die Distanzen zwischen Snake und Apfel zum vorherigen und aktuellen Zeitpunkt dar.\\
Sollte die Timeout Strategy auslösen, so werden die letzten $P$ Erfahrungen entsprechend angepasst und in Mem2 verschoben.\\
Als maßgebliches Kriterien zur Evaluation der Leistung des DQN wurde die Performance, gemessen im Score und die steps survived, also die überlebten Schritte herangezogen.

\subsection{Diskussion} \label{sec:Paper_1_Diskussion}
Sowohl die Training Gap also auch Dual Experience Replay und Timeout Strategy stellen vielversprechende Optimierungen dar, welche, auf experimentellen Resultaten basierend, gute Ergebnisse erzielen konnten. Jedoch existieren auch Diskussionspunkte an der Ausarbeitung.
Das Env wird im Paper nur kurz vorgestellt, jedoch lässt sich aus dem Abschnitt Game Environment schließen, dass alle Anforderungen des Env \ref{sec:Anforderungen_Env} erfüllt sind.\\
\\Die Verfasser führen keinen Vergleich mit anderen Algorithmen durch, lediglich ein DQN Agent wird betrachtet. 
Daher sind auch die Optimierungen, darunter Dual Experience Replay, Training Gap und Timeout Strategy, nicht für den PPO-Algorithmus anwendbar. 
Zwar sind alle Funktionalitäten des, im Paper verwendeten, DQN gegeben 
(siehe \ref{sec:Anforderungen_funktionalität_Env}), jedoch wurde der Fokus kaum auf eine Parametrisierung \ref{sec:Anforderungen_Parametrisierung} gelegt, da dieses Paper hauptsächlich nur eine Agenten betrachtet hat und nicht mehrere Varianten, mit Ausnahme der optimierten Agenten. Diese unterscheiden sich nur durch die Optimierungen.\\
\\Auch bei der statistischen Datenerhebung existieren Abweichungen zu den Anforderungen in \ref{sec:Anforderungen_an_die_Datenerhebung}. Zhepei Wei et al. verzichteten auf eine mehrfache Datenerhebung und markierten daher ihre Ergebnisse als experimentell. Dem Leser werden nur indirekt Informationen über die erhobenen Daten mitgeteilt. Aus Statistiken lässt sich schließen, dass der Scores und die steps (Anzahl der Schritt) gespeichert wurden.\\
\\Weiterhin wurde sich bei den Evaluationskriterien einzig auf die Performance, gemessen am Score, und Spielzeit, gemessen in steps, konzentriert. Weitere Kriterien wie, beispielsweise die Robustheit oder Siegrate \ref{tab:Kriterien}, werden nicht betrachtet.\\
Dies soll in dieser Ausarbeitung jedoch geschehen, da diese Faktoren ebenfalls wichtig für eine voll umfassende Bewertung der Agenten sind, besonders in Realwelt-Applikationen (siehe \ref{sec:Motivation}).\\ 
Des Weiteren ist erwähnenswert, dass die Verfasser auf das Evaluationskriterium der steps survived setzten, welches im kompletten Widerspruch zur Effizienz steht. Zhepei Wei et al. sehen ein langes Überleben der Snake als positiv an, wohingegen dies in dieser Ausarbeitung kritisch betrachtet wird, da es kein zielgerichtetes Verhalten bestärkt. Ziel der Agenten in dieser Ausarbeitung ist es, das Spiel Snake möglichst effizient zu lösen und es nicht lange zu überleben.

\section{UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene} \label{sec:Paper_2}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{UAV} bezogen.
Die Arbeit "`UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene"' wurde von Chunxue Wu et al. verfasst und am 5. August 2019 veröffentlicht. Dabei wird das Spiel Snake als ein dynamisches Pathfinding Problem interpretiert, auf dessen Basis unbemannte Drohnen in Katastrophensituationen zum Einsatz kommen sollen.\\
\\Auch in diesem Paper verwenden die Autoren Optimierungen, um den Lernerfolg zu steigern.\\
Einer dieser Optimierungen wurde auf Basis des Geruchssinnes konzipiert. Der Odor Effect erzeugt um den Apfel drei aneinanderliegende Geruchszonen, in welchen ein größerer Reward zurückgegeben wird als außerhalb der Geruchszonen. Dabei unterscheiden sich diese in der Höhe des zurückgegebenen Rewards, sodass die dritte Zone den geringsten und die erste Zone den größten Reward von allen zurückgibt ($r_1 > r_2 > r_3$, wobei $r_x$ der Reward des x-ten Kreises darstellt).
Diese Zonen stellen den zunehmenden Duft von Nahrung dar, wobei dieser immer stärker wird, um so näher man sich der Quelle nährt.\\
Eine weitere Optimierungsstrategie basiert auf dem Loop Storm Effect, welcher das Verhalten beschreibt, um den Apfel zu laufen. Die Verfasser haben festgestellt, dass dieser Effekt zu einem schlechten Lernerfolg führt. Darum haben Wu et al. einen dynamischen Positionsspeicher konzipiert, welcher Loops erkennt und diese, durch das Zurückgeben einer Zufallsaktion, welche nicht auf dem Loop liegt, unterbricht. Experimentelle Ergebnisse des Papers haben gezeigt, dass der Loop Storm Effect kaum mehr in Erscheinung trat.\\
Auf Basis einer Standard Reward Funktion, welche für das Essen eines Apfels +100 und für das Sterben -100 zurückgibt, wurden ein Versuch durchgeführt. Dem Agenten war es nicht möglich gegen die optimale Lösung zu konvergieren, aufgrund von Loop Storms und einer unzureichende Reward Funktion. Hingegen war es dem Agenten mit Odor Effect und dynamischen Positionsspeicher möglich, eine Konvergenz nach 8 Millionen Epochs (hier Trainingsdurchläufe eines DQN) zu erreichen.\\
Die Reward Funktion entspricht daher dem Odor Effect.
Auch in diesem Paper bleibt die Performance maßgeblicher Evaluationsfaktor, wobei der Fokus auf den erreichten Reward gelegt wurde, welcher stark mit der Performance korreliert.


\subsection{Diskussion}\label{sec:Paper_2_Diskussion}
Auch dieses Paper besitzt interessante Verbesserungen, welche, nach den ersten Ergebnissen, gute Resultate vorweist. Jedoch legt auch dieses Paper andere Schwerpunkte als diese Ausarbeitung mit seinen Anforderungen (siehe \ref{chap:Anforderungen}).\\
\\Aus Graphiken geht hervor, dass die Verfasser ein Env mit einer Visualisierung besitzen. Weiterhin ist davon auszugehen, dass auch alle weiteren Anforderungen bezüglich des Env (siehe \ref{sec:Anforderungen_an_die_Evaluation}) erfüllt worden sind, da ansonsten kein Training eines Agenten möglich wäre. Dennoch wurde das Env nur auf grundlegende Weise behandelt.\\
Das Paper legt seinen Schwerpunkt deutlich mehr auf die Grundlagen des RL, wie z.B. auf den Markov Decision Process und die RL Kernbegriffe (siehe \ref{sec:Vokabular}).\\
\\Auch erfüllt die Ausarbeitung alle Anforderungen an die Funktionalitäten von Agenten (siehe \ref{sec:Agent_Funktionalitäten}). Dennoch wurde auch hier weniger der Fokus auf eine Parametrisierung der Agenten gelegt, da wieder nur ein DQN Agent näher untersucht wurde. Zwar werden gegen Ende einige Vergleich zu einer Hand voll anderer Agenten getätigt, jedoch sind diese Vergleiche nur sehr oberflächlich, da auf diesen Punkt nicht das Hauptaugenmerk der Arbeit liegt. 
Im Gegensatz dazu, soll in dieser Ausarbeitung der Vergleich von Agenten im Mittelpunkt liegen.\\
\\Wie auch im ersten Paper (siehe \ref{sec:Paper_1_Diskussion}), setzen die Verfasser nicht auf eine mehrfache Datenerhebung für die statistische Auswertung, sondern kennzeichnen ihrer Ergebnisse als experimentell. 
Eine direkte Erwähnung der Daten, welche im Verlauf des Trainings und es Testens gespeichert werden, wird nicht durchgeführt. Auch dies soll im Rahmen dieser Ausarbeitung geschehen um die Parametrisierung (siehe \ref{sec:Anforderungen_Parametrisierung}) herauszustellen.  
Die gezeigten Statistiken weisen jedoch darauf hin, dass der Score sowie der mittlere Aktionswert (Q-Value) gespeichert wurden. Zuzüglich werden, für die Optimierungen, die steps und das Auftreten von Loop Storms erfasst und gespeichert, anders als in dieser Ausarbeitung, wo immer die gleichen Daten erhoben werden, unbeachtet von optimierten oder unoptimierten Agenten.\\
\\Chunxue Wu et al. verwendeten zudem hauptsächlichen den Score und die Q-Values als Evaluationskriterium. Um die Effizienz der im Paper verwendeten Strategien (Optimierungen) zu zeigen wurden ebenfalls die gemittelten Steps pro Spiel und das Auftreten von Loop Stoms als weitere untergeordnete Evaluationskriterien verwendet.


\section{Zusammenfassung}
Sowohl "`Autonomous Agents in Snake Game via Deep Reinforcement Learning"' \ref{sec:Paper_1} als auch "`UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene"' \ref{sec:Paper_2} bieten einige Optimierungsstrategien, welche im weiteren Verlauf dieser Ausarbeitung als Vorbild dienen sollen.\\
Besonders zu betonen ist, dass die vorgestellten Arbeiten alle verschiedene Schwerpunkte gesetzt haben und daher nicht immer die Anforderungen dieser Ausarbeitung erfüllt haben.
