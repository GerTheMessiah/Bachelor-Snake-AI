\chapter{Verwandte Arbeiten} \label{chap:Verwandte_Arbeiten}
In diesem Kapitel soll es thematisch über den momentanen Stand der bereits durchgeführten Forschung gehen. Dabei sollen die Arbeiten gezielt unter den folgenden Aspekten untersucht werden. 
\begin{itemize}
	\item Optimierungsstrategien
	\item Reward Funktion
	\item Evaluationskriterien
\end{itemize}
Anschließend folgt die Diskussion über die einzelnen verwandten Arbeiten. 
Ausgewählt wurde diese aufgrund ihres thematischen Hintergrundes zum Spiel Snake, in Verbindung mit dem RL.

\section{Autonomous Agents in Snake Game via Deep Reinforcement Learning} \label{sec:Verwandte_Arbeiten_Paper_1}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{Autonomous_Agents_in_Snake_Game_via_DRL} bezogen.
In der Arbeit "`Autonomous Agents in Snake Game via Deep Reinforcement Learning"' wurden mehrere Optimierungen an einem DQN Agenten durchgeführt, um eine größere Performance im Spiel Snake zu erzielen. Sie wurde von Zhepei Wei et al. verfasst und im Jahr 2018 veröffentlicht.\\
\\Thematisch wurden in diesem Paper drei Optimierungsstrategien vorgestellt, welche auf einen Baseline DQN (Referenz DQN) Agenten angewendet worden sind. Bei diesen Strategien handelt es sich um den Training Gap, die Timeout Strategy und den Dual Experience Replay.\\
Dieser (Splited Memory) besteht aus zwei Sub-Memories, in welchen Erfahrungen belohnungsbasiert einsortiert und gespeichert werden. Mem1 besteht dabei nur aus Erfahrungen, welche einen Reward aufweisen, der größer als ein vordefinierter Grenzwert ist. Die restlichen Erfahrungen werden in Mem2 eingepflegt. 
Zu Beginn des Lernens werden 80\% Erfahrungen aus Mem1 und 20\% Erfahrungen aus Mem2 entnommen, um den Lernerfolg zu beschleunigen. Im weiteren Lernverlauf wird dieses Verhältnis normalisiert (Mem1: 50\% and Mem2: 50\%).\\
Der Training Gap beschreibt die Erfahrungen, welche der Agent zum Zweck des performanteren Lernens nicht verarbeiten soll. Zu diesen zählen die Erfahrungen direkt nach dem Konsum eines Apfels, sodass der Agent die Neuplatzierung dieses erlernen würde. 
Da der Agent auf diesen Prozess jedoch keinen Einfluss hat, könnte die Verarbeitung dieser Daten den Lernerfolg mindern, weshalb die Training Gap Strategie etwaige Erfahrungen nicht speichert und das Lernen während dieser Periode verhindert.\\
Die Timeout Strategy sorgt für eine Bestrafung, wenn der Agent über eine vordefinierte Anzahl an Schritten $P$ keinen Apfel mehr gefressen hat. Dabei werden die Rewards der letzten $P$ Erfahrungen mit einem Malus verrechnet, was den Agenten dazu anhält, die schnellste Route zum Apfel zu finden. Die Höhe des Malusses ist antiproportional zur Länge der Snake (geringe Länge $\rightarrow$ großer Malus; große Länge $\rightarrow$ geringer Malus).\\
Die optimierte Reward Funktion, welche das Paper verwendet, besteht damit aus dem Distanz Reward, welcher sich aus der Addition des vorherigen Rewards mit $\Delta r$ ergibt \fullref{eq:Verwandte_Arbeiten_r}.
\begin{align}
	r_{res} = r_t + \Delta r
	\label{eq:Verwandte_Arbeiten_r}
\end{align}
Zusätzlich wird der resultierende Reward zwischen 1 und -1 geclipt $r_{res} = clip(r_{res}, -1, 1)$.\\
$\Delta r$ ist dabei wie folgt definiert \fullref{eq:Verwandte_Arbeiten_Delta_r}:
\begin{align}
	\Delta r(L_t, D_t,D_{t+1}) = \log_{L_t}\frac{L_t + D_t}{L_t + D_{t + 1}}
	\label{eq:Verwandte_Arbeiten_Delta_r}
\end{align}
Wobei $t$ den vorherigen und $t+1$ den aktuellen Zeitpunkt darstellt. $L_t$ ist die Läge der Snake zum vorherigen Zeitpunkt und $D_t$ und $D_{t+1}$ stellen die Distanzen zwischen Snake und Apfel zum vorherigen und aktuellen Zeitpunkt dar.\\
Sollte die Timeout Strategy auslösen, so werden die letzten $P$ Erfahrungen entsprechend angepasst und in Mem2 verschoben.\\
Als maßgebliches Kriterium zur Evaluation der Leistung des DQN wurde die Performance herangezogen, gemessen am Score und an den steps survived, also die überlebten Schritte.

\subsection{Diskussion} \label{subsec:Verwandte_Arbeiten_Paper_1_Diskussion}
Sowohl die Training Gap Strategy also auch Dual Experience Replay und Timeout Strategy stellen vielversprechende Optimierungen dar, welche auf experimentellen Resultaten basierend gute Ergebnisse erzielen konnten. Jedoch existieren auch Diskussionspunkte an der Ausarbeitung.
Das Env wird im Paper nur kurz vorgestellt, jedoch lässt sich aus dem Abschnitt Game Environment schließen, dass alle Anforderungen des Env \fullref{sec:Anforderungen_Env} erfüllt sind.\\
\\Die Verfasser führen keinen Vergleich mit anderen Algorithmen durch, lediglich ein DQN Agent wird betrachtet. 
Es ist unklar, ob auch die Optimierungen, darunter Dual Experience Replay, Training Gap und Timeout Strategy, für den PPO-Algorithmus geeignet.
Zwar sind alle Funktionalitäten des im Paper verwendeten DQN gegeben \fullref{subsec:Anforderungen_Funktionalitäten_Agent}, jedoch wurde der Fokus kaum auf eine Parametrisierung \fullref{subsubsec:Anforderungen_Parametrisierung} gelegt, da dieses Paper hauptsächlich nur eine Agenten betrachtet hat und nicht mehrere Varianten, mit Ausnahme der optimierten Agenten. Diese unterscheiden sich nur durch die Optimierungen.\\
\\Auch bei der statistischen Datenerhebung existieren Abweichungen zu den Anforderungen in \autoref{sec:Anforderungen_an_die_Datenerhebung}. Zhepei Wei et al. verzichteten auf einen mehrfache Datenerhebung ihrer experimentellen Ergebnisse. Dem Leser werden nur indirekt Informationen über die erhobenen Daten mitgeteilt. Aus Statistiken lässt sich jedoch schließen, dass der Scores und die steps (Anzahl der Schritt) gespeichert wurden.\\
\\Weiterhin wurde sich bei den Evaluationskriterien einzig auf die Performance und Spielzeit konzentriert. Weitere Kriterien wie beispielsweise die Robustheit oder Siegrate \fullref{tab:Anforderungen_Kriterien}, werden nicht betrachtet.\\
Dies soll in dieser Ausarbeitung jedoch geschehen, da diese Faktoren ebenfalls wichtig für eine voll umfassende Bewertung der Agenten sind, besonders in Real World Applikationen \fullref{sec:Einleitung_Motivation}.\\ 
Des Weiteren ist erwähnenswert, dass die Verfasser auf das Evaluationskriterium der steps survived setzten, welches im kompletten Widerspruch zur Effizienz \fullref{tab:Anforderungen_Kriterien} steht. Zhepei Wei et al. sehen ein langes Überleben der Snake als positiv an, wohingegen dies in dieser Ausarbeitung kritisch betrachtet wird, da es kein zielgerichtetes Verhalten bestärkt. Ziel der Agenten in dieser Ausarbeitung ist es, das Spiel Snake möglichst effizient zu lösen und es nicht lange zu überleben.

\section{UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene} \label{sec:Verwandte_Arbeiten_Paper_2}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{UAV} bezogen.
Die Arbeit "`UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene"' wurde von Chunxue Wu et al. verfasst und am 5. August 2019 veröffentlicht. Dabei wird das Spiel Snake als ein dynamisches Pathfinding Problem interpretiert, auf dessen Basis unbemannte Drohnen in Katastrophensituationen zum Einsatz kommen sollen.\\
\\Auch in diesem Paper verwenden die Autoren Optimierungen, um den Lernerfolg zu steigern.\\
Einer dieser Optimierungen wurde auf Basis des Geruchssinnes konzipiert. Der Odor Effect erzeugt um den Apfel drei aneinanderliegende Geruchszonen, in welchen ein größerer Reward zurückgegeben wird als außerhalb der Geruchszonen. Dabei unterscheiden sich diese in der Höhe des zurückgegebenen Rewards, sodass die dritte Zone den geringsten und die erste Zone den größten Reward von allen zurückgibt ($r_1 > r_2 > r_3$, wobei $r_x$ der Reward der x-ten Zone darstellt).
Diese Zonen stellen den zunehmenden Duft von Nahrung dar, wobei dieser immer stärker wird, umso näher man sich der Quelle nährt.\\
Eine weitere Optimierungsstrategie basiert auf dem Loop Storm Effect, welcher das Verhalten beschreibt, um den Apfel zu laufen. Die Verfasser haben festgestellt, dass dieser Effekt zu einem schlechten Lernerfolg führt. Darum haben Wu et al. einen dynamischen Positionsspeicher konzipiert, welcher Loops erkennt und diese durch das Zurückgeben einer Zufallsaktion, welche nicht auf dem Loop liegt, unterbricht. Experimentelle Ergebnisse des Papers haben gezeigt, dass der Loop Storm Effect, nach Implementierung des dynamischen Positionsspeichers kaum mehr in Erscheinung trat.\\
Auf Basis einer Standard Reward Funktion, welche für das Essen eines Apfels +100 und für das Sterben -100 zurückgibt, wurden ein Versuch durchgeführt. Dem Agenten war es nicht möglich, gegen die optimale Lösung zu konvergieren, aufgrund von Loop Storms und einer unzureichende Reward Funktion. Hingegen war es dem Agenten mit Odor Effect und dynamischen Positionsspeicher möglich, eine Konvergenz zu erreichen.\\
Die Reward Funktion entspricht daher dem Odor Effect.
Auch in diesem Paper bleibt die Performance maßgeblicher Evaluationsfaktor, wobei der Fokus auf den erreichten Reward gelegt wurde, welcher natürlich stark mit der Performance korreliert.


\subsection{Diskussion}\label{subsec:Verwandte_Arbeiten_Paper_2_Diskussion}
Auch dieses Paper besitzt interessante Verbesserungen, welche nach den ersten Ergebnissen gute Resultate vorweist. Jedoch legt auch dieses Paper andere Schwerpunkte als diese Ausarbeitung mit ihren Anforderungen \fullref{chap:Anforderungen}.\\
\\Aus Grafiken geht hervor, dass die Verfasser ein Env mit einer Visualisierung verwenden. Weiterhin ist davon auszugehen, dass auch alle weiteren Anforderungen bezüglich des Env \fullref{sec:Anforderungen_an_die_Evaluation} erfüllt worden sind, da ansonsten kein Training eines Agenten möglich wäre. Dennoch wurde das Env nur auf grundlegende Weise behandelt.\\
Das Paper legt seinen Schwerpunkt deutlich mehr auf die Grundlagen des RL, wie z.B. auf den Markov Decision Process und die RL Kernbegriffe \fullref{subsec:Grundlagen_Vokabular}.\\
\\Auch erfüllt die Ausarbeitung alle Anforderungen an die Funktionalitäten von Agenten \fullref{sec:Anforderungen_Agenten}. Dennoch wurde auch hier weniger der Fokus auf eine Parametrisierung der Agenten gelegt, da wieder nur ein DQN Agent näher untersucht wurde. Zwar werden, gegen Ende, einige Vergleich zu einer Handvoll anderer Agenten getätigt, jedoch sind diese Vergleiche nur sehr oberflächlich, da auf diesen Punkt nicht das Hauptaugenmerk der Arbeit liegt. 
Im Gegensatz dazu, soll in dieser Ausarbeitung der Vergleich von Agenten im Mittelpunkt liegen.\\
\\Wie auch im ersten Paper \fullref{subsec:Verwandte_Arbeiten_Paper_1_Diskussion}, setzen die Verfasser nicht auf eine mehrfache Datenerhebung für die statistische Auswertung ihrer experimentellen Ergebnisse.
Eine direkte Erwähnung der Daten, welche im Verlauf des Trainings und es Testens gespeichert werden, wird nicht durchgeführt. Auch dies soll im Rahmen dieser Ausarbeitung geschehen, um die Parametrisierung \fullref{subsubsec:Anforderungen_Parametrisierung} herauszustellen.  
Die gezeigten Statistiken weisen jedoch darauf hin, dass der Score sowie der mittlere Aktionswert (Q-Value) gespeichert wurden. Zuzüglich werden, für die Optimierungen, die steps und das Auftreten von Loop Storms erfasst und gespeichert. Anders als in dieser Ausarbeitung, wo immer die gleichen Daten erhoben werden \fullref{tab:Anforderungen_Datenerhebung}.\\
\\Chunxue Wu et al. verwendeten zudem hauptsächlichen den Score und die Q-Values als Evaluationskriterien. Um die Effizienz der im Paper verwendeten Strategien (Optimierungen) zu zeigen, wurden ebenfalls die gemittelten Steps pro Spiel und das Auftreten von Loop Stoms als weitere untergeordnete Evaluationskriterien verwendet. In diesem Paper werden daher im Vergleich zum ersten Paper, deutlich mehr Evaluationskriterien genutzt, welche sich partiell mit denen aus dieser Ausarbeitung überschneiden. Dennoch legt das hier betrachtete Paper deutlich mehr Wert auf den Fehler des Agenten. In dieser Ausarbeitung liegt der Fokus deutlich mehr auf der Leistung und Effizienz.


\section{Zusammenfassung} \label{sec:Verwandte_Arbeiten_Zusammenfassung}
Sowohl "`Autonomous Agents in Snake Game via Deep Reinforcement Learning"' \fullref{sec:Verwandte_Arbeiten_Paper_1} als auch "`UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene"' \fullref{sec:Verwandte_Arbeiten_Paper_2} bieten einige Optimierungsstrategien, welche im weiteren Verlauf dieser Ausarbeitung, als Vorbild dienen sollen.\\
Besonders zu betonen ist, dass die vorgestellten Arbeiten alle verschiedene Schwerpunkte gesetzt haben und daher nicht immer die Anforderungen dieser Ausarbeitung erfüllt haben.\\ 
So wurden zwar die Anforderungen zum Env \fullref{sec:Anforderungen_Env} und zu den Agenten \fullref{sec:Anforderungen_Agenten}, mit Ausnahme der Parametrisierung \fullref{subsubsec:Anforderungen_Parametrisierung} und der Diversität der Algorithmen \fullref{subsec:Anforderungen_Diversität}, alle erfüllt. Dennoch wurden auch Anforderungen bei der statistischen Datenerhebung und Evaluation nicht erfüllt. Zu diesen zählen z.B. die mehrfache Datenerhebung \fullref{subsec:Anforderungen_mehrfache_Datenerhebung} und die Evaluation-Anforderungen \fullref{sec:Anforderungen_an_die_Evaluation}. Letztere weichen, verständlicherweise, von den Anforderungen ab, da sich die Verfasser der Arbeiten auf andere Aspekte konzentriert haben und daher nicht dieselben Evaluationskriterien gewählt haben.\\
Insgesamt scheinen die betrachten Arbeiten jedoch ungeeignet zu sein, um die in dieser Ausarbeitung aufgestellte Forschungsfrage zu beantworten \fullref{sec:Einleitung_Forschungsfrage}. Sie führen keinen Vergleich durch, um einen optimalen Agenten zu erhalten sondern setzen mehr auf das optimieren einzelner ausgewählter Agenten. In dieser Arbeit soll daher ein Auswahlprozess stattfinden und kein Optimierungsprozess eines einzelnen Agenten.\\
Im Weiteren wird nun mit dem Konzept dieser Ausarbeitung fortgefahren.