\chapter{Verwandte Arbeiten} \label{chap:Verwandte_Arbeiten}
In diesem Kapitel soll es thematisch über den momentanen Stand der bereits durchgeführten Forschung gehen. Dabei sollen die Arbeiten gezielt nach den folgenden Aspekten durchsucht und diese anschließend diskutiert werden. Die Arbeiten wurde aufgrund ihres thematischen Hintergrundes zum Spiel Snake, in Verbindung mit dem RL, ausgewählt. Zu den Aspekten gehören:
\begin{itemize}
	\item Optimierungsstrategien
	\item Reward Function
	\item Evaluationskriterien
\end{itemize}

\section{Autonomous Agents in Snake Game via Deep Reinforcement Learning} \label{sec:Paper_1}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{Autonomous_Agents_in_Snake_Game_via_DRL} bezogen.
In der Arbeit "Autonomous Agents in Snake Game via Deep Reinforcement Learning" wurden mehrere Optimierungen an einem DQN Agenten durchgeführt, um eine größere Performance im Spiel Snake zu erzielen. Sie wurde von Zhepei Wei et al. verfasst und im Jahr 2018 veröffentlicht.\\
\\Thematisch werden in diesem Paper drei Optimierungsstrategien verwendet, welche auf einen Baseline DQN (Referenz DQN) angewendet worden sind. Bei diesen Strategien handelt es sich um den Training Gap, die Timeout Strategy und den Dual Experience Replay.\\
Der Dual Experience Replay (Splited Memory) besteht aus zwei Sub-Memories, in welchen Erfahrungen belohnungsbasiert sortiert und gespeichert werden. Mem1 besteht nur aus Erfahrungen, welche einen Reward größer als ein vordefinierter Grenzwert besitzen. Die restlichen Erfahrungen wandern in Mem2. Beim Lernen wird, zu beginn, eine überproportional größere Menge an Erfahrungen aus Mem1 ausgewählt, um den Lernerfolg zu beschleunigen. Im weiteren Lernverlauf wird dann das Entnahmeverhältnis normalisiert (Mem1: 50\% \& Mem2: 50\%).\\
Der Training Gap stellt die Erfahrungen dar, welche der Agent, zum Zwecke des performanteren Lernens, nicht verarbeiten soll. Zu diesen zählt die Erfahrung direkt nach dem Konsum eines Apfel, sodass der Agent die Neuplatzierung eines Apfels erlernt. Da der Agent auf diesen Prozess keinen Einfluss hat, könnte die Verarbeitung dieser Daten den Lernerfolg mindern, weshalb die Training Gap Strategie etwaige Erfahrungen löscht.\\
Die Timeout Strategy sorgt für eine Bestrafung, wenn der Agent über eine vordefinierte Anzahl an Schritten $P$ keinen Apfel mehr gefressen hat. Dabei werden die Rewards mit der letzten $P$ Erfahrungen mit einem Malus verrechnet, was den Agenten dazu anhält die schnellste Route zum Apfel zu finden. Die Höhe des Malus ist antiproportional zur Länge der Snake (geringe Länge $\rightarrow$ großer Malus; große Länge $\rightarrow$ geringer Malus).\\
Die optimierte Reward Function, welche das Paper verwendet, besteht damit aus dem Standard Distanz Reward ohne Training Gap Erfahrungen. Letzterer bestimmt sich aus der Distanz zwischen Kopf und Apfel und der Snake-Länge, wobei der Faktor der Länge wieder antiproportional den Reward beeinflusst. Sollte die Timeout Strategy auslösen, so werden die letzten $P$ Erfahrungen entsprechend angepasst und in Mem2 verschoben.\\
Als maßgebliche Kriterien zur Evaluation der Leistung des DQN wurde die Performance, daher die Anzahl an gefressenen Äpfeln und die steps survived, also die überlebten Schritte herangezogen.

\subsection{Diskussion} \label{sec:Paper_1_Diskussion}
Sowohl die Training Gap also auch Dual Experience Replay und Timeout Strategy stellen vielversprechende Optimierungen dar, welche, auf experimentellen Resultaten basierend, gute Ergebnisse erzielen konnten. Jedoch ist auch Kritik am Paper anzubringen.
Das Env wird im Paper nur kurz und nicht detailliert genug vorgestellt. Aus dem Abschnitt Game Environment lässt sich jedoch schließen, dass alle Anforderungen des Env \ref{sec:Anforderungen_Env} vorhanden sind.\\
\\ Negativ anzumerken ist jedoch, dass die Verfasser keinerlei Vergleich mit anderen Algorithmus-Arten vorgesehen haben. Auch die Optimierungen darunter Dual Experience Replay, Training Gap und Timeout Strategy sind nicht direkt auf den PPO-Algorithmus anwendbar. 
Zwar sind die Funktionalitäten des im Paper verwendeten DQN gegeben \ref{sec:Anforderungen_funktionalität_Env}, jedoch wurde nicht auf eine Parametrisierung \ref{sec:Anforderungen_Parametrisierung} geachtet. Es wird nur ein unoptimierter DQN Agent und ein optimierter DQN Agent betrachtet. Diese unterscheiden sich nur durch die Optimierungen, andere Kriterien wurden nicht berücksichtigt.\\
\\Auch bei der statistischen Datenerhebung existieren Abweichungen zu den Anforderungen in \ref{sec:Anforderungen_an_die_Datenerhebung}. Zhepei Wei et al. verzichteten auf eine mehrfache Datenerhebung, stattdessen markierten sie ihre Ergebnisse als experimentell. Dem Leser werden des Weiteren keine Informationen über die erhobenen Daten direkt mitgeteilt. Aus Statistiken lässt sich jedoch schließen, dass Scores und Spielzeiten gespeichert wurden.\\
\\Weiterhin ist erwähnenswert, dass die Evaluationskriterien sich einzig auf die Performance und Spielzeit beschränken. Diese wird dabei am Score gemessen.\\
Eine weitere Betrachtung, beispielsweise der Robustheit oder Trainingszeit \ref{tab:Kriterien}, wird vernachlässigt. Dies soll in dieser Ausarbeitung jedoch geschehen, da diese Faktoren ebenfalls wichtig für die voll umfassende Bewertung der Agenten sind, besonders in Real World Applications \ref{sec:Motivation}.


\section{UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene} \label{sec:Paper_2}
In der folgenden Auseinandersetzung wird sich auf \cite{UAV} bezogen.
Die Arbeit UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene wurde von Chunxue Wu et al. verfasst und am 5. August 2019 veröffentlicht. Dabei wird das Spiel Snake als ein dynamisches Pathfinding Problem interpretiert, auf dessen Basis unbemannte Drohnen in Katastrophensituationen zum Einsatz kommen sollen. 
Auch in diesem Paper verwenden die Autoren Optimierungen, um den Lernerfolg zu steigern.\\
\\Einer dieser Optimierungen wurde auf Basis des Geruchssinnes konzipiert. Der Odor Effect erzeugt um den Apfel drei aneinanderliegende Gebiete, in welchen ein größerer Reward zurückgegeben wird als außerhalb der Gebiete. Dabei unterscheiden sich diese in der Höhe des Rewards, sodass der dritte den geringsten und der erste den größten Reward von allen zurückgibt ($r_1 > r_2 > r_3$, wobei $r_x$ der Reward des x-ten Kreises darstellt). Diese Zonen stellen den zunehmenden Duft von Nahrung dar, wobei dieser immer stärker wird, um so näher man sich der Quelle nährt.\\
Eine weitere Optimierungsstrategie basiert auf dem Loop Storm Effect, welcher das Verhalten beschreibt, um den Apfel zu laufen. Die Verfasser haben festgestellt, dass dieser Effekt zu einem schlechten Lernerfolg und zu langen Trainingsdauern führt. Darum haben Wu et al. einen dynamischen Positionsspeicher konzipiert, welcher Loops erkennt und diese durch das Zurückgeben einer Zufallsaktion, welche nicht auf dem Loop liegt, unterbricht. Experimentelle Ergebnisse des Papers zeigen, dass der Loop Storm Effect kaum mehr präsent ist.
\\Auf Basis einer Standard Reward Funktion, welche für das Essen eines Apfels +100 und für das Sterben -100 zurückgibt, wurden ein Versuch durchgeführt. Dem Agenten war es nicht möglich gegen die optimale Lösung zu konvergieren, aufgrund von Loop Storms und einer unzureichende Reward Funktion. Hingegen war es dem Agenten mit dem Odor Effect und der Breakout Loop Storm Strategy möglich, eine Konvergenz nach 8 Millionen Epochs (hier Trainingsdurchläufe) zu verzeichne.\\
Auch in diesem Paper bleibt die Performance maßgeblicher Evaluationsfaktor, wobei der Fokus auf den erreichten Reward gelegt wurde, welcher stark mit der Performance korreliert.

\subsection{Diskussion}\label{sec:Paper_2_Diskussion}
Auch dieses Paper besitzt interessante Verbesserungen, welche nach den ersten Ergebnissen gute Resultate vorweist. Dennoch sind auch bei diesem viele der in Kapitel drei gestellten Anforderungen \ref{chap:Anforderungen} nicht beachtet oder erfüllt worden.\\
\\Aus Graphiken geht hervor, dass die Verfasser ein Env mit einer Visualisierung besitzen. Weiterhin ist davon auszugehen, dass auch alle weiteren Anforderungen bezüglich des Env \ref{sec:Anforderungen_an_die_Evaluation} erfüllt worden sind, da ansonsten kein Training eines Agenten möglich wäre. Dennoch wurde der Leser nur unzureichend mit Details über das Env informiert.\\
\\Das Paper legt seinen Schwerpunkt deutlich mehr auf die Grundlagen des RL, wie z.B. auf den Markov decision process und die RL Kernbegriffe \ref{sec:Vokabular}.\\
\\Auch erfüllt die Ausarbeitung alle Anforderungen an die Funktionalitäten von Agenten \ref{sec:Agent_Funktionalitäten}. Dennoch wurde auch hier wenig Aufmerksamkeit in die Parametrisierung der Agenten investiert. Zuzüglich missachtet dieses Paper ebenfalls die Algorithmus Diversität. Zwar werden gegen Ende einige Vergleich zu einer Hand voll anderer Agenten getätigt, jedoch wird dies nur sehr rudimentär durchgeführt. Dem Leser bleiben sowohl die genauen Anforderungen als auch die Rahmenbedingungen verwehrt.\\
\\Wie auch im ersten Paper setzen die Verfasser nicht auf eine mehrfache Datenerhebung für die statistische Auswertung. Auch wird nicht erwähnt was für Daten im Verlauf des Trainings und es Spielens gespeichert werden. Einig die gezeigten Statistiken weisen jedoch darauf hin, dass der Score sowie der mittlere Aktionswert (Q-Value) gespeichert werden. Zuzüglich werden, für die Optimierungen, die steps und das Auftreten von Loop Storms gespeichert.\\
\\ Chunxue Wu et al. verwendeten zudem nur den Score als hauptsächlichen Evaluationskriterium. Um die Effizient der im Paper verwendeten Strategien zu zeigen wurden ebenfalls die gemittelten Steps pro Spiel und das Auftreten von Loop Stoms als weitere untergeordnete Evaluationskriterien verwendet. Diese wirken jedoch aufgesetzt und ideal gewählt, um die Leistungsfähigkeit der Optimierungen zu zeigen.

\section{Zusammenfassung}
Sowohl ''Autonomous Agents in Snake Game via Deep Reinforcement Learning'' \ref{sec:Paper_1} als auch ''UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene'' \ref{sec:Paper_2} bieten einige Optimierungsstrategien, welche im weiteren Verlauf dieser Ausarbeitung angewendet werden sollen. Damit die Optimierungen auf beide Agenten Arten (DQN und PPO) angewendet werden können, müssen sie jedoch an die Algorithmus-Arten angepasst werden.\\
Des Weiteren ist zu bemerken, dass beide Paper nur sehr eingeschränkt die in Kapitel drei gewählten Anforderungen erfüllen. Besonders auffällig ist dabei die geringe Algorithmus Diversität. Es wurden hauptsächlich DQN verwendet und andere Algorithmen wurden entweder nur unzureichend in einen Vergleich eingebunden oder vollkommen ausgelassen.
