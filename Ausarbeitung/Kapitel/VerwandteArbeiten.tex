\chapter{Verwandte Arbeiten} \label{chap:Verwandte_Arbeiten}
In diesem Kapitel soll es thematisch über den momentanen Stand der bereits durchgeführten Forschung gehen. Dabei sollen drei ausgewählte Arbeiten vorgestellt und diskutiert werden. Diese wurden aufgrund ihres thematischen Hintergrundes zu dem Spiel Snake, in Verbindung mit dem RL, ausgewählt.\\
Besonders folgende Aspekte soll bei der Vorstellung und Diskussion thematisiert werden.
\begin{itemize}
	\item Auswahlprozess der Agenten Art
	\item Wahl der Hyperparameter
	\item Netzsturktur
	\item Observation
	\item Reward Function
	\item Leistungsmessung
	\item andere Besonderheiten
\end{itemize}

\section{Autonomous Agents in Snake Game via Deep Reinforcement Learning} \label{sec:Paper_1}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{Autonomous_Agents_in_Snake_Game_via_DRL} bezogen.
In der Arbeit Autonomous Agents in Snake Game via Deep Reinforcement Learning wurden mehrere Optimierungen an einem DQN Agenten durchgeführt, um mit diesen eine größere Performance beim Sammeln von Äpfeln im Spiel Snake zu erzielen. Die Arbeitet wurde von Zhepei Wei et al., welche an verschiedenen Universitäten und Forschungsinstituten, wie z.B. College of Computer Science and Technology Jilin University (Changchun, China) und die Science and Engineering Nanyang Technological University (Singapore) arbeiten, verfasst und im Jahr 2018 veröffentlicht.

\subsection{Vorstellung}
Thematisch werden zwei Optimierungen in diesem Paper vorgestellt, welche auf einen Baseline DQN (Referenz DQN) angewendet worden sind.
Die Verfasser setzen dabei als Obs. \ref{sec:Observation} Screenshot des Spiels ein, um den Agenten mit Informationen über das Spiel zu versorgen. Diese werden mittels eines Convolutional Neural Network (CNN) verarbeitet, dass auf 3 Conv. und einem Linear Layer besteht.\\
Die vom Paper verwendete Reward-Funktion basiert auf drei Faktoren, wobei der erste die Distanz ist. Um so weiter sich die Snake von dem Apfel entfernt, um so größer (negativ) wird der Reward. Nährt sich hingegen die Snake dem Apfel an, so wird der Reward immer größer (positiv).\\
Der zweite Faktor ist der sogenannte Training Gap. Dieser stellt Erfahrungen dar, welche im Verlauf des Spiels nicht erlernt werden soll, wie z.B. die Erfahrungen direkt nach dem Fressen eines Apfels. Der Agent würde durch diese Erfahrungen lernen, wie das Spiel die Äpfel zufallsbasiert neu verteilt, was nicht dem Trainingsziel entspricht. Das Entfernen dieser Erfahrungen führt zu einem besseren Pathfinding.\\
Der dritte Faktor ist die Timeout Strategy. Diese sorgt für eine Bestrafung, wenn der Agent über eine gewissen Anzahl an Schritten $P$ keinen Apfel gefressen hat. Der Agent wird dadurch angehalten die schnellste Route zu finden. Dabei werden die Rewards \ref{sec:Reward} der letzten $P$ Schritte mit einem Malus verrechnet, der sich nach der Länge der Snake richtet.
Insgesamt setzt sich die Reward-Funktion nun folgendermaßen zusammen:\\
Der Distance Reward bildet den Basis-Reward. Sollte sich das Spiel im Training Gap befinden, so wird der Reward des Training Gap gewählt. Überschreitet der Agent des weiteren die Granze von $P$ Schritten ohne einen Apfel gefressen zu haben, wird der Timeout Strategy Reward Malus auf die letzten $P$ Schritte aufaddiert.\\
\\Des Weiteren stellt das Paper eine neues Design von Experience Replay Buffer vor. Hierbei wird der Experience Replay Buffer in zwei Hälften geteilt, in $MP_{1}$ und $MP_{2}$. Der Unterschied zwischen beiden besteht darin, dass in $MP_{1}$ nur Erfahrungen mit einem Reward $r > 0.5$ gespeichert werden. Alle anderen Erfahrungen werden in $MP_{2}$ gespeichert.\\
Ähnlich der $\epsilon$-greedy Strategie des DQN, wird das Verhältnis zwischen den Erfahrungen von $MP_{1}$ und $MP_{2}$ mit einer Konstante $\eta$ gesteuert. Zu Beginn liegt diese bei $\eta = 0.8$, es werden daher durchschnittlich 80\% Erfahrungen aus $MP_{1}$ und 20\% aus $MP_{2}$ gewählt. Durch eine stetige Abnahme fällt $\eta$ gegen ende der Abnahme auf minimal $\eta = 0.5$.
Durch diese Aufteilung soll ein schnellerer Lernerfolg gerade zu Beginn erzielt werden.

\subsection{Diskussion}
Wie bereits oben erwähnt, befasst sich die Arbeit mit zwei Optimierungsstrategien, welche nach experimentellen Experimenten bereits gute Resultate erzielt haben.\\
Jedoch bietet das Paper auch einiges Diskussionspotential. So verzichtet man auf einen differenzierten Auswahlprozess der Art der RL-Agenten (DQN, PPO oder A2C). Es wird einfach auf einen DQN vertraut \cite[Kapitel 1]{Autonomous_Agents_in_Snake_Game_via_DRL}.\\
Auch wirft die verwendete Obs. Fragen auf. So werden lediglich Screenshot des Spiels als Obs. verwendet. Eine weitere Methode, um dem Agenten Informationen über das Spiel zukommen zu lassen wird nicht verwendet. Des Weiteren könnte das Verwenden von Screenshot des ganzen Spiels zu schlechteren Lernerfolg führen, da auch viele nicht benötigte Informationen dem Agenten übergeben werden.\\ 
Die Netzstruktur könnte diesen Effekt noch weiter verstärken, da sie aus drei Conv. Layern mit großen Filtergrößen (7x7, 5x5, 3x3) und Strides (4,2,2) besteht. Ein möglicher Informationsverlust wie auch eine Verlängerung der Lernzeit könnte durch die gewählte Konfiguration begünstigt werden.\\

\section{Exploration of Reinforcement Learning to SNAKE} \label{sec:Paper_2}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{Exploration_of_Reinforcement_Learning_to_SNAKE} bezogen.
In der Arbeit Exploration of Reinforcement Learning to Snake stellen die Verfasser Bowei Ma, Meng Tang, Jun Zhang einen Vergleich zweier RL-Agenten vor. Die Arbeit war ein Ergebnis einer Projektgruppe der Universität Stanford und wurde im Jahres 2016 veröffentlicht.

\subsection{Vorstellung}
Wie bereits erwähnt, wird in dem Paper der Vergleich zweier RL-Agenten vorgestellt. Dabei handelt es sich um einen DQN und um einen SARSA (State-Action-Reward-State-Action). Beide Agenten arbeiten auf Basis einer Value Function, sie sind daher value-based \ref{sec:RL_policy_value}. Nach einer kurzen Vorstellung der beiden Agenten und eines weiteren, auf einer Heuristik basierenden, optimalen Agenten, welcher als Referenz dient, präsentiert das Paper direkt seine Ergebnisse. So waren weder der DQN als auch der SARSA in der Lage das optimale Ergebnis für die entsprechende Spielfeldgröße. Die Agenten lernten dabei zumeist unter festen Hyperparametern, welche durch Ausprobieren ermittelt wurden. Bei dem Vergleich der beiden Agenten wurde sich zudem nur auf die Performance konzentriert.\\
Des Weiteren wurden einige Auffälligkeiten erwähnt, wie z.B. das Lernverhalten der beiden RL-Agenten. So lernte der DQN bei kleineren Durchläufen schneller als der SARSA. Dieses ändert sich jedoch mit Anstieg der Lernzeit. Weiterhin konnte eine starke Instabilität beim Training des DQN festgestellt werden, die unter gewissen Trainingssituationen auftrat. In dem 

\subsection{Diskussion}
Auch dieses Paper wirft einige Fragen auf, so ist beispielsweise die Wahl der beiden zu vergleichenden Agenten nicht begründet worden. Auch eine differenzierte Herangehensweise an die Wahl der Hyperparameter wird nicht erwähnt. Stattdessen wurden sie mittels einer ''trial and error'' Taktik bestimmt. 
Weiterhin ist besonders das Auslassen von Informationen über die Netzstruktur hervorzuheben. Ähnlich verhält es sich mit den Informationen über die Obs.\\
Auch wurde hauptsächlich bei dem Vergleich der beiden RL-Agenten nur die Performance verglichen. Andere Aspekte, wie beispielsweise die Robustheit, Trainingszeit oder die Spielzeit \ref{sec:Anforderungen_an_die_Evaluation}, werden nicht weiter für den Vergleich in Betracht gezogen. Die Verfasser benutzen des weiteren für das Lernen ihres Agenten eine einfach Reward Function, die +500 returned, wenn die Snake einen Apfel isst. Sollte die Snake sterben, so wird -100 ausgegeben. Ansonsten wird für jeden Step -10 veranschlagt. zwar ist die Reward Function effektiv aber es bleibt auch viel Potential ungenutzt. Eine effektive formulierte Function kann den Lernerfolg und die Lernzeit drastisch steigern.

\section{UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene} \label{sec:Paper_3}
In der folgenden Auseinandersetzung wird sich auf die Quelle \cite{UAV} bezogen.
Die Arbeit UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene wurde von Chunxue Wu et al. verfasst und am 5. August 2019 veröffentlicht. Thematisch wird das Spiel Snake als ein dynamisches Pathfinding Problem interpretiert, auf dessen Basis unbemannte Drohnen gesteuert werden sollen, welche in Katastrophensituationen zum Einsatz kommen sollen.

\subsection{Vorstellung}
Das hier vorliegende Paper behandelt viele Aspekt. So haben sich die Autoren das Ziel gesetzt, einen Agenten zu erstellen, der so viele Snake Spiele wie möglich spielen kann. Die Verfasser setzen dabei auf eine ausgedehnte Netzstruktur, welche aus mehreren Convolutional, Pooling und Linear Layers besteht.\\
Wie auch bei den \ref{sec:Paper_1}, verwendet die Autoren Screenshot des gesamten Spiels, um die Obs. aufzuspannen.\\
Auch bei der Reward Function bestehen gewisse Ähnlichkeiten zu \ref{sec:Paper_1}. So wird eine Reward Function benutzt, welche auf einer Duftspur um den Apfel basiert. Implementiert wird diese durch einen festen Bereich um den Apfel, der einen größeren Reward returned als die allgemeine Umgebung. Dieser Bereich ist wiederum in drei weitere Ringe eingeteilt, sodass der Reward des ersten Rings größer als der des zweiten ist und der Reward des zweiten größer als der des dritten. Vergleichbar ist dies durch einen Berg, der in drei Höhenbereiche eingeteilt ist. Auf der Spitze befindet sich der Apfel. Der Reward liegt dabei höher, wenn man in einen höheren Höhenbereiche befindet. Entsprechendes gilt für tiefere Bereiche.\\
Ebenfalls erwähnt das Paper die in Teilen die Wahl der Hyperparameter des DQN. So wird z.B. $\epsilon$ nicht mit $\epsilon= 1.00$ initialisiert sondern mit $\epsilon=0.1$. Dieser Wert wird in den nächsten 10.000.000 Zügen auf $\epsilon=0.0001$ gesenkt.\\
Auch begründet das Paper die Wahl eines DQN, so ist dieser deutlich besser geeignet als ein Q-Table, da dieser bei großen State-Action-Räumen eine große Menge an Ressourcen bindet.\\
Eine weitere Besonderheit die die sogenannte Loop Strom Strategy. So kann es bei der verwendeten Reward Function passieren, dass der Agent einen größeren Reward erhält, indem er immer um den Apfel sich in Kreisen (Loops) umherbewegt. Offensichtlich leidet die Performance darunter, da kaum Äpfel gefressen werden. Daher haben Chunxue Wu et al. eine Erkennungsmethode von Loops, welche auf einem dynamischen Array basiert, entwickelt. Sollte diese erkennen, dass sich der Agent in einem Loop befindet, so sieht der Agent von einer Aktionsbestimmung via. NN ab und nutze eine Zufallsentscheidung, um den Loop zu verlassen.\\
Gegen Ende des Papers erfolge die Evaluation des DQN. Diese bezieht nur den erreichten Score ein, welcher sich über alle erhaltenden Rewards einer Spielepisode zusammensetzt. Verglichen werden diese Scores mit denen anderer Algorithmen, welche im Paper nicht weiter thematisiert werden und mit den Leistungen, welche von Menschen erbracht worden sind.

\subsection{Diskussion}
Von allen bereits diskutierten Papers stellt dieses das umfangreichste dar. Die Autoren haben viele Aspekte mit in die Bearbeitung einfließen lassen. Dies stellt sich jedoch auch als ein möglicher Nachteil dar. Zwar zeigen erste Auswertungen, dass Teile der Arbeit, wie z.B. die Reward Function und die Loop Storm Strategy, die Performance verbessern, jedoch wurden dafür auch an anderen Stellen Abstriche gemacht. So fußt die Auswahl des Agenten, auf einem Vergleich mit einem Q-Table. Dieser Vergleich ist jedoch nicht sonderlich aussagekräftig, da Q-Table nur sehr eingeschränkt benutzt werden können, da sie bei großen State-Action-Räumen keine guten Ergebnisse erzielen. Ein Vergleich zu anderen konkurrierenden RL-Agenten Arten wurde nicht durchgeführt.\\
Auch die Wahl der Netzstruktur und der damit einhergehenden Obs. eröffnet Kritikpunkte. So wird zwar ein großes ConvNet verwendet, welches jedoch wieder das gesamte Spielfeld überblicken muss. Wie auch schon in \ref{sec:Paper_1} angesprochen, kann dies zu Informationsverlust führen. Auch ist die Strategie alle 0.03 Sekunden einen Screenshot zu erstellen und diesen dann durch das NN zu propagieren fragliche, da es zwar in real-world Anwendungen natürlich Anwendung findet wird in einem Snake Env. mit diskreten Gitterstrukturen eher wenig angepasst wirkt.\\
Zwar haben sich die Verfasser mehrere Gedanken zur Wahl der Hyperparameter gemacht, diese jedoch nur selten begründet. So wird der Parameter $\epsilon$ zwar genaustens beleuchtet, jedoch die ungewöhnlich niedrige Lernrate von $1\mathrm{e}{-6}$ kaum erwähnt.\\
Wie auch schon bei den anderen Papers wird sich bei der Evaluation hauptsächlich nur auf einen bis zwei Parameter beschränkt. Sowohl der Score als auch die Q-Values werden genauer beleuchtet. Andere Evaluationsparameter, wie die Spielzeit, Robustheit usw. werden nicht mit eingebracht.\\
Dennoch kann das Paper mit seiner Loop Storm Strategy und Reward Function überzeugen.

\section{Zusammenfassung}
Zusammenfassend lässt sich sagen, dass die vorgestellten Arbeiten alle ihr Qualitäten besitzen. Das Paper ''Autonomous Agents in Snake Game via Deep Reinforcement Learning'' \ref{sec:Paper_1} wie auch das Paper ''UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene'' \ref{sec:Paper_3} verwendeten beispielsweise effizientere und kreativere Reward Functions im Vergleich zum Standard Belohnung nur für das Essen eines Apfels; Bestrafung für das Sterben; weder Belohnung noch Bestrafung für das Umherlaufen). Auch konnten die beiden Papers mit neuen Ansätzen wie der Loop Strom Strategy und der Training Gap Strategy überzeugen.\\
Das Paper ''Exploration of Reinforcement Learning to SNAKE'' geht diesbezüglich noch einen Schritt weiter und beschäftigt sich vollkommen mit einem Vergleich von RL-Agenten.\\
\\ Dennoch lässt sich an den Papers auch Kritik üben. So konnte keines mit einer adäquaten Begründung aller der oben genannten Faktoren, wie z.B. Netzstruktur, Leistungsmessung usw., überzeugen. Besonders die Leistungsmessung, Wahl der Agenten Art und die Wahl der Netzstruktur fielen bei den Arbeiten besonders unzureichend aus. Häufig wurde mehr Fokus auf spezifische Besonderheiten und auf die Reward Function gelegt.
