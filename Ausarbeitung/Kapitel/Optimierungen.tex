\chapter{Optimierungen} \label{chap:Optimierungen}
In diesem Kapitel werden die anzuwendenden Optimierungen vorgestellt und erklärt, welche nach dem Basis Vergleich die Leistung in den einzelnen Evaluationskategorien noch weiter verstärken soll. Zu diesem Zweck sollen vier Optimierungen auf die Baseline Agenten (Agenten ohne Optimierungen) angewendet werden, welche aus den verwandten Arbeiten \ref{chap:Verwandte_Arbeiten} und eigenen Ideen stammten.

\section{Optimierung 1 - Dual Experience Replay}
Die Idee für Dual Experience Replay oder auch hier Splited Memory genannt, stammt aus der Arbeit "`Autonomous Agents in Snake Game via Deep Reinforcement Learning"' und wurde in \ref{sec:Paper_1} bereits analysiert und diskutiert.\\
Diese Optimierung zielt darauf ab, den Replay Baffer (Memory) zu zweiteilen, sodass Mem1 und Mem2 entstehen. In Mem1 werden ausschließlich Erfahrungen gespeichert, welche einen Reward vorweisen können, der größer als ein vordefinierter Grenzwert ist. In Mem2 werden alle übrigen Erfahrungen gespeichert. Diese Aufteilung zielt darauf ab, dass zu beginn ein größerer Anteil an guten Erfahrungen für das Lernen verwendet wird, um das Lerntempo zu erhöhen. Den Verfassern schwebt ein Verhältnis von (80\% guten und 20 \% schlechteren Erfahrungen vor). Dieses Verhältnis normalisiert sich über die Trainingszeit, daher zum Verhältnis von (50\% zu 50\%).\\
Wie bereits in \ref{sec:Paper_1} angesprochen, ist die erfahrungsorientierte Aufteilung schlecht bis gar nicht umsetzbar, da die Rewards des PPO im Nachhinein noch diskontiert werden müssen. Dafür muss die komplette Episode an Erfahrungen in ihrer ursprünglichen Reihenfolge vorhanden sein, was nach dem Sortieren nicht der Fall wäre. Daher scheidet ein Sortieren Rewards aus.\\
Alternativ wurde vorgeschlagen, dass die Sortierung nicht belohnungsbasiert, sondern Episoden basiert geschehen kann. Es werden daher die besten Episoden, gemessen an ihren Scores, in die Memories einsortiert.\\
\\ Zu diesem Zweck wird eine weitere Klasse SplitedMemory in den Memory Files des DQN und PPO definiert, welchen diese beschriebene Funktionalität bereitstellt.