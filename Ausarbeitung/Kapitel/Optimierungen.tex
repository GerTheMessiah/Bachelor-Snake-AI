\chapter{Optimierungen} \label{chap:Optimierungen}
In diesem Kapitel werden die anzuwendenden Optimierungen vorgestellt und erklärt, welche nach dem Basis Vergleich die Leistung in den einzelnen Evaluationskategorien noch weiter verstärken soll. Zu diesem Zweck sollen vier Optimierungen auf die Baseline Agenten (Agenten ohne Optimierungen) angewendet werden, welche aus den verwandten Arbeiten \ref{chap:Verwandte_Arbeiten} und eigenen Ideen stammten.

\section{Optimierung 1 - Dual Experience Replay}
Die Idee für Dual Experience Replay oder auch hier Splited Memory genannt, stammt aus der Arbeit "`Autonomous Agents in Snake Game via Deep Reinforcement Learning"' und wurde in \ref{sec:Paper_1} bereits analysiert und diskutiert.\\
Diese Optimierung zielt darauf ab, den Replay Baffer (Memory) zu zweiteilen, sodass Mem1 und Mem2 entstehen. In Mem1 werden ausschließlich Erfahrungen gespeichert, welche einen Reward vorweisen können, der größer als ein vordefinierter Grenzwert ist. In Mem2 werden alle übrigen Erfahrungen gespeichert. Diese Aufteilung zielt darauf ab, dass zu beginn ein größerer Anteil an guten Erfahrungen für das Lernen verwendet wird, um das Lerntempo zu erhöhen. Den Verfassern schwebt ein Verhältnis von (80\% guten und 20 \% schlechteren Erfahrungen vor). Dieses Verhältnis normalisiert sich über die Trainingszeit, daher zum Verhältnis von (50\% zu 50\%).\\
Wie bereits in \ref{sec:Paper_1} angesprochen, ist die erfahrungsorientierte Aufteilung schlecht bis gar nicht umsetzbar, da die Rewards des PPO im Nachhinein noch diskontiert werden müssen. Dafür muss die komplette Episode an Erfahrungen in ihrer ursprünglichen Reihenfolge vorhanden sein, was nach dem Sortieren nicht der Fall wäre. Daher scheidet ein Sortieren Rewards aus.\\
Alternativ wurde vorgeschlagen, dass die Sortierung nicht belohnungsbasiert, sondern Episoden basiert geschehen kann. Es werden daher die besten Episoden, gemessen an ihren Scores, in die Memories einsortiert.\\
\\ Zu diesem Zweck wird eine weitere Klasse SplitMemory in den Memory Files des DQN und PPO definiert, welchen diese beschriebene Funktionalität bereitstellt. Diese enthält drei Instanzen der ursprünglichen Memory-Klasse, Mem1, Mem2 und ein temporären Memory. In letzteren werden die Erfahrungen temporär eingelagert bis die Episode terminiert ist. Dann wird anhand des gesamt Scores der Episode die Zuteilung in Mem1 oder Mem2 erfolgen.\\
Beim DQN Memory wird zusätzlich darauf geachtet, dass beim Einfügen einer ganzen Episode an Erfahrungen die Ring-Buffer Eigenschaften des Memory erhalten bleiben. Beim Memory des PPO ist dies nicht nötig.

\section{Optimierung 2 - Joined Reward Function}
Die Joined Reward Function wurde im Paper "`Autonomous Agents in Snake Game via Deep Reinforcement Learning"' vorgestellt und in \ref{sec:Paper_1} erklärt. Sie setzt sich aus drei Teilen zusammen. Die Basis bildet ein Distanz Reward, welcher dessen Höhe antiproportionale zur Distanz ist. Um unerwünschte Lerneffekte, von beispielsweise der Neuerzeugung eines Apfels, zu verhindern wird werden diese Erfahrungen aus dem Memory gelöscht, was die Training Gap Strategie und den zweiten teil der Reward Function darstellt. Zur Verstärkung des Pathfindings wird die Timeout Strategy angewendet, welche den Agenten für nicht zielgerichtetes Verhalten, wie z.B. das unnötige Umherlaufen, bestraft.\\
\\ Die Implementierung dieser neuen Reward Function findet in der SnakeGame Klasse im snake\_game File statt. Dort wird die evaluate Methode erweitert, sodass das Wechseln zwischen verschiedenen Reward Functions ermöglicht wird. Dabei stellt die die Distanz Reward Function wie folgt dar:
\begin{align}
	r_{distanz}(dis, len, size) = \frac{10}{dis} \times \frac{len}{size}
\end{align}
$dis$ stellt die Distanz gemessen mit der Euklidischen Norm dar, $len$ ist die Länge der Snake und $size$ ist die Größe des Spielfeldes (bei einer Spielfeldform von 8x8 ergibt sich einer Größe von 64).\\ Da die Training Gap Strategy zu Problemen beim PPO führen würde, wird diese nicht beachtet. Die Timeout Strategy wird in abgewandelter Form, wie in \ref{sec:Paper_1_Diskussion} diskutiert, implementiert. Dies geschieht nach folgender Formel:
\begin{align}
	r_{timeout}(steps, len) = \frac{1}{100} \times \frac{steps}{len}
\end{align}
$steps$ beschreibt die Anzahl an Schritten die seit dem letzten Konsum eines Apfels gelaufen worden sind. 
Die Joined Reward Function lautet daher: $r_{res}(dis, len, size, steps) = 	r_{distanz}(dis, len, size) - r_{timeout}(steps, len)$.

\section{Optimierung 3 - Odor Reward Function \& Loop Storm Strategy}
Die Odor Reward Function wurde im Paper "`UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene"' vorgestellt um in \ref{sec:Paper_2} erklärt und diskutiert. Die Odor Reward Function basiert auf dem olfaktorisch Sinn. Dabei wird um den Apfel drei Geruchszonen generiert, in welchen der Reward größer ist als in der geruchsfreien Zone. Die drei Geruchszonen unterscheiden sich in ihrer Intensität der Geruch in Zone eins $z1$ ist größer als in Zone zwei $z2$. Daher gilt für den Geruch und damit auch für den Reward $z1 > z2 > z3 > \text{außerhalb der Zonen}$.
Da RL Agenten nach immer größer werdenden Rewards sterben besteht eine große Gefahr, dass der Agent um den Apfel rotiert, um so einen größeren Reward in dieser Episode zu erreichen. Um diesen sogenannte "`Loop Storm Effect"'  zu vermeiden, setzten die Verfasser auf eine dynamischer Array welches Loops erkennt und bei Eintreten eine zufällige Action auswählt um den Loop zu unterbrechen.\\
\\Auch die Odor Reward Function wird in der SnakeGame-Klasse, welche sich im snake\_game File befindet, implementiert. Aufgrund der geringen Größe des Standard Spielfeldes (8x8), werden nur zwei Zonen mit einer jeweiligen Breite von einem Kästchen verwendet. So soll ein Reward von 0,5 in der zweiten und ein Reward von 1,5 in der ersten Zone zurückgegeben werden. Die loop Strom Strategy wird wie in \ref{sec:Paper_2_Diskussion} implementiert. Es werden daher die Schritte in der Zone gezählt und bei Übersteigen der Anzahl an Kästchen in den Zone, wird eine zufällige Actions ausgeführt, welche den Loop unterbricht. Diese Erweiterung wird in der Aktionswahlprozedur, welche in \ref{sec:Impl_DQN_Aktionsauswahlprozess} und \ref{sec:Impl_PPO_Aktionsauswahlprozess} dargestellt sind, implementiert.

\section{Optimierung 4 - Dynamische Lernrate}
Die vierte Optimierung versucht das Lernen des Agenten durch das stetige Anpassen der Lernrate (LR) zu verbessern. Mit Hilfe eines vom PyTorch bereitgestellten Schedulers lässt sich die Lernrate während des Trainingsprozess manipulieren. Dabei wird diese Optimierung in der main Methode implementiert.\\
Die Lernrate wird immer dann mit 0.95 multipliziert und als neue LR gesetzt, sobald keine Performance Steigerung in den lernen 200 Schritten erfolgt ist. 


