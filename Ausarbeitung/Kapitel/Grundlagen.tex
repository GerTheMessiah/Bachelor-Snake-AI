
\chapter{Grundlagen}\label{chap:Grundlagen}
Im folgenden Kapitel soll das, zu benötigende, Wissen vermittelt werden. Dazu sollen verschiedene RL Algorithmen, wie auch grundlegende Informationen des RL selbst, thematisiert werden. Zuerst wird sich jedoch dem Spiel Snake beschäftigt. 

\section{Game of Snake} \label{sec:Grundlagen_Game_of_Snake}
\begin{figure}[H]
	\centering
	\includesvg[scale=0.90]{Game_of_Snake}
	\caption[Game of Snake]{Game of Snake - Abbildung eines Snake Spiels, in welchem der Apfel durch das rote und der Snake Kopf durch das dunkelgrüne Quadrat dargestellt wird. Die hellgrünen Quadrate stellen den Schwanz der Snake dar.}
	\label{fig:Grundlagen_Game_of_Snake}
\end{figure}
Snake zählt zu den bekanntesten Computerspielen unserer Zeit. Es zeichnet sich durch sein simples und einfach zu verstehendes Spielprinzip aus.
Das komplette Spielfeld wird durch eine zweidimensionale Ebene beschreibe, in welcher sich die Snake bewegt. Häufig wird diese als einfacher grüner Punkt (Quadrat) dargestellt. Dieser stellt den Kopf der Snake dar. Neben diesem befindet sich auf dem Spielfeld auch noch der Apfel. Dieser wird häufig als roter Punkt (Quadrat) dargestellt \fullref{fig:Grundlagen_Game_of_Snake}.
Ziel der Snake ist es Äpfel zu fressen, indem der Kopf der Snake auf das Feld des Apfels läuft. Danach verschwindet der Apfel und ein neuer erscheint in einem zufälligen freien Feld. Außerdem wächst, durch das Essen des Apfels, die Snake um ein Schwanzglied. Diese Glieder folgen dabei, in ihren Bewegungen, den vorangegangenen Schwanzglied bis hin zum Kopf. Dem Spieler ist es nur möglich den Kopf der Snake zu steuern.
Dieser ist es nicht erlaubt die Wände oder sich selbst zu berühren. Geschieht dies jedoch im Laufe des Spiels, so wird dieses sofort beendet. Diese Einschränkung führt zu einem Anstieg der Komplexität gegen Ende des Spiels. 
Ein Spiel gilt als gewonnen, wenn es der Snake gelungen ist das komplette Spielfeld auszufüllen.\\
Nach der Beleuchtung des Spiels Snake, folgen im Weiteren die Grundlagen des Reinforcement Learnings.

\section{Reinforcement Learning} \label{sec:Grundlagen_RL}
Das Reinforcement Learning (RL) ist einer der drei großen Teilbereiche, des Machine Learnings.
Auf Menschen wirkt das RL, im Vergleich zu den anderen Disziplinen des Machine Learnings, am nachvollziehbarsten.
Dies liegt an der Lernstrategie, welche Anwendung findet. 
Beim RL wird mithilfe eines "`trial-and-error"' Verfahrens gelernt. Ein gutes Beispiel für eine solche Art des Lernens ist die Erziehung eines Kindes. Wenn das Kind etwas Gutes tut, dann wird es belohnt. Angetrieben von der Belohnung, versucht dieses das Verhalten fortzusetzen. 
Entsprechend wird das Kind bestraft, wenn es etwas Schlechtes tut, sodass schlechtere Verhaltensweisen weniger häufig zum Vorschein kommen, um Bestrafungen zu entgehen. \citep[S.1 ff.]{Sutton1998}\\
Beim RL funktioniert es ganz ähnlich, was auch der Grund dafür ist, dass viele der Aufgaben des RL dem menschlichen Arbeitsspektrum sehr nahe sind. 
Deshalb findet das RL auch immer mehr Anwendung, im Finanzhandel, in der Robotik, im autonomen Fahren usw. \citep[Kapitel 18]{DRL_Lapan}

\subsection{Vokabular} \label{subsec:Grundlagen_Vokabular}
Um ein tiefer gehendes Verständnis für das RL zu erhalten, müssen zuerst die Begrifflichkeiten erlernt und deren Bedeutung verstanden werden.

\subsubsection{Agent} \label{subsubsec:Grundlagen_Agent}
Im Zusammenhang mit dem RL, ist häufig die Rede von Agenten. Sie sind zentrale Instanzen, welche die RL Algorithmen in ein festes Objekt einbinden. Dabei werden Methoden, Hyperparameter und das NN in die Agenten eingebunden. \citep[S. 31]{DRL_Lapan}
Bei den Agenten handelt es sich gewöhnlich um die einzigen Instanzen, welche mit dem Environment (der Umgebung) interagieren. Zu diesen Interaktionen zählen das Entgegennehmen von Observations und Rewards, wie auch das Übergeben von Aktions. \citep[S. 2ff.]{Sutton1998}

\subsubsection{Environment} \label{subsubsec:Grundlagen_Environment}
Das Environment (Env) bzw. die  Umgebung ist vollständig außerhalb des Agenten angesiedelt. Es spannt das zu manipulierende Umfeld auf, in welchem der Agent Interaktionen tätigen kann. An ein Environment werden unter anderem verschiedene Ansprüche gestellt, damit ein RL Agent mit ihm in Interaktion treten kann. Zu diesen gehören die Fähigkeiten Observations und Rewards zu liefen, sowie Aktionen zu verarbeiten. \citep[S. 31 \& S.2 ff.]{DRL_Lapan, Sutton1998}

\subsubsection{Action} \label{subsubsec:Grundlagen_Action}
Die Actions bzw. Aktionen sind einer der drei Datenübermittlungswege. Bei ihnen handelt es sich um Handlungen, welche im Env ausgeführt werden. Aktionen können z.B. Züge in Spielen, das Abbiegen im autonomen Fahren oder das Ausfüllen eines Antrages sein. Es wird ersichtlich, dass die Aktionen, welche ein RL Agent ausführen kann, prinzipiell nicht in der Komplexität beschränkt sind. 
Dennoch ist es gängige Praxis geworden, dass diese möglichst einfach seinen sollen.\\
Im Environment wird zwischen diskreten und stetigen Aktionsraum unterschieden. Der diskrete Aktionsraum umfasst eine endliche Menge an sich gegenseitig ausschließenden Aktionen. Ein Beispiel dafür wäre das Gehen an einer T-Kreuzung. Der Agent kann entweder Links, Rechts oder zurück gehen.
Anders verhält es sich bei einem stetigen Aktionsraum. Dieser zeichnet sich durch stetige Werte aus. Das Steuern eines Autos wäre hierfür beispielhaft. Es stellen sich Fragen, wie z.B. um wie viel Grad muss der Agent das Steuer drehen und um wie viel Prozent muss er bremsen, damit das Fahrzeug in der Spur bleibt? \citep[S. 31 f.]{DRL_Lapan}

\subsubsection{Observation} \label{subsubsec:Grundlagen_Observation}
Die Observation (Obs) bzw. die Beobachtung ist ein weiterer Datenübermittlungsweg, welche den Agenten mit dem Env verbindet. Bei dieser handelt es sich um einen oder mehrere Vektoren bzw. Matrizen, welche dabei den momentanen Zustand des Environments repräsentieren. \citep[S. 381]{Sutton1998}\\
Je nach Anwendungsbereich, fällt die Obs sehr unterschiedlich aus. In der Finanzwelt könnte diese z.B. die neusten Börsenkurse beinhalten oder in der Welt der Spiele könnten die aktuelle Punktezahl wiedergeben werden. \citep[S. 32]{DRL_Lapan}

\subsubsection{Reward} \label{subsubsec:Grundlagen_Reward}
Der Reward bzw. die Belohnung ist der letzte Datenübertragungsweg.
Bei diesem handelt es sich um einen einfachen skalaren Wert, welche vom Env übermittelt wird. Dieser gibt an, wie gut bzw. schlecht eine ausgeführte Aktion im Env war. \citep[S. 42]{Sutton1998}\\
Um eine solche Einschätzung zu tätigen, ist es nötig eine Bewertungsfunktion zu implementieren, welche den Reward bestimmt.

\subsubsection{State} \label{subsubsec:Grundlagen_State}
Der State bzw. Zustand ist eine Widerspieglung der zum Zeitpunkt $t$ vorherrschenden Situation im Env. 
Dieser wird von der Obs (Observation) repräsentiert. Häufig findet sich der Begriff des State in der Fachliteratur und in anderen Ausarbeitungen zu diesem Themengebiet. \citep[s. 381 ff.]{Sutton1998}

\subsubsection{Policy} \label{subsubsec:Grundlagen_Policy}
Informell lässt sich die Policy als eine Menge von Regeln beschreiben, welche das Verhalten eines Agenten steuern. Formal ist die Policy $\pi$ als eine Wahrscheinlichkeitsverteilung über alle möglichen Aktionen $a$ im State $s$ des Env definiert. \citep[S. 44]{DRL_Lapan}\\
Sollte daher ein Agent der Policy $\pi_{t}$ zum Zeitpunkt $t$ folgen, so entspricht $\pi_{t}(a_t|s_t)$ der Wahrscheinlichkeit, dass die Aktion $a_t$ im State $s_t$ gewählt wird. \citep[S. 45 ff.]{Sutton1998}

\subsubsection{Value} \label{subsubsec:Grundlagen_Value}
Die Values geben eine Einschätzung ab, wie gut oder schlecht ein State bzw. State-Action-Pair ist. Sie werden gewöhnlich mit einer Value Funktion $V$ bestimmt, sodass beispielsweise $V(s)$ dem Wert des States $s$ gleichkommt. Der Value entspricht dabei der diskontierten Summe aller noch zu erwartenden Rewards in der verbleibenden Spielepisode. Damit ist dieser ein Maß dafür, wie gut es für den Agenten ist, in diesen State zu wechseln.\\
Der sogenannte Q-Value $Q(s,a)$ gibt Aufschluss darüber, welche Aktion $a$ im State $s$ den größten Return (Summe aller diskontierten Rewards in der noch verbleibenden Spielepisode) erzielen wird.\\
Die Values werden ebenfalls unter einer Policy (Regelwerk des Agenten) bestimmt, daher folgt, dass für die Value Funktionen $V_\pi(s)$ und $Q_\pi(s,a)$, wobei $\pi$ die Policy ist. \citep[S. 46]{Sutton1998}

\subsection{Funktionsweise} \label{subsec:Grundlagen_Funktionsweise}
Die folgende Beschreibung der Funktionsweise orientiert sich an der Abbildung \ref{fig:Grundlagen_Reinforcement_Learning}.
\begin{figure}[H]
	\centering
	\includesvg[scale=1.00]{Reinforcement_Learning}
	\caption[Reinforcement Learning]{Schematische Funktionsweise des Reinforcement Learning - Der Agent erhält einen state $S_{t}$ und falls 
		$t \neq 0$ einen reward $R_{t}$. Daraufhin wird vom Agenten eine action $A_{t}$ ermittelt, welche im Env ausgeführt wird. Das Env übermittelt den neuen state $S_{t+1}$ und reward $R_{t+1}$ an den Agenten. Diese Prozedur wird wiederholt. Bildquelle: \citep[S. 38]{Sutton1998}}
	\label{fig:Grundlagen_Reinforcement_Learning}
\end{figure}
Zu Beginn wird dem Agenten vom Environment der initialer State $S_{t=0}$ übermittelt, welcher einen Entscheidungsfindungsprozess anstößt. Es wird eine Aktion $A_{t}$ ermittelt, welche an das Env weiterleitet und dort ausgeführt wird.
Das manipulierte Env befindet sich nun im neuen State $S_{t+1}$, welcher an den Agenten weitergeleitet wird. Des Weiteren bestimmt und sendet das Env noch einen Reward $R_{t+1}$.
Mit dem neuen State $S_{t+1}$, kann der Agent wieder eine Aktion $A_{t+1}$ bestimmen, welche ausgeführt wird. Daraufhin werden wieder der neue State $S_{t+2}$ und Reward $R_{t+2}$ ermittelt und übertragen usw. \citep[S. 37 ff.]{Sutton1998}

\subsection{Arten von RL Verfahren} \label{subsec:Grundlagen_Arten_RL_Verfahren}
Nachdem das Basisvokabular erklärt wurde, soll nun ein tieferer Blick in die verschiedenen Arten der RL geworfen werden.\\
Alle RL Verfahren lassen sich, in Klassen einordnen, welche Aufschluss über verschiedene Details des Algorithmus geben. Es existieren natürlich viele Möglichkeiten, RL Verfahren zu klassifizieren, aber vorerst soll sich auf die folgenden zwei beschränkt werden.

\subsubsection{Policy-Based und Value-Based Verfahren} \label{subsubsec:Grundlagen_policy_value_based}
Die Einordnung in policy-based und value-based Verfahren gibt Aufschluss über den Entscheidungsfindungsprozess des RL Verfahrens.
Policy-Based Agenten versuchen unmittelbar die Policy zu berechnen, umzusetzen und sie zu optimieren. Sie besitzen dafür meist ein eigenes NN, welches die Policy $\pi$ für einen State $s$ bestimmt.
Gewöhnlich wird diese als eine Wahrscheinlichkeitsverteilung über alle Aktionen angegeben \fullref{subsubsec:Grundlagen_Policy}. Jede Aktion erhält damit einen Wert zwischen null und eins, welcher Aufschluss über die Qualität der Aktion im momentanen Zustand des Env liefert. \citep[S. 100]{DRL_Lapan} \\
Basierend auf dieser Wahrscheinlichkeitsverteilung $\pi$ wird die nächste Aktion $a$ bestimmt. Dabei wird nicht immer die optimale Aktion gewählt, um die Exploration voranzutreiben.\\
Die value-based Verfahren bestimmen ihre Aktionen nicht mit einer Wahrscheinlichkeitsverteilung. Die Policy wird indirekt durch das Bestimmen aller Values, \fullref{subsubsec:Grundlagen_Value} für alle Aktionen, ermittelt. Es wird daher immer die Aktion $a$ gewählt, welche zu dem State $s$ führt, der über den größten Value verfügt. \citep[S. 100]{DRL_Lapan}

\subsubsection{On-Policy und Off-Policy Verfahren} \label{subsubsec:Grundlagen_on_off_policy}
Eine Klassifikation in on-policy und off-policy Verfahren gibt Aufschluss über den Zustand der Daten.\\ 
Off-Policy RL Verfahren sind in der Lage, von Daten zu lernen, welcher nicht unter der momentanen Policy \fullref{subsubsec:Grundlagen_Policy} generiert wurden. Die Aktualität der Daten spielt daher keine Rolle für dieses RL Verfahren.\\
On-Policy RL Verfahren sind dagegen sehr wohl abhängig von der Aktualität der Daten, da sie versuchen die Policy indirekt oder direkt zu optimieren. Sollte ein on-policy Agent mit veralteten Daten trainiert werden, so würde dies die Policy des Agenten zerstören. \citep[S. 210 f.]{DRL_Lapan}\\
Nach diesen Einordnungen sollen nun die ausgewählten Algorithmen näher erklärt werden. Begonnen wird mit dem Proximal Policy Optimization Algorithmus (PPO).

\section{Proximal Policy Optimization} \label{sec:Grundlagen_PPO}
Der Proximal Policy Optimization Algorithmus (PPO) wurde von dem Open-AI-Team entwickelt. Im Jahr 2017 erschien das gleichnamige Paper, welches den Aufbau und die Umsetzung des Algorithmus genauer erläutert. \citep{PPO} \\
Agenten, welche auf dem PPO Algorithmus basieren \fullref{subsubsec:Grundlagen_Agent}, konnten, in der Vergangenheit, bereits durch gute Leistungen überzeugen.\\
Um einen groben Überblick des Algorithmus zu erhalten, werden zuerst, im nächsten Abschnitt, dessen Architektur und Vorläufer näher thematisiert.

\subsection{Actor Critic Modell} \label{subsec:Grundlagen_actor_critic}
Der PPO Algorithmus ist ein policy-based \fullref{subsubsec:Grundlagen_policy_value_based} RL Verfahren, welches, im Vergleich zu anderen Verfahren, einige Verbesserungen aufweist. So ist der PPO eine Weiterentwicklung normaler Policy Gradient Verfahrens und basiert intern auf dem sogenanntes Actor bzw. Policy Network und auf dem Critic bzw. Value Network. \citep[S. 273 f.]{Sutton1998}\\
Beide neuronalen Netzwerke können aus mehreren Schichten bestehen, jedoch sind Actor und Critic streng voneinander getrennt und besitzen mit einer Ausnahme keine gemeinsamen Parameter. Gelegentlich werden jedoch beiden Netzen (Actor bzw. Critic) noch ein weiteres Netz vorgeschoben. In diesem Ausnahmefall können Actor und Critic gemeinsame Parameter besitzen.\\
Das Actor Network ist für die Bestimmung der Policy zuständig. Anders als bei value-based \fullref{subsubsec:Grundlagen_policy_value_based} RL Verfahren, wird diese direkt bestimmt und kann auch direkt angepasst werden. Die Policy wird als eine Wahrscheinlichkeitsverteilung über alle möglichen Aktionen vom Actor Network zurückgegeben. \fullref{subsubsec:Grundlagen_Policy}\\
Das Critic bzw. Value Network evaluiert die Aktionen, welche vom Actor Network bestimmt worden sind. Genauer gesagt, schätzt das Value Network den sogenannten Return bzw. die Discounted Sum of Rewards \fullref{subsubsec:Grundlagen_Return} zu einem Zeitpunkt $t$, basierend auf dem momentanen State $s$. Auf dieser Architektur wird ein PPO Agent aufgebaut.\\
Neben der Architektur, ist die Aktualisierungsprozedur ein weiterer wichtiger Bestandteil des Algorithmus. Diese wird daher im Folgenden näher betrachtet.

\subsection{PPO Formelelemente}
Das Herzstück eines jeden RL Verfahrens wird durch seine Aktualisierungsprozedur dargestellt, in welcher der Fehler (Loss) des Algorithmus bestimmt wird. Um die dem PPO zugrunde liegende Aktualisierungsprozedur besser zu verstehen, folge eine Erklärung ihrer einzelnen mathematischen Bestandteile. 
Diese basieren auf den PPO Paper \citep{PPO}.
\begin{longtable}[h]{|p{1.5cm}|p{\linewidth - 2.5cm}|}
	\hline
	Symbol & Erklärung \\
	\hline
	$t$ & Zeitpunkt \\
	\hline
	$\mathbb{\hat{E}}[X]$ & $\mathbb{\hat{E}}[X]$ ist der Erwartungswert einer zufälligen Variable $X$, z.B. $\mathbb{\hat{E}}[X] = \sum_{x}p(x)x$. \citep[Summary of Notation S. xv]{Sutton1998} \\
	\hline
	$\theta$ & Theta beschreibt die Parameter, aus denen sich die Policy des PPO ergibt. Sie sind die Gewichte, welche das Policy Network definiert. \\
	\hline
	$\pi_{\theta}$ & Die Policy bzw. Entscheidungsfindungsregeln sind eine Wahrscheinlichkeitsverteilung über alle möglichen Aktions. Eine Aktion $a$ wird auf Basis der Wahrscheinlichkeitsverteilung gewählt (siehe \citep[Summary of Notation S. xvi]{Sutton1998} und \autoref{subsubsec:Grundlagen_Policy}). \\
	\hline
	$L^\text{CLIP} (\theta)$ & $L^\text{CLIP} (\theta)$ bezeichnet den sogenannten Policy bzw. Actor Fehler, welche in Abhängigkeit zu der Policy $\pi_{\theta}$ steht. Dabei handelt es sich um einen Zahlenwert, welcher den Fehler über alle Parameter approximiert. Dieser wird für das Lernen des Actor Networks benötigt. \\
	\hline
	$r_{t}(\theta)$ & $r_{t}(\theta)$ stellt den Quotienten zwischen alter Policy (nicht als Abhängigkeit angegeben, da sie nicht verändert wird) und aktueller Policy zum Zeitpunkt $t$ dar. Daher auch Probability Ratio genannt. \\
	\hline
	$\hat{A}_{t}(s, a)$ & Erwartete Vorteil bzw. Nachteil einer Aktion $a$, welche im State $s$ ausgeführt wurde. \\
	\hline
	$\text{clip}$ & Mathematische Funktion zur Beschneidung eines Eingabewertes. Clip setzt eine Ober- und Untergrenze fest. Sollte ein Wert, der dieser Funktion übergeben wird, sich nicht mehr innerhalb der Grenzen befinden, so wird der jeweilige Grenzwert zurückgegeben. \\
	\hline
	$\epsilon$ & Epsilon ist ein Hyperparameter, welcher die Ober- und Untergrenze der Clip Funktion festlegt. Gewöhnlich wird für $\epsilon$ ein Wert zwischen $0.1$ und $0.2$ gewählt. \\
	\hline
	$\gamma$ \label{sign:Gamma} & Der Gamma-Wert bzw. Abzinsungsfaktor ist ein Hyperparameter, welcher die Zeitpräferenz des Agenten kontrolliert. Gewöhnlich liegt Gamma $\gamma$ zwischen 0.9 bis 0.99. Große Werte sorgen für ein weitsichtiges Lernen des Agenten, wohingegen kleine Werte zu einem kurzfristigen Lernen führen \citep[S. 43 bzw. Summary of Notation S. xv]{Sutton1998}. \\
	\hline
	\caption{Formelelemente des PPO Algorithmus}
	\label{tab:Grundlagen_Formelelemente_PPO} 
\end{longtable}

\subsection{PPO Fehlerfunktion} \label{subsec:Grundlagen_PPO_Fehlerfunktion}
Nun, da die Grundlagen näher beleuchtet worden sind, ist das nächste Ziel, die Fehlerfunktion des PPO zu verstehen, damit diese im weiteren Verlauf angewendet werden kann. Der PPO Fehler wird durch folgende Funktion definiert \citep[S. 5]{PPO}:
\begin{align}
	\label{eq:PPO_Training_Loss}
	L^\text{PPO}_{t} (\theta) = L^\text{CLIP + VF + S}_{t} (\theta) = \mathbb{\hat{E}}_{t} [L^{\text{CLIP}}_{t}(\theta) - c_{1}L^{\text{VF}}_{t} + c_{2}S[\pi_{\theta}](s_{t})]
\end{align}
Dabei besteht die Fehlerfunktion aus dem Actor Fehler $L^{\text{CLIP}}_{t}(\theta)$, aus dem Critic Fehler $L^{\text{VF}}_{t}$ und aus dem Entropy Fehler bzw. Bonus $S[\pi_{\theta}](s_{t})$. Der Actor Fehler ist dabei durch folgenden Term gegeben \citep[S. 3]{PPO}:
\begin{align}
	\label{eq:Grundlagen_Actor_Loss}
	L^\text{CLIP}_{t} (\theta) = \mathbb{\hat{E}}_{t} [ \min(r_{t}(\theta) \hat{A}_{t}(s, a), \text{clip}(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_{t}(s, a))]
\end{align}
Dieser wird im Weiteren kleinschrittig erklärt. Begonnen wird mit dem Advantage $\hat{A}_{t}(s, a)$, welcher sich aus dem Return und dem Baseline Estimate berechnet.

\subsubsection{Return} \label{subsubsec:Grundlagen_Return}
Der Return $R_{t}$ stellt die diskontierte Summe der Rewards in der vom Zeitpunkt $t$ an verbleibenden Spielepisode dar. Die einzelnen Rewards werden dabei mit dem Discount Factor $\gamma$ multipliziert \fullref{tab:Grundlagen_Formelelemente_PPO}, um die Zeitpräferenz des Agenten besser zu steuern. Gamma $\gamma$ liegt dabei gewöhnlich zwischen einem Wert von 0.9 bis 0.99. Kleine Werte für Gamma sorgen dafür, dass der Agent eher dazu tendiert Aktionen zu wählen, welche unmittelbar zu positiven Rewards führen. Entsprechend verhält es sich mit großen Werten für Gamma $\gamma$. \citep[S. 42 ff.]{Sutton1998}\\
Für die Bestimmung des Advantage benötigt man neben dem Return jedoch auch noch den Baseline Estimate Wert.

\subsubsection{Baseline Estimate} \label{subsubsec:Grundlagen_Baseline_Estimate}
Der Baseline Estimate Wert $b(s_{t})$ ist das Ergebnis der Value Funktion \fullref{subsubsec:Grundlagen_Value}, welche durch das Critic Network \fullref{subsec:Grundlagen_actor_critic} realisiert wird. Die Value Funktion schätzt den noch zu erwartenden Return $R_{t}$, vom aktuellen State $s_{t}$ an. 
Da es sich hierbei um die Ausgabe eines NN handelt, wird in dieser immer eine Varianz bzw. ein Rauschen vorhanden sein. \citep[Kapitel 3]{asynchronous_methods_for_deep_rl}\\
Mit diesem letzten Wert lässt sich nun der Advantage bestimmen.

\subsubsection{Advantage} \label{subsubsec:Grundlagen_Advantage}
Der Advantage $\hat{A}_{t}(s, a)$ stellt einen Funktionsbestandteil des Actor Fehlers dar. Dieser wird durch die Subtraktion des Returns $R_{t}$ mit dem Baseline Estimate Wert $b(s_{t})$ bestimmt. Die folgende Formel \fullref{eq:Grundlagen_Advantage} ist eine zusammengefasste Version der Originalformel aus \citep{PPO}:
\begin{align}
	\hat{A}_{t}(s, a) = R_{t} - b(s_{t})
	\label{eq:Grundlagen_Advantage}
\end{align}
Der Advantage gibt an, um wie viel besser oder schlechter eine Aktion gewesen ist, basierend auf der Erwartung der Value Funktion des Critics. 
Es wird also die Frage beantwortet, ob eine gewählte Aktion $a$ im Zustand $s_{t}$ zum Zeitpunkt $t$ besser oder schlechter als erwartet war. \citep[Kapitel 3]{asynchronous_methods_for_deep_rl}\\
Um nun den ersten Surrogate Fehler, welcher im Weiteren noch thematisiert wird \fullref{subsubsec:Grundlagen_Surrogate_Fehlerfunktion}, zu bestimmen, wird noch die Probability Ratio benötigt.

\subsubsection{Probability Ratio} \label{subsubsec:Grundlagen_Probability_Ratio}
Die Probability Ratio $r_{t}(\theta)$ ist der nächste Funktionsbestandteil des $L^\text{CLIP}_{t} (\theta)$ \fullref{eq:Grundlagen_Actor_Loss}.
In normalen Policy Gradient Methoden bestehe ein Problem zwischen der effizienten Datennutzung und der Aktualisierung der Policy. Dieses Problem tritt z.B. im Zusammenhang mit dem Advantage Actor Critic (A2C) Algorithmus auf und reglementiert die effiziente Nutzung von Daten. 
So ist es dem A2C nur möglich von Daten zu lernen, welche on-policy \fullref{subsubsec:Grundlagen_on_off_policy} erzeugt wurden.
Der PPO umgeht diese Problematik jedoch partiell durch das Importance-Sampling (IS, deutsch: Stichprobenentnahme nach Wichtigkeit).
Bei der Fehlerfunktion des A2C Algorithmus \fullref{eq:Grundlagen_A2C_Loss} wird deutlich,
\begin{align}
	\label{eq:Grundlagen_A2C_Loss}
	\mathbb{\hat{E}}_t[\log_{\pi_{\theta}}(a_t|s_t)A_t]
\end{align}
dass die Daten für die Fehlerbestimmung nur unter der aktuellen Policy $\pi_{\theta}$ generiert wurden (on-policy) \citep[S. 591]{DRL_Lapan}.\\
Schulman et al. ist es jedoch gelungen, diesen Ausdruck durch einen mathematisch äquivalenten zu ersetzen, welcher auf zwei Policies basieren kann. Eine davon stellt die aktuelle $\pi_{\theta}$ und die andere eine ältere $\pi_{\theta_{\text{old}}}$ dar.
\begin{align}
	\label{eq:Grundlagen_Probability_Ratio}
	r_{t}(\theta) = \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{\text{old}}}(a_{t}|s_{t})}
\end{align}
Die Daten können nun mittels $\pi_{\theta_{\text{old}}}$, effizienter genutzt werden, da sie nun nicht mehr aus der aktuellen Policy stammen müssen. Dies erlaubt ein häufigeres Nutzen der Daten, ohne eine Zerstörung der Policy zu riskieren. \citep[Zeitpunkt: 9:25]{Deep_RL_Bootcamp}

\subsubsection{Surrogate Fehlerfunktion} \label{subsubsec:Grundlagen_Surrogate_Fehlerfunktion}
Die Surrogate Fehlerfunktionen ergeben sich aus der Tatsache, dass die Fehlerfunktion des PPO nicht mit der gewöhnlich verwendeten logarithmierten Policy $\mathbb{\hat{E}}_t[\log_{\pi_{\theta}}(a_t|s_t)A_t]$ arbeitet, sondern mit dem Surrogate (Ersatz) der Probability Ratio $r_{t}(\theta)$ \fullref{subsubsec:Grundlagen_Probability_Ratio} zwischen alter und neuer Policy. \citep{TRPO}\\
\\Der Actor Fehler wird mithilfe zweier Surrogate Fehlerfunktionen bestimmt. Die erste Fehlerfunktion $surr_1$ \fullref{eq:Grundlagen_surrogate_1}
\begin{align}
	\label{eq:Grundlagen_surrogate_1}
	r_{t}(\theta) \hat{A}_{t}(s, a)
\end{align}
stellt die normale TRPO Fehlerfunktion dar, ohne die durch den TRPO Algorithmus vorgesehene KL-Penalty. \citep[S. 3 f.]{PPO}
Die alleinige Nutzung dieser Funktion hätte jedoch destruktiv große Policy Aktualisierungen zufolge. Aus diesem Grund haben John Schulman et al. eine zweite Surrogate Fehlerfunktion $surr_2$ \fullref{eq:Grundlagen_surrogate_2}, dem PPO Algorithmus hinzugefügt. 
\begin{align}
	\label{eq:Grundlagen_surrogate_2}
	\text{clip}(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_{t}(s, a)
\end{align}
Die einzige Veränderung dieser Funktion im Vergleich zur Ersten ist, dass der berechnete Fehler geclipped (abgegrenzt) wird. Sollte sich die Probability Ratio zu weit von eins entfernen, so wird $r_{t}(\theta)$ entsprechende auf $1 - \epsilon \text{ bzw. } 1 + \epsilon$ begrenzt. Es wird sich daher in einer Trust Region bewegt, da man sich nie allzu weit von der ursprünglichen Policy entfernt. \citep{TRPO, PPO}

\subsubsection{Zusammenfassung der PPO Fehlerfunktion} \label{subsubsec:Grundlagen_PPO_Actor_Loss}
Der Actor Fehler lässt sich nun für jede Erfahrung, welche gesammelt wurde, bestimmen. Dabei wird das Minimum der beiden Surrogate Fehlerfunktionen ausgewählt und als Fehler zurückgegeben \fullref{eq:Grundlagen_Actor_Loss_Single}.
\begin{align}
	\min(r_{t}(\theta) \hat{A}_{t}(s, a), \text{clip}(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_{t}(s, a))
	\label{eq:Grundlagen_Actor_Loss_Single}
\end{align}
Da für das Lernen immer mehrere Erfahrungen genutzt werden, wird der empirische Durchschnittsfehler bestimmt. Dies wird durch $\mathbb{\hat{E}}_{t}$ impliziert. Somit ergibt sich die Actor Fehlerfunktion \fullref{eq:Grundlagen_Actor_Loss}. \citep[S. 3f.]{PPO}\\

\subsubsection{Zusammenfassung der PPO Fehlerfunktion} \label{subsubsec:Grundlagen_PPO_Fehlerfunktion} 
Zum Schluss wird die PPO Fehlerfunktion definiert. Damit dies jedoch geschehen kann, muss vorher noch der Critic $L^{\text{VF}}_{t}$ und Entropie Fehler $S[\pi_{\theta}](s_{t})$ bestimmt werden. Die Critic Fehlerfunktion setzt sich folgendermaßen zusammen:
\begin{align}
	L^{\text{VF}}_{t} = (V_{\theta}(s_{t})-V_{t}^{targ})^2 \text{ wobei } V_{t}^{targ} = r_{t}(\theta)
	\label{eq:Grundlagen_Critic_Loss}
\end{align} 
Der letzte Teil, welcher für die Bestimmung der PPO Fehlerfunktion benötigt wird, ist der Entropy Fehler bzw. Bonus. Dabei handelt es sich um die Entropie der Policy (Wahrscheinlichkeitsverteilung \fullref{subsubsec:Grundlagen_Policy}).\\
Es ergibt sich damit der bereits oben erwähnte PPO Training Loss \fullref{eq:PPO_Training_Loss}:
\begin{center}
	$L^\text{PPO}_{t} (\theta) = L^\text{CLIP + VF + S}_{t} (\theta) = \mathbb{\hat{E}}_{t} [L^{\text{CLIP}}_{t}(\theta) - c_{1}L^{\text{VF}}_{t} + c_{2}S[\pi_{\theta}](s_{t})]$
\end{center}
$c_1 \text{ und } c_2$ sind in dieser Steuerungsfaktoren zur Regulierung der Fehlerhöhe.\\
Um den PPO Algorithmus mit seiner Fehlerfunktion richtig anwenden zu können, wird im Weiteren noch der Algorithmus näher erklärt.

\subsection{PPO - Algorithmus} \label{subsec:Grundlagen_PPO_Algorithmus}
Um die Theorie in die Praxis zu überführen, soll nun der Algorithmus komprimiert dargestellt werden, um diesen später umsetzen zu können. 
Diese Darstellung basiert dabei auf der Quelle \citep{PPO} und einigen weiteren Anpassungen.
\begin{enumerate}
	\item Initialisiere alle Hyperparameter und die neuronalen Netzwerke für Actor und Critic mit den Gewichten $\theta$ bzw. $w$. Erstelle einen Experience Buffer $EB$.
	\item Bestimme mit dem Zustand $s$ und dem Actor Network eine Aktion $a$. Dies geschieht durch $\pi_{\theta}(s)$.
	\item Führe Aktion $a$ aus und ermittle den Reward $r$ und den Folgezustand $s'$.
	\item Speichere Zustand, Aktion, Policy der Aktion, Reward und is\_terminal in $EB$.
	\item Ersetze den Zustand mit dem Folgezustand $s' \longrightarrow s$.
	\item Wiederhole alle Schritte ab Schritt 2, bis die Spielepisode endet.
	\item Entnehme einen Batch aus dem Buffer.
	\item Bestimmt die Ratios $r_t(\theta)$. \fullref{subsubsec:Grundlagen_Probability_Ratio}.
	\item Berechne die Advantages $\hat{A}_{t}(s, a)$. \fullref{subsubsec:Grundlagen_Advantage}.
	\item Berechne die Surrogate Fehler $surr_1$ und $surr_2$. \fullref{subsubsec:Grundlagen_Surrogate_Fehlerfunktion}.
	\item Bestimmt den PPO Fehler. \fullref{subsubsec:Grundlagen_PPO_Fehlerfunktion}
	\item Aktualisiere die Gewichte des Actors und Critics. $\theta_{\text{old}} \longleftarrow \theta$ und $w_{\text{old}} \longleftarrow w$.
	\item Wiederhole alle Schritte ab Schritt 7 $K$-mal. $K$ stellt ein Hyperparameter dar.
	\item Wiederhole alle Schritte ab Schritt 2 erneut, bis das Verfahren konvergiert.
\end{enumerate}
Neben dem PPO Algorithmus soll auch noch eine weiter behandelt werden. Der DQN Algorithmus wird im Folgenden vorgestellt.

\section{Deep Q-Network} \label{sec:Grundlagen_Deep_Q_Learning}
Der DQN (Deep Q-Network-Algorithmus) ist ein weiterer Reinforcement Learning Algorithmus, welcher auf einer ihm zugrunde liegenden Formel basiert. Er hat bereits große Erfolge erzielen können, besonders im Gaming Bereich \citep{DBLP:journals/corr/MnihKSGAWR13}. Daher erscheint es auch nicht weiter verwunderlich, dass sich dieser Algorithmus einer großen Beliebtheit erfreut. Unter anderem wurden aus diesem Grund bereits viele Erweiterungen implementiert, wie der DDQN (Double Deep Q-Network) oder der DQN mit Verrauschen Netzen usw.\\
Basierend auf seiner großen Beliebtheit ergibt sich die Frage, ob der DQN dieser auch gerecht wird? Diese Frage lässt sich am besten mit einem Vergleich beantworten, was diesen Algorithmus zu einem guten Kandidaten für diesen macht.\\ 
Angestammtes Ziel aller Q-Learning-Algorithmen ist es, jedem State-Action-Pair $(s,a)$ (Zustands-Aktions-Paar), einem Aktionswert (Q-Value) $Q$ zuzuweisen \citep[S. 126]{DRL_Lapan}. 
Dies ist beispielsweise über eine Tabelle möglich, was jedoch bei komplexen Environments schnell ineffizient wird. Ein weiterer Ansatz sind NN, welche die Q-Value Funktion nachbilden und erlernen können.\\
\\Ein kleiner Blick in die dem Algorithmus zugrunde liegende Logik eröffnet dahingehend einen besseren Überblick. 
So ist die Idee von vielen RL Verfahren, die Aktionswert Funktion mithilfe der Bellman Gleichung iterativ zu bestimmen. Daraus ergibt sich die folgende Formel \citep{DBLP:journals/corr/MnihKSGAWR13}
\begin{align}
	Q_{i+1}(s,a) = \mathbb{E} \bigl\lbrack r + \gamma \max_{a'} Q(s',a')|s,a \bigr\rbrack
	\label{eq:Grundlagen_Bellman_Gleichung_iterativ}
\end{align}
wobei, $Q_{i+1}(s,a)$ den $i+1$ aktualisierten Aktionswert der Aktion $a$ im Zustand $s$ darstellt.
Die iterativ angewendete Bellman Gleichung \fullref{eq:Grundlagen_Bellman_Gleichung_iterativ} besagt daher, dass die Aktionswert Funktion $Q$ dem empirischen Durchschnitt aller iterativen Aktualisierungen entspricht.
In der Theorie konvergiert ein solches Iterationsverfahren $Q_i \longrightarrow Q^*$ jedoch nur, wenn i gegen unendlich läuft $i \longrightarrow \infty$, wobei $Q^*$ die optimale Aktionswert Funktion darstellt. Da dies jedoch nicht möglich ist, muss $Q^*$ approximiert werden $Q(s,a;\theta) \approx Q^*$. Dies geschieht mittels eines NN.
Damit dieses nicht ausschließlich zufällige Q-Values ermittelt, ist eine Anpassung der Gewichte nötig. Dafür muss jedoch zuerst der Fehler des DQN Algorithmus bestimmt werden, was mithilfe der Fehlerfunktionen geschieht. Diese ist wie folgt definiert \citep[S.146 f.]{DRL_Lapan}:
\begin{align}
	L(\theta) = \mathbb{E} \Bigl\lbrack ((r(s,a) +\gamma \max_{a'}Q(s',a';\theta)) - Q(s,a;\theta))^2 \Bigr\rbrack
	\label{eq:DQN_Loss}
\end{align}
Die Formel \fullref{eq:DQN_Loss} besagt, dass der Fehler eines zufällig ausgesuchten State-Action Tupels (s,a) sich wie folgt zusammensetzt. 
Der Fehler ist die Differenz aus dem Aktionswerts $Q(s,a; \theta)$, welcher Aufschluss über den, in dieser Episode, zu erwartenden Reward liefert und $y_i$. Dabei ist $y_i$ der Reward, welcher durch die Ausführung der Aktion $a$ im Zustand $s$ erzielt wurde, addiert mit dem maximalen Q-Value der Folgeaktion $a'$ im Folgezustand $s'$.\\
Da $Q(s,a)$ rekursiv definiert werden kann, ergibt sich in vereinfachter Form \citep[S.126 und S. 146 f.]{DRL_Lapan}:
\begin{align}
	Q(s,a) = r + \gamma \max_{a \in A}Q(s',a') = \mathbb{E} \bigl\lbrack r+\gamma \max_{a'}Q(s',a';\theta)|s,a \bigr\rbrack = y_i
	\label{eq:Q-Value_rekursive_def}
\end{align}
Es wird erkenntlich, dass $Q(s,a;\theta)$ dem Q-Target $Q(s,a;\theta) \longrightarrow y_i$ entsprechen soll, darum wird die Differenz zwischen beiden bestimmt und als Fehler deklariert. Dieser wird noch quadriert, damit er stets positiv ist.

\subsection{DQN - Algorithmus} \label{subsec:Grundlagen_DQN_Algorithmus}
Zur besseren Anwendbarkeit haben Volodymyr Mnhi et al. einen Algorithmus entworfen, welcher den DQN anschaulich erklärt \citep{DBLP:journals/corr/MnihKSGAWR13}. Da jedoch in diesem Algorithmus weiterhin auf Teile der Fehlerfunktion eingegangen wird, folge eine bereinigte Version, welche sich auch für den allgemeinen Gebrauch besser anbietet. \citep[S. 149 f.]{DRL_Lapan}
\begin{enumerate} 
	\item Initialisiere alle Hyperparameter das Q-NN mit zufallsbasierten Gewichten. Setzte Epsilon $\epsilon = 1.00$ und erzeuge einen leeren Experience Buffer $EB$.
	\item Wähle mit der Wahrscheinlichkeit $\epsilon$ eine zufällige Aktion $a$ oder nutze $a = \arg\max_{a} Q(s,a)$.
	\item Führt Aktion $a$ aus und ermittle den Reward $r$ und den Folgezustand $s'$.
	\item Speichern von Zustand $s$, Aktion $a$, Reward $r$ und Folgezustand $s'$.
	\item Reduziere $\epsilon$, sodass die Wahrscheinlichkeit, eine zufällige Aktion zu wählen, minimiert wird. Gewöhnlich existiert eine untere Grenze für $\epsilon$, sodass immer noch einige wenige Aktionen zufällig gewählt werden.
	\item Entnehme zufallsbasiert einen Batch aus dem Experience Buffer $EB$.
	\item Berechne $y = r + \gamma \max_{a \in A}\hat{Q}(s',a')$.
	\item Berechne den Fehler $\mathbb{L} = (Q(s,a) - y)^2$
	\item Update $Q_\theta(s,a)$ mithilfe von Gradientenabstiegsverfahren. Daher $Q_{\theta_{i}}(s,a) \longrightarrow Q_{\theta_{i+1}}(s,a)$
	\item Wiederhole alle Schritte von Schritt 2 an, bis sich eine Konvergenz ergibt.
\end{enumerate}
