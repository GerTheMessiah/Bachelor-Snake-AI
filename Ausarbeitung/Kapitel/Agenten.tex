\chapter{Agenten} \label{chap:Agenten}
Ein zentraler Aspekt der eines Vergleiches von verschiedenen RL-Agenten ist die genaue Definition der einzelnen Agenten. Basierende auf den Grundlagen \ref{sec:Agent} soll in diesem Kapitel der Begriff vervollständigt und die zu vergleichenden Agenten sollen vorgestellt werden.\\
Erste statistische Erhebungen haben gezeigt, dass die ausgewählten Hyperparameter einen immensen Einfluss auf das Verhalten der Agenten haben. Bestätigt wird diese Aussage durch die Quelle \cite{Sutton1998}. Ein Vergleich zwischen DQN und PPO mit wahllos gewählten Hyperparametern ist folglich wenig aussagekräftig. Daher ist auch die Definition des Begriffs Agent, welcher nur zwischen DQN und PPO differenziert, unzureichend.\\
\\Angebracht wäre eine neuer erweiterte Definition des Begriffs Agent für diese Ausarbeitung. Diese soll um den entscheidenden Faktor der Hyperparameter erweitert werden. Ein Agent wird daher nicht mehr alleinig durch seine RL-Klasse (Q-Learning oder Policy Gradient) und Algorithmus-Klasse(DQN oder PPO) definiert, sondern ebenfalls durch die ausgewählten Hyperparameter.

\section{Agenten}
Im Folgenden werden die einzelnen Agenten, welche untereinander verglichen werden sollen, tabellarisch vorgestellt. Daher wird Aufschluss über Details, wie z.B. die RL-Klasse, Algorithmus-Klasse, Hyperparameter und die Netzstruktur gegeben.
\begin{longtable}[h]{|p{3.5cm}|p{2.5cm}|p{1.5cm}|p{6cm}|}
	\caption{Agenten}
	\label{tab:Agenten} 
	\endfirsthead
	\endhead
	\hline
	Agentenname & RL-Klasse & Algo-rithmus-klasse & Hyperparameter \\
	\hline
	DQN\_0.99\_64\_5e-6\_2**12\_5e-4 & Q-Learning & DQN & 
	\begin{itemize}
		\item gamma ($\gamma$) = 0.99
		\item batch\_size = 64 = $2^{6}$
		\item epsilon\_decrement = $5\mathrm{e}{-6}$
		\item max\_mem\_size = $2^{12}$
		\item lr = $5\mathrm{e}{-4}$
	\end{itemize} 
	\\
	\hline
	DQN\_0.95\_128\_1e-5\_2**13\_1e-4 & Q-Learning & DQN & 
	\begin{itemize}
		\item gamma ($\gamma$) = 0.95
		\item batch\_size = 128 = $2^{7}$
		\item epsilon\_decrement = $1\mathrm{e}{-5}$
		\item max\_mem\_size = $2^{13}$
		\item lr = $1\mathrm{e}{-4}$
	\end{itemize} 
	\\
	\hline
	PPO\_0.99\_128\_10\_1e-3\_1.5e-3\_0.5\_1e-3\_128\_2**11 & Policy Gradient & PPO & 
	\begin{itemize}
		\item gamma ($\gamma$) = 0.99
		\item K\_epochs = 10
		\item epsilion\_clip = 0.2
		\item lr\_actor = $1\mathrm{e}{-3}$
		\item lr\_critic = $1.5\mathrm{e}{-3}$
		\item critic\_loss\_coefficient = 0.5
		\item entropy\_coefficient = 0.001
		\item batch\_size = 128
		\item max\_mem\_size = $2^{11}$
	\end{itemize} 
	\\
	\hline
	PPO\_0.95\_128\_8\_0.5e-3\_1e-3\_0.5\_1e-4\_64\_2**9 & Policy Gradient & PPO & 
	\begin{itemize}
		\item gamma ($\gamma$) = 0.99
		\item K\_epochs = 10
		\item epsilion\_clip = 0.2
		\item lr\_actor = $1\mathrm{e}{-3}$
		\item lr\_critic = $1.5\mathrm{e}{-3}$
		\item critic\_loss\_coefficient = 0.5
		\item entropy\_coefficient = 0.001
		\item batch\_size = 128
		\item max\_mem\_size = $2^{11}$
	\end{itemize} 
	\\
	\hline
\end{longtable}