\chapter{Evaluation} \label{chap:Evaluation}
In dem Kapitel Evaluation sollen die Ergebnisse der angewendeten Methodik präsentiert werden. Des Weiteren wird eine Anforderungsanalyse durchgeführt, um zu beurteilen, welche Anforderungen umgesetzt wurden. Sollen einige nicht umsetzbar gewesen sein, so werden diese in diesem Abschnitt erläutert.

\section{Ergebnisevaluation der Vergleiche} \label{sec:Evaluation_Ergebnisevaluation}
In der Evaluation sollen die Ergebnisse, welche aus den Vergleichen stammten präsentiert werden. Dabei wird sich am Vorgehen orientiert.

\subsection{Evaluation der Baseline Vergleiche}
Basierend auf dem Vorgehen (siehe Abschnitt \ref{sec:Konzept_Vorgehen}) wird mit den Baseline Vergleichen begonnen. Bei diesen handelt es sich um die Vergleiche, welche von  nicht optimierten (baseline) Agenten durchgeführt worden sind (siehe Abbildung \ref{fig:Agenten}). Diese Agenten wurden in einem Trainingsverlauf, entsprechend der Beschreibung in Abschnitt \ref{sec:Konzept_Datenerhebung}, trainiert. 
Während dieses Prozesses wurden die Trainingsdaten erhoben, die graphisch dargestellt wurden. 
Als nächster Schritt wurden die Testdaten in Testläufen ermittelt, welche im Folgenden tabellarisch ausgewertet werden mit Ausnahme der Robustheit.

\subsubsection{Performance}
Einleitend in die Baseline Vergleichsauswertung soll mit der Performance gestartet werden. Dabei ist in der Abbildung \ref{fig:Baseline_01_performance} die Performance bzw. der Apfeldurchschnitt der letzten 100 Epochs pro Epoch des ersten Baseline Vergleiches abgebildet.\\
Die DQN Agenten waren in den Vergleichen nicht in der Lage, eine durchschnittliche Apfelsammelrate von 30 Äpfeln pro Spiel zu erreichen. 
Eine vermutliche Erklärung warum die Agenten des DQN Algorithmus keine guten Leistungen erzielen konnte, liegt in der Wahl von Zufallsaktionen. Die Komplexität des Spiels Snake steigt gegen Ende immer mehr an, da die Snake, mit zunehmenden Spielfortschritt, keine nicht zielführenden Schritte mehr gehen darf. In einer beengten Spielsituation könnte jeder falsche Schritt zum Tod führen, wobei durch das zufällige Wählen von Aktionen falsche Schritte unternommen würden. Dadurch, dass das Spiel immer vorzeitig beendet würde, könnte der DQN Algorithmus auch seine Leistung nicht weiter steigern, weil ihm die Daten zum Lernen fehlten.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/baseline-performance.png}
	\caption[Baseline Vergleich Performance]{Baseline Vergleich der gemittelten Performance.}
	\label{fig:Baseline_01_performance}
\end{figure}
Im Training konnten besonders die Leistungen der PPO Agenten PPO-03 und PPO-01, in den Vergleichen, überzeugen. 
Der PPO-03 kann dabei besonders mit seinem schnellen Lernerfolg punkten, wohingegen der PPO-01 mit seiner annähernd linearen Stetigkeit überzeugen kann.
PPO-02 könnte zwar ebenfalls ein fast lineare Steigerung seiner Performance erzielen, jedoch konvergierte dieser früher als der PPO-01.
Dieses entsprechen den in Abschnitt \ref{sec:Konzept_Vorstellung_Agenten} angedachten.
\begin{longtable}[h]{|p{2.7cm}|p{4.5cm}|p{4cm}|}
	\hline
	Agent & Gemittelte Performance & Standardabweichung \\
	\hline
	DQN-01 & 17.7812 & 2.9577 \\
	\hline
	DQN-02 & 26.4226 & 4.3482 \\
	\hline
	DQN-03 & 25.4506 & 4.8280 \\
	\hline
	PPO-01 & 44.4544 & 19.6957 \\
	\hline
	PPO-02 & 38.7325 & 11.5756 \\
	\hline
	PPO-03 & 46.4268 & 14.0280 \\
	\hline
\caption{Testdatenauswertung der Performance}
\label{tab:Evaluation_Testdaten_Performance} 
\end{longtable}
Auch die Auswertung der Testdaten (siege Tabelle \ref{tab:Evaluation_Testdaten_Performance}) zeigt den, sich in den Trainingsdaten abzeichnenden, Trend. So erbringt der PPO-03 die beste und der PPO-01 die zweitbeste Leistung. Die Standardabweichung der Agenten PPO-03 und PPO-01 zeigt jedoch, dass die Leistungen nicht konsistent sind. Besonders PPO-01 zeigte eine schwankende Performance, welche auch in den Trainingsdaten (siehe Abbildung \ref{fig:Baseline_01_performance}) zu beobachten ist.
Aufgrund der erzielten Performance dieser PPO Agenten ist daher festzustellen, dass der PPO-03 und PPO-01 das Evaluationskriterium der Performance am besten erfüllt haben. Sie sind daher die Sieger im Evaluationskriterium der Performance.\\
Stark mit der Performance korrelierend, stellt die Siegrate das nächste zu untersuchende Evaluationskriterium dar (siehe Abschnitt \ref{sec:Evaluation_Siegrate}).

\subsubsection{Siegrate} \label{sec:Evaluation_Siegrate}
Ähnlich, wie bei der Performance, verhält es auch mit dem Evaluationskriterium der Siegrate (siehe Abbildung \ref{fig:Baseline_winrate}).
Die PPO Agenten PPO-03 und PPO-01 besitzen die besten Siegraten während des Trainings (siehe Abbildung \ref{fig:Baseline_winrate}). 
Die DQN Agenten sind nicht in der Lage gewesen, Siege zu erreichen und fallen daher aus der Betrachtung heraus.
Der PPO-02 zeigt eine geringe Siegrate, trotz ähnlicher Performances zu den Agenten PPO-01 und PPO-03 (siehe Abbildung \ref{fig:Baseline_01_performance}).
Bemerkenswert ist des Weiteren, dass der PPO-03 zwar ähnliche Leistungen wie der PPO-01 erreicht (siehe Abbildung \ref{fig:Baseline_01_performance}), jedoch der PPO-01 deutlich bessere Siegraten erzielen konnte. Dies ist möglicherweise auf den stetigen Lerncharakter des Agenten zurückzuführen (siehe Abschnitt \ref{sec:Konzept_Vorstellung_Agenten}).
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/baseline-winrate.png}
	\caption[Baseline Vergleich Siegrate]{Baseline der gemittelten Siegrate}
	\label{fig:Baseline_winrate}
\end{figure}
Auch die Testdaten in der Tabelle \ref{tab:Evaluation_Testdaten_Winrate}, zeigt, dass die PPO Agenten PPO-01 und PPO-03 die Sieger sind, wobei sich der eben beschriebene Trend, aus den Trainingsdaten, auch in den Testdaten widerspiegelt. Es ist daher festzuhalten, dass der PPO-01 und PPO-03 die Sieger im Evaluationskriterium der Siegrate sind.

\begin{longtable}[h]{|p{3.7cm}|p{4.5cm}|p{4.5cm}|}
	\hline
	Agent & Gemittelte Siegraten & Standardabweichung \\
	\hline
	PPO-01 & 0.6759 & 0.33306 \\
	\hline
	PPO-02 & 0.0926 & 0.20478 \\
	\hline
	PPO-03 & 0.3316 & 0.32751 \\
	\hline
	\caption{Testdatenauswertung der Baseline Siegrate}
	\label{tab:Evaluation_Testdaten_Winrate} 
\end{longtable}

\subsubsection{Effizienz}
Bei der Effizienz treten nun interessante Effekte auf. Diese ist nämlich als Schritte pro Apfelanzahl definiert (siehe \ref{fig:Evaluation_Baseline_Effizienz}). Mit dieser Berechnungsweise lässt sich ein Zusammenhang zwischen Effizienz und der Performance herstellen. 
Wie in der Abbildung \ref{fig:Evaluation_Baseline_Effizienz} zu erkennen ist, sind die DQN Agenten DQN-01 und DQN-02 diejenigen, welche die beste Effizienz-Leistung erbringen. Dies gilt jedoch nur bis zu einer Apfelanzahl von 43. Denkbar wären daher die DQN Agenten in Einsatzgebieten mit wenigen Zielen aber großen Distanzen einzusetzen, sodass der Effizienzcharakter des Agenten hilft Kraftstoff bzw. Energie, am Beispiel der unbemannter Drohnen, zu sparen. Eine weitere Betrachtung der DQN Agenten unter dem Kriterium der Effizienz wird jedoch aufgrund der fehlenden Daten nicht durchgeführt.
Da die DQN Agenten ausgeschieden sind, bleiben nur noch die PPO Agenten. Diese verfügen über den gesamten Trainingslauf eine solide Effizienz. 
Der PPO-02 konnte dabei die besten Ergebnisse erzielen, gefolgt von PPO-03.
Erwähnenswert ist ebenfalls, dass das die Effizient-Differenz zwischen den Agenten PPO-01 bis PPO-03 gegeben Ende deutlich abnimmt, da durch die Gesetzmäßigkeiten des Spiels Snake, kaum effizientere Routen zum Apfel zu finden sind.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/baseline-effizienz.png}
	\caption[Baseline Vergleich Effizienz]{Baseline Vergleich der gemittelten Effizienz}
	\label{fig:Evaluation_Baseline_Effizienz}
\end{figure}
Auch bei der Auswertung der Testdaten in Tabelle \ref{tab:Evaluation_Testdaten_Effizienz}, wird diese Trend deutlich.
\begin{longtable}[h]{|p{3.5cm}|p{4.0cm}|}
	\hline
	Agent & Effizienz-durchschnitt \\
	\hline
	PPO-01 & 0.08786 \\
	\hline
	PPO-02 & 0.10438 \\
	\hline
	PPO-03 & 0.09661 \\
	\hline
	\caption{Testdatenauswertung der Baseline Effizienz}
	\label{tab:Evaluation_Testdaten_Effizienz} 
\end{longtable}
Die gute Effizienz des PPO-02 kann dabei unter anderen auf den niedrigen Gamma Wert von 0.93 zurückgeführt werden, welcher den Agenten dazu anhält, schnell viele gute Rewards zu erzielen (siehe Abschnitt \ref{sec:Konzept_Vorstellung_Agenten}).

\subsubsection{Robustheit}
Die Robustheit stellt ein besonderes Evaluationskriterium dar, denn sie wird ausschließlich aus Testdaten bestimmt. Diese werden zur besseren Übersicht in eine Grafik überführt. Die Robustheit ist definiert als die durchschnittliche Performance pro Spielfeldgröße.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/baseline-robustheit.png}
	\caption[Baseline Vergleich Robustheit]{Baseline Vergleich der gemittelten Robustheit}
	\label{fig:Evaluation_Baseline_Robustheit}
\end{figure}
Wie in Abbildung \ref{fig:Evaluation_Baseline_Robustheit} zu erkennen ist, zeigen die DQN Agenten 
Die DQN-Agenten erreichen auf kleineren Spielfeldgrößen bessere Ergebnisse als auf der Standard Spielfeldgröße von 8x8. Sollte sich jedoch das Spielfeld vergrößern, so stagnieren ihre Leistungen.
Bei den PPOs zeigt sich ein ähnliches Bild. Ihre Leistungen steigen ebenfalls nicht mit dem sich vergrößernden Spielfeld an. Jedoch waren der PPO-03 und der PPO-01 in der Lage die besten Ergebnisse in der unbekannten Gebieten zu erzielen. Im Baseline Run 2 war es dem PPO-03 sogar möglich, ein deutlich besseres Ergebnis zu erzielen, als mit der Standard Spielfeldgröße (8, 8).\\
Daher sind die Agenten PPO-03 und PPO-01 die Gewinner Agenten was das Evaluationskriterium der Robustheit betrifft.

\subsection{Evaluation der Optimized Vergleiche}
Nach der Durchführung der Baseline Vergleiche werden nun, entsprechend des Vorgehens (siehe Abschnitt \ref{sec:Konzept_Vorgehen}), die Baseline Gewinner Agenten (siehe Abbildung \ref{fig:Vorgehen}) optimiert und dann untereinander verglichen. Dabei werden ebenfalls die Baseline Agenten Gewinner mit in den Vergleich eingebunden. Die Ergebnisse dieser Vergleiche finden sich in den Folgenden Abschnitten.

\subsubsection{Performance}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/optimized-performance.png}
	\caption[Optimized Vergleich Performance]{Baseline Vergleich eins (oben) und zwei (unten) der Effizienz}
	\label{fig:Optimized_Performance}
\end{figure}
Wie in Abbildung \ref{fig:Optimized_Performance} zu erkennen ist, konnten alle Agenten, mit Ausnahme vom PPO-03-opt-a im OV-1 (Optimized Vergleich 1), den Lernprozess vorzeitig beenden. Auffällig ist, dass die Agenten PPO-01-opt-a (dunkelblau), PPO-01-opt-b (orange) und der PPO-03-opt-b (lila) Agent  ungefähr zeitgleich eine Siegrate von 60\% erreichten, gefolgt vom den Baseline Agenten und PPO-03-opt-a.
\begin{longtable}[h]{|p{3.2cm}|p{6cm}|p{4cm}|}
	\hline
	Agent & Durchschnittliche Performance & Standardabweichung \\
	\hline
	PPO-01 & 44.4544 & 19.69573 \\ 
	\hline
	PPO-01-opt-a & 45.8125 & 18.11826 \\ 
	\hline
	PPO-01-opt-b & 45.6213 & 18.72417 \\ 
	\hline
	PPO-02 & 38.7325 & 11.57559 \\ 
	\hline
	PPO-02-opt-a & 34.8827 & 6.92731 \\ 
	\hline
	PPO-02-opt-b & 42.5969 & 12.44162 \\ 
	\hline
	PPO-03 & 46.4268 & 14.02801 \\ 
	\hline
	PPO-03-opt-a & 48.7674 & 14.95799 \\ 
	\hline
	PPO-03-opt-b & 56.1117 & 12.37732 \\ 
	\hline
	\caption{Testdatenauswertung der Performance}
	\label{tab:Evaluation_Testdaten_Performance_Optimized} 
\end{longtable}
Aus den Testdaten (siehe Tabelle \ref{tab:Evaluation_Testdaten_Performance_Optimized}) geht hervor, dass sowohl PPO-03 als auch PPO-03-opt-a die besten Agenten dieses Vergleiches sind. Jedoch konnte der Baseline Agent PPO-03 die besseren Ergebnisse, im Mittel, vorweisen. Mit einer durchschnittlichen Performance von 51.5295 Äpfeln in den Testläufen, stellt der PPO-03 den optimalen Agenten für das Evaluationskriterium der Performance dar. Jedoch verrät die Varianz ein instabiles Sammelverhalten, sodass bei der Anwendung dieses Agenten davon auszugehen ist, dass dieser die gezeigte Leistung nicht konstant erbringen wird.

\subsubsection{Siegrate}
Bei der Siegrate konnten fast immer alle Agenten sehr guten Ergebnissen erzielen. Einzig PPO-03-opt-a lag im ersten Optimized Vergleich unter der erwarteten Siegrate von 60\%, wie in Abbildung \ref{fig:Optimized_Winrate} zu erkennen ist.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/optimized-winrate.png}
	\caption[Optimized Vergleich Siegrate]{Optimized Vergleich eins (oben) und zwei (unten) der Siegrate}
	\label{fig:Optimized_Winrate}
\end{figure}
Wie die Auswertung der Testdaten (siehe Tabelle \ref{tab:Evaluation_Testdaten_Winrate_Optimized}) aufzeigt, liefern die Agenten PPO-01-opt-b und PPO-03-opt-b sehr gute Ergebnisse. Jedoch gewinnt der PPO-03-opt-b diesen Vergleich mit einer durchschnittlichen Siegrate von 0,73405, dicht gefolgt vom PPO-01-opt-b mit 0,7325. Insgesamt ist dennoch zu bemerken, dass dieser kleine Abstand in der Siegrate auch durch Zufallseffekte zu erklären ist.
\begin{longtable}[h]{|p{3.2cm}|p{6cm}|p{4cm}|}
	\hline
	Agent & Durchschnittliche Sieg-Rate & Standardabweichung \\
	\hline
	PPO-01 & 0.6759 & 0.3331 \\ 
	\hline
	PPO-01-opt-a & 0.6551 & 0.3377 \\ 
	\hline
	PPO-01-opt-b & 0.7186 & 0.3011 \\ 
	\hline
	PPO-02 & 0.0926 & 0.2048 \\ 
	\hline
	PPO-02-opt-a & 0.0001 & 0.0071 \\ 
	\hline
	PPO-02-opt-b & 0.2354 & 0.2496 \\ 
	\hline
	PPO-03 & 0.3316 & 0.3275 \\ 
	\hline
	PPO-03-opt-a & 0.5273 & 0.3479 \\ 
	\hline
	PPO-03-opt-b & 0.8466 & 0.2482 \\ 
	\hline
	\caption{Testdatenauswertung der Optimized Sieg-Raten}
	\label{tab:Evaluation_Testdaten_Winrate_Optimized} 
\end{longtable}

\subsubsection{Effizienz}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/optimized-effizienz.png}
	\caption[Optimized Vergleich Siegrate]{Optimized Vergleich eins (oben) und zwei (unten) der Siegrate}
	\label{fig:Optimized_Effizienz}
\end{figure}
folgt im selben Stil.

\subsubsection{Robustheit}
Nach der Testauswertung (siehe Abbildung \ref{fig:Optimized_Robustheit}) hat sich der PPO-03-opt-b als der robusteste Agent dargestellt. Von allen Agenten weist er die kontinuierlichste Performance vor. Besonders auf größeren Spielfeldern zeigt er eine gute Leistung und damit eine solide Robustheit.\\
Es ist daher festzuhalten, dass der PPO-01-opt-b der Sieger der Vergleiches für das Evaluationskriterium der Robustheit ist.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4517]{Abbildungen/Evaluation/optimized-robustheit.png}
	\caption[Optimized Vergleich Siegrate]{Optimized Vergleich eins (oben) und zwei (unten) der Siegrate}
	\label{fig:Optimized_Robustheit}
\end{figure}

\section{Anforderungsevaluation}
Zu Beginn sollen die Anforderungen an das Environment evaluiert werden. Danach folgen die Evaluationen der Anforderungen Agenten und der Datenerhebung. Zum Schluss sollen evaluiert werden, ob alle Anforderungen bezüglich der Statistiken und der Evaluation selbst erfüllt worden sind.

\subsection{Anforderungsevaluation der Environment}
Die Hauptanforderung an das Env besagt, dass das Spiel Snake implementiert werden soll. Im Rahmen dieser Ausarbeitung wurde das Spiel Snake nach der Beschreibung in Anschnitt \ref{sec:Snake} implementiert. Diese Anforderung kann daher als erfüllt angesehen werden. Eine Darstellung der Implementierung findet sich im Abschnitt des Konzepts (siehe Abschnitt \ref{sec:Konzept_Environment}) und in der Implementierung (siehe Abschnitt \ref{sec:Implementierung_Environment}).\\
\\ Ebenfalls wurde die Anforderung der Standardisierten Schnittstelle (siehe Abschnitt \ref{sec:Anforderungen_Schnittstelle}), welche zu einer Normung und damit zu einer einfacheren Benutzung des Environment führen soll, erfüllt. Wie in dem Abschnitt \ref{sec:Konzept_Schnittstelle} und \ref{sec:Implementierung_train_Methode} zu sehen ist, werden Aktionen, durch die step Methoden entgegengenommen, Rewards und Observationen zurückgegeben. Neben der step Methode wird die Observation auch noch durch die Schnittstellenmethode reset geliefert. Diese beiden Methoden spannen die standardisierte Schnittstelle auf.\\
\\ Auch sind die funktionalen Anforderungen, welche den geregelten Ablauf im Environment garantieren, beachtet worden.
So wird die Aktionsausführung (siehe Abschnitt \ref{sec:Anforderungen_Aktionsausführung}) durch die action Methode in der SnakeGame Klasse durchgeführt, welche von der Schnittstellenmethode step aufgerufen wird. Die reset und render Anforderungen (siehe Abschnitte \ref{sec:Anforderung_Reset} und \ref{sec:visualisierung_Env}) werden durch die gleichnamigen Schnittstellenmethode abgedeckt (siehe Abschnitte \ref{sec:Konzept_Spielablauf} und \ref{sec:Implementierung_Environment}).

\subsection{Anforderungsevaluation der Agenten}
Der nächste großer Anforderungsbereich behandelt die Agenten. Zu diesem Zweck wurden die Anforderungen der Aktionsbestimmung und des Lernens aufgestellt. Diese stellen die grundlegenden Funktionen der Agenten dar. 
Wie im Konzept dargestellt ist, wurde sowohl der DQN als auch der PPO Agent mit einer Aktionsauswahlmethode (siehe Abschnitte \ref{sec:Implementierung_act_PPO} und \ref{sec:Implementierung_act_DQN}) und einer learn Methode (siehe Abschnitte \ref{sec:Implementierung_learn_DQN} und \ref{sec:Implementierung_learn_PPO}) ausgestattet. Diese implementieren das geforderte Verhalten aus den Anforderungen (siehe Abschnitt \ref{sec:Agent_Funktionalitäten}).\\
\\Neben den funktionalen, existieren noch zwei weitere Anforderungen. Zu diesen gehört die Diversität der Algorithmen (siehe Abschnitt \ref{sec:Anforderungen_Diversität}).\\
Diese fordert den Vergleich verschiedener Algorithmen und dabei insbesondere des PPO und DQN Algorithmus. Mit dieser Forderung kann die entwickelte Methodik (siehe Abschnitt \ref{sec:Konzept_Vorgehen}) besser bewertet werden. Diese Anforderung kann ebenfalls als erfüllt angesehen werden, da sowohl ein DQN (siehe Abschnitt \ref{sec:Implementierung_DQN_Agent}) als auch ein PPO Agent (siehe Abschnitt \ref{sec:Implementierung_PPO_Agent}) implementiert wurden.\\
\\ Die andere der zwei erwähnten Anforderungen behandelt die Parametrisierung der Agenten. Das System soll mehrere Agenten gleichen Algorithmus erstellen können, welche sich jedoch durch die Hyperparameter unterscheiden sollen. Wie auch in der Evaluation der Ergebnisse (siehe Abschnitt \ref{sec:Evaluation_Ergebnisevaluation}) und in dem Konzept (siehe Abschnitt \ref{sec:Konzept_Vorstellung_Agenten})zur erkennen ist, werden mehrere Agenten des gleichen Algorithmus miteinander verglichen.

\subsection{Anforderungsevaluation an die Datenerhebung} \label{sec:Evaluation_Datenerhebung}
Auch an die Datenerhebung wurden einige Anforderungen im Rahmen dieser Ausarbeitung gestellt. Zu diesen gehört die Forderung, dass die zu erhebenden Daten mehrfach erhoben werden sollen 
(siehe Abschnitt \ref{sec:Anforderungen_mehrfache_Datenerhebung}). Dies soll die Validität der statistischen Untersuchung steigern. Wie im Abschnitt der Datenerhebung (siehe Abschnitt \ref{sec:Konzept_Datenerhebung}) und in der Evaluation der Ergebnisse zu sehen ist, werden die auszuwertenden Daten doppelt erhoben. Daher sind aus immer zwei Statistiken zu einem Evaluationskriterium zu sehen.\\
Daraus lässt sich ebenfalls schließen, dass die Daten für die Statistiken gespeichert werden. Damit wird die Anforderung der Datenspeicherung erfüllt (siehe Abschnitt \ref{sec:Anforderungen_Datenspeicherung}). Dies wurde darüber hinaus in dem Abschnitt der Datenerhebung des Konzepts (siehe Abschnitt \ref{sec:Konzept_Datenerhebung}) und in der Implementierung (siehe Abschnitt \ref{sec:Implementierung_train_Methode}) dargestellt.

\subsection{Anforderungen an die Statistiken}
Insgesamt sind die Anforderungen zu den Statistiken (siehe Abschnitt \ref{sec:Anforderungen_Statistik}) darauf ausgelegt, dass mit dem implementierten System Statistiken entsprechend der Evaluationskriterien generiert werden können. Ein solche Funktionalität wurde implementiert, wie die Abschnitte \ref{sec:Konzept_Datenerhebung_Verarbeitung} und \ref{sec:Implementierung_Statistiken} beweisen. Auch die Grafiken in der Evaluation der Ergebnisse (siehe Abschnitt \ref{sec:Evaluation_Ergebnisevaluation}) bestätigen dies.

\subsection{Anforderungen an die Evaluation}
In den Anforderungen der Evaluation war gefordert, das die Evaluationskriterien Performance, Effizienz Robustheit und Siegrate untersucht werden. Wie im Abschnitt Ergebnisevaluation der Vergleiche \ref{sec:Evaluation_Ergebnisevaluation} zu erkennen ist, wurde genau dies durchgeführt. Mithilfe dieser Evaluationskriterien konnte, auf Grundlage der erhobenen Statistiken, ein optimaler Agent für jedes Kriterium ausgewählt werden.