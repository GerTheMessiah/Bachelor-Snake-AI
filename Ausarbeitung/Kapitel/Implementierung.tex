\chapter{Implementierung} \label{chap:Implementierung}
In diesem Kapitel soll die Implementierung des Spiels Snake, der beiden Algorithmen, der Ablaufroutinen sowie der Statistik Erzeugung thematisiert werden. Als Programmiersprache wurde Python (3.7) gewählt, da diese über viele Frameworks im Bereich Machine Learning verfügt.
In dieser Implementierung wird das Machine Learning Framework PyTorch (\url{https://pytorch.org/}) verwendet.

\section{Snake Environment} \label{sec:Implementierung_Environment}
Zur Implementierung des Spiels Snake wurde das Framework gym von OpenAI genutzt (siehe \url{https://gym.openai.com/}). Eine Darstellung der Implementierung findet sich in der \autoref{fig:Anhang_SnakeEnv}. Das Snake Environment implementiert entsprechend der Anforderung \ref{subsec:Anforderungen_Schnittstelle} drei zentrale Methoden, welche für den Informationsaustausch mit dem Agenten sorgen.\\ 
Die step Methode \fullref{fig:Implementierung_Sequenzdiagram} wird in der Ablaufprozedur (Test- bzw. Trainingsmethode) aufgerufen. Um eine Aktion auszuführen, ruft diese die action Methode auf. Diese führt die Aktion aus und manipuliert damit das SnakeGame.
Nach dem Aufruf von  action wird die nächste Obs mithilfe der make\_obs Methode bestimmt. Im Anschluss werden zudem noch der Reward sowie weitere Statusinformation gebildet und zurückgegeben \fullref{fig:Implementierung_Sequenzdiagram}.\\
Die reset Methode \fullref{fig:Implementierung_Sequenzdiagram} wird zu Beginn des Spielablaufs aufgerufen, um eine initiale Obs zu erhalten. Sie setzt dazu den momentanen Spielfortschritt zurück und generiert eine neue Obs \fullref{subsubsec:Konzept_Spielablauf}.\\
Die render Methode \fullref{fig:Implementierung_Sequenzdiagram} aktualisiert die GUI, indem diese die updateGUI Methode aufruft. Sie kann nur in Testläufen aufgerufen.\\
\\Da die action Methode die eigentliche Aktionsausführung implementiert, wird diese noch näher erläutert \fullref{subsubsec:Konzept_Spielablauf}.
Die Spiellogik ist hauptsächlich in der action Methode implementiert \fullref{subsubsec:Konzept_Spielablauf}. Ein schematischer Ablauf der action Methode ist in \autoref{fig:Implementierung_action_method} dargestellt.
\begin{figure}[H]
	\centering
	\includesvg[scale=0.110]{Sequenzdiagramm}
	\caption[Darstellung der Schnittstellenmethoden in einem Sequenzdiagramm]{Darstellung der Schnittstellenmethoden step, reset und render in einem Sequenzdiagramm}
	\label{fig:Implementierung_Sequenzdiagram}
\end{figure}
Zu Beginn wird geprüft, ob die maximale Anzahl an Schritten ohne einen Apfel gefressen zu haben überschritten ist. Sollte dies der Fall sein, so wird is\_terminal gesetzt und die Methode terminiert. Andernfalls wird die Aktion umgesetzt, indem die Player.direction angepasst wird. Diese gibt die Laufrichtung für den nächsten Schritt an. Daraufhin wird überprüft, ob der neue Kopf außerhalb des Spielfelds liegt. Wenn dies zutrifft, wird das Spiel terminiert.
Ansonsten wird der neue Kopf in die Liste aller Snake-Glieder (Player.tail), eingepflegt. Sollte die Snake zu diesem Zeitpunkt das gesamte Spielfeld ausfüllen, so hat sie gewonnen und die Methode terminiert. Ansonsten wird geprüft, ob die Snake im momentanen Schritt einen Apfel gefressen hat. Ist dem so, wird der Apfel entfernt und ein neuer generiert. Zudem wird der inter\_apple\_steps Zähler zurückgesetzt. Hat die Snake keinen Apfel gefressen, wird das letzte Schwanzstück entfernt, um die Illusion von Bewegung zu erzeugen und der inter\_apple\_steps Zäher wird inkrementiert.\\
Zum Schluss wird Player.tail noch auf Duplikate überprüft, um auszuschließen, dass die Snake in sich selbst gelaufen ist. Sind keine vorhanden, wird das Spielfeld (playground) mit den Elementen der Snake aktualisiert, andernfalls wird sie terminiert.
\begin{figure}[H]
	\centering
	\includesvg[scale=0.095]{action_method}
	\caption[Ablaufdiagramm der action Methode]{Darstellung des Ablaufs der action Methode in einem Ablaufdiagramm}
	\label{fig:Implementierung_action_method}
\end{figure}

\section{AV-Network} \label{sec:Implementierung_AV_Network}
Das AV-Network stellt die Netzstruktur dar, welche die AV (around\_view) verarbeitet. Um dies bewerkstelligen zu können, wird das Netz mithilfe des PyTorch Frameworks erstellt. Die Implementierung des NN findet dabei analog zu den Schilderungen in \autoref{subsec:Konzept_Netzstruktur} statt. Das AV-Network wird von allen Algorithmen und daher auch von den DQN Agenten genutzt, welche als Nächstes thematisiert werden.

\section{DQN} \label{sec:Implementierung_DQN}
Die Implementierung des DQN Algorithmus basiert auf \cite{Charles2013}.\\
Der DQN Algorithmus stellt einen der beiden zu implementierenden Algorithmen dar und beinhaltet die folgenden Klassen \fullref{fig:Anhang_DQN_Klassendiagramm}:\\
Die Agent Klasse implementiert die act \fullref{subsubsec:Konzept_Aktionsauswahlprozess_DQN} und learn \fullref{subsubsec:Konzept_Lernprozess_DQN} Methoden.
Die Memory Klasse entspricht der Memory-Komponente. Die QNetwork Klasse beinhaltet das NN zur Aktionsbestimmung \fullref{subsec:Konzept_Netzstruktur}) und wird im Folgenden näher thematisiert.

\subsection{Q-Network} \label{subsec:Implementierung_Q-Network}
Das Q-Network stellt das Instrument zur Bestimmung der Q-Values dar und besteht aus dem AV-Network (AV\_NET) und dem Q-Network-Tail (Q-net) \fullref{subsec:Konzept_Netzstruktur}. Es verfügt über eine
forward Methode, welche die AV (around\_view) durch das AV\_NET leitet. Danach wird der Output des AV\_NET mit der SO (scalar\_obs) verbunden und durch das Q\_net propagiert. Das Ergebnis wird zurückgeliefert und im Memory gespeichert. 

\subsection{Memory} \label{subsec:Implementierung_Memory_DQN}
Die Klasse Memory besteht aus einer Reihe von Tensoren, welche Daten mittels einer zusätzlichen Dimension speichern.
Das Memory verfügt über eine get\_data Methode, welche einen zufallsbasierten Batch an Erfahrungen zurückliefert. Die im Konzept erwähnte Ring Buffer Funktionalität wird durch einen Zähler realisiert. \fullref{subsec:Konzept_DQN}

\subsection{Agent} \label{subsec:Implementierung_DQN_Agent}
Der DQN Agent besteht aus den act Methoden, welche die Aktionen bestimmen und aus der learn Methode, die für das Training zuständig ist.

\subsubsection{Aktionsbestimmung} \label{subsubsec:Implementierung_Aktionsbestimmung_DQN}
In der Agent Klasse werden die zwei Methoden act und act\_test definiert. Die act Methode wird dabei analog zur Beschreibung im \autoref{subsubsec:Konzept_Aktionsauswahlprozess_DQN} implementiert.
Die act\_test Methode ist für die Aktionsbestimmung während der Testläufe zuständig und ermittelt Aktionen ausschließlich mittels des Q-Networks.

\subsubsection{Trainingsroutine} \label{subsubsec:Implementierung_Trainingsroutine_DQN}
Die learn Methode, welche nach den Darstellungen im Konzept implementiert wurde, stellt die Trainingsroutine dar \fullref{subsubsec:Konzept_Lernprozess_DQN}.\\
Sie überprüft als Erstes, ob das Memory genügend Daten für einen Mini-Batch besitzt. Ist dem nicht so, terminiert die Methode.
Danach wird die get\_data aufgerufen, welche einen Mini-Batch zurückliefert.\\
Zu Beginn werden daher die Q-Values der gespeicherten Aktion bestimmt. Im Anschluss folgen die Q-Values aller Aktionen der Nachfolgezustände \fullref{code:Bestimmung_Q-Values}.
\begin{lstlisting}[caption=Bestimmung der Q-Values, label=code:Bestimmung_Q-Values, style=Python]
av, scalar_obs, actions, rewards, is_terminal, av_, scalar_obs_, \
	batch_index = self.MEM.get_data()
q_eval = self.Q_NET(av, scalar_obs)[batch_index, actions]
q_next = self.Q_NET(av_, scalar_obs_)
\end{lstlisting}
Daraufhin wird, wie in \autoref{eq:DQN_Loss} und \autoref{subsubsec:Konzept_Lernprozess_DQN} dargestellt, Q-Target bestimmt \fullref{code:Bestimmung_Q-Target}.
\begin{lstlisting}[caption=Bestimmung von Q-Target, label=code:Bestimmung_Q-Target, style=Python]
q_next[is_terminal] = 0.0
q_target = rewards + self.GAMMA * T.max(q_next, dim=1)[0]
\end{lstlisting}
Zum Schluss wird der Fehler des DQN mit dem MSE (Mean Squared Error) bestimmt und das Q-Net aktualisiert \fullref{code:Bestimmung_DQN-Loss}.
\begin{lstlisting}[caption=Bestimmung des DQN Loss \& Update des Q-Networks, label=code:Bestimmung_DQN-Loss, style=Python]
loss = self.LOSS(q_target, q_eval)
self.Q_NET.OPTIMIZER.zero_grad()
loss.backward()
self.Q_NET.OPTIMIZER.step()
\end{lstlisting}
Zusätzlich wird noch Epsilon verringert, um die Anzahl an Zufallsaktionen während des nächsten Trainingslaufs zu senken.\\
Neben dem DQN Agenten wurde jedoch noch gefordert \fullref{subsec:Anforderungen_Diversität}, dass ein PPO Agent implementiert wird. Dies geschieht im Folgenden.

\section{PPO} \label{sec:Implementierung_PPO}
Die Implementierung des PPO basiert auf \cite{pytorch_minimal_ppo} und \cite{Charles2013}.\\
Der PPO Algorithmus beinhaltet die folgenden Klassen \fullref{fig:Anhang_DQN_Klassendiagramm}:\\
Die Agent Klasse definiert die learn Methode und ist als PPO-Komponente zu interpretieren \fullref{subsec:Konzept_PPO}.
Die Memory Klasse speichert die gesammelten Erfahrungen.
Die ActorNetwork und CriticNetwork Klassen beinhalten die Networks.
In der ActorCritic Klasse befinden sich zentrale Methoden zur Durchführung des Lernens, wie z.B. evaluate. 
Des Weiteren verbindet die ActorCritic Klasse das Actor- und Critic-Network miteinander, welche im nächsten Abschnitt näher betrachtet werden.

\subsection{Actor und Critic} \label{subsec:Implementierung_Actor_und_Critic}
Der Actor bzw. Critic wird durch die ActorNetwork bzw. CriticNetwork Klasse aufgespannt.
Neben dem AV-Network wird für den Actor bzw. Critic noch der Actor-Tail bzw. Critic-Tail definiert. Dabei handelt es sich um das Pendant zum Q-Network-Tail (Q-net) \fullref{subsec:Implementierung_Q-Network}.\\
Die forward Methoden von Actor und Critic sind dabei ebenfalls analog zu der des Q-Network-Tail. 
Sowohl ActorNetwork als auch CriticNetwork findet man jedoch nur gekapselt in der ActorCritic Klasse vor.

\subsection{ActorCritic} \label{subsec:Implementierung_ActorCritic}
Die ActorCritic Klasse verbindet Actor und Critic miteinander. Sie dient daher als Schnittstelle und definiert die drei Methoden act, act\_test und evaluate.\\
Diese ist in der Lage, aus ActorNetwork und CriticNetwork ein Gesamtnetzwerk aufzuspannen, welches alle Parameter der beiden Networks enthält. Eine der wichtigsten Aufgaben dieser Klasse ist das Bestimmen von Aktionen.

\subsubsection{Aktionsbestimmung} \label{subsubsec:Implementierung_Aktionsbestimmung_PPO}
Die PPO Agenten verfügen, wie die DQN Agenten, über die zwei Methoden act und act\_test. Die act Methode bestimmt mithilfe des Actors eine Wahrscheinlichkeitsverteilung über alle Aktionen.
Daraufhin wird eine Aktion entsprechend dieser bestimmt \fullref{subsubsec:Konzept_Aktionsauswahlprozess_PPO}.\\
Die act\_test Methode verzichtet, wie beim DQN, wieder auf Zufallselemente.
Am Ende der act\_test Methode wird die Aktion ausgewählt, welche die größte Wahrscheinlichkeit vorweist.

\subsection{Memory} \label{subsec:Implementierung_Memory_PPO}
Das Memory oder auch Replay Buffer genannt, besteht aus einer Reihe von Tensoren, welche die generierten Daten mittels einer zusätzlichen Dimension speichern. Die Rewards und Terminals (is\_terminal) werden jedoch in Listen eingepflegt, da dies das spätere Diskontieren (Abzinsen) erleichtert.\\
Mit der get\_data Methode werden die gesamten Erfahrungen der letzten Spielepisoden zurückgegeben. 

\subsection{Agent} \label{subsec:Implementierung_PPO_Agent}
Die Agent Klasse implementiert die learn Methode und verwaltet das Memory und zwei ActorCritic Networks. Diese stellen die alte und neue Policy dar. Während mit der alten Policy immer die Trainingsdaten generiert werden, wird mit der neuen Policy trainiert. Nach dem Lernen wird die alte Policy mit der neuen aktualisiert.

\subsubsection{Trainingsroutine} \label{subsubsec:Implementierung_Trainingsroutine_PPO}
Die hier dargestellte Implementierung basiert auf den Grundlagen des PPO Algorithmus \fullref{sec:Grundlagen_PPO} und auf dem Konzept \fullref{subsec:Konzept_PPO}.
Als Erstes wird überprüft, ob das Memory genügend Erfahrungen besitzt, um ein effektives Lernen durchzuführen. Sollte dies nicht der Fall sein, so wird die Methode terminiert. 
Ansonsten werden die Daten aus dem Memory mit der get\_data Methode entnommen.
Danach werden die Rewards mit der generate\_rewards Methode diskontiert (abgezinst).\\
Daraufhin wird die folgende Prozedur $K\_Epochs$-mal wiederholt.\\
\\Zu Beginn wird die evaluate Methode aufgerufen. Diese bestimmt die neuen logarithmierten Wahrscheinlichkeiten (log\_probs) aller gespeicherten Aktionen. Mit diesen und den gespeicherten alten (old\_log\_probs) werden daraufhin die ratios gebildet.
Zuzüglich werden mithilfe der Values, welche ebenfalls von evaluate bestimmt wurden, die Advantages erstellt \fullref{code:Bestimmung_Ratio_Advantages}.
\begin{lstlisting}[caption=Bestimmung der Ratios und Advantages, label=code:Bestimmung_Ratio_Advantages, style=Python]
probs, state_values, dist_entropy = self.POLICY. \
evaluate(old_av_b, old_scalar_b, old_action_b)	
ratios = T.exp(probs - probs_old_b)	
advantages = rewards_b - state_values.detach()
\end{lstlisting}
Mit diesen Werten ist es nun möglich, die Surrogate Fehler und anschließend den Actor Fehler zu bestimmen.
In dem Actor Fehler befindet sich zur einfacheren Handhabung noch der Entropy Fehler bzw. Bonus.
\newpage
\begin{lstlisting}[caption=Bestimmung des Surrogate und Actor Fehlers, label=code:Bestimmung_Surrogate_Losses, style=Python]
surr1 = ratios * advantages
surr2 = T.clamp(ratios, 1 - self.EPS_CLIP, 1 + self.EPS_CLIP) * \
advantages
loss_actor = -(T.min(surr1, surr2) + dist_entropy * \
 self.ENT_COEFFICIENT).mean()
\end{lstlisting}
Zur Bestimmung des gesamten Fehlers fehlt noch der Critic Fehler, welcher sich aus der Differenz zwischen Returns und den Values des Critics ermittelt.
Danach werden Actor und Critic Fehler zusammenaddiert, sodass der PPO Fehler entsteht. Dieser wird dann dazu genutzt, um das Actor\_Critic NN zu aktualisieren \fullref{code:Bestimmung_Surrogate_Losses}.
\begin{lstlisting}[caption=Bestimmung des PPO-Losses und Update der Netze, label=code:Bestimmung_PPO_Losses_update_NN, style=Python]
loss_critic = self.CRITIC_COEFFICIENT * \ 
self.LOSS(rewards_b, state_values)
loss = loss_actor + loss_critic 
self.POLICY.OPTIMIZER.zero_grad()
loss.backward()
self.POLICY.OPTIMIZER.step()
\end{lstlisting}
Zum Schluss wird, nachdem die Prozedur $K\_Epochs$-mal durchgeführt wurde, die alte Policy mit der neuen aktualisiert. Zuzüglich wird das Memory geleert.\\
Damit jedoch die Lernprozedur aufgerufen wird, benötigt es zentrale Ablaufroutinen, welche in den train und test Methoden implementiert sind.

\section{Train Methoden} \label{sec:Implementierung_train_Methode}
Zu Beginn werden Agent und Env zusammen mit den Datenhaltungslisten \fullref{subsec:Konzept_Datenerhebung} erstellt. Ein Scheduler wird für die Optimierung B ebenfalls erzeugt \fullref{subsec:Konzept_Optimierung02}. Danach wird die Trainingsprozedur durchgeführt, welche in \autoref{fig:Implementierung_Train_Method} dargestellt ist.\\
Zu Beginn wird eine Obs bestehend aus AV (around\_view) und SO (scalar\_obs) durch die reset Methode \fullref{subsec:Konzept_Schnittstelle} generiert. 
Diese Obs wird dem Agenten übergeben, welcher eine Aktion und weitere Daten zurückgibt.\\
Danach wird die bestimmte Aktion mit der step Methode im Env ausgeführt \fullref{subsec:Konzept_Schnittstelle}.
Anschließend wird das Memory mit den entstandenen Erfahrungen aktualisiert und die alte Obs wird durch die neue ersetzt.\\
Sollte die Spielepisode terminieren, so wird die learn Methode des Agent aufgerufen und die generierten Episodendaten werden in die Datenhaltungslisten eingefügt.\\
Wird die Optimierung B verwendet, so wird alle 100 Spielepisoden die Steigung der Performance der letzten 100 Trainingsspiele bzw. Epochs ermittelt. Sollte diese nicht größer als null sein, so wird die Lernrate mit 0.95 multipliziert.\\
Sind alle 30.000 Epochs abgeschlossen, werden das NN und die Trainingsdaten gespeichert und die Methode terminiert.\\
\begin{figure}[H]
	\centering
	\includesvg[scale=0.105]{Training_PPO}
	\caption[Ablaufdiagramm der train Methode]{Ablaufdiagramm der Train Methode}
	\label{fig:Implementierung_Train_Method}
\end{figure}
Neben der train Methode wird jedoch auch eine Testmethode zur Erstellung von Testdaten benötigt.

\subsection{Test Methoden} \label{sec:Implementierung_test_Methode}
Die Testmethoden sind bis auf wenige Ausnahmen mit den train Methoden übereinstimmend. Es werden in diesem Abschnitt daher nur die Unterschiede aufgezeigt. Einer dieser Unterschiede ist, dass das NN geladen und nicht erzeugt wird. Mit diesem wird dann der Test durchgeführt. Alle Elemente des Lernens sind aus der test Methode entfernt worden. Zu diesen gehören der Scheduler, das Aufrufen der learn Methode und das Speichern des NNs.
Hinzukommt die Prozedur, um die Spielfeldgröße zu ändern, für die Bestimmung der Robustheit \fullref{subsec:Konzept_Datenverarbeitung}.
Des Weiteren wird für die Aktionsbestimmungen nur die act\_test Methode verwendet und es kommt die Funktionalität der grafischen Umsetzung hinzu.
Ansonsten treten keine Unterschiede zwischen den train und test Methoden auf.

\section{Statistik} \label{sec:Implementierung_Statistiken}
Die Statistiken werden mit der generate\_statistic Funktion erstellt.
Zuerst werden alle CSV-Dateien, welche die Test- und Trainingsdaten beinhalten, eingelesen. Dabei findet auch die Mittelung der Daten aus den verschiedenen Datenerhebungen \fullref{subsec:Anforderungen_mehrfache_Datenerhebung} statt.
Danach werden die eingelesenen Daten jedem Agenten zugeordnet und dann der make\_statistics Methode übergeben, wo die eigentlichen Statistiken generiert und gespeichert werden.