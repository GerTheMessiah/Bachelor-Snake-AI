\chapter{Implementierung}
Für eine Umsetzung eines solchen Vergleichs, wie er in dem Kapitel \ref{chap:Vorgehen} beschrieben worden ist, ist es nötig eine Implementierung des Spiels Snake und der beiden Agenten, inklusive der Ablaufroutine, durchzuführen. Als Programmiersprache wurde Python (3.7) gewählt.\\
Python bietet im Bereich des DRL eine Vielzahl an Frameworks, welche nicht nur bei der Implementierung des Envs. helfen, sondern auch welche, die Funktionalität der Neuronalen Netzwerke bereitstellen.

\section{Snake Environment}
Zur Implementierung des Spiels Snake wurde das Framework gym von OpenAI genutzt (\url{https://gym.openai.com/}). Dieses bietet viele Methoden und Vorgaben in der Projektstruktur, welche das Implementieren erleichtern. So besteht das Snake Environment Package (snake\_env), aus den wesentlichen Files:

\begin{itemize}
	\item gui
	\item observation
	\item snake\_env
	\item snake\_game
\end{itemize}

\subsection{Schnittstelle} \label{sec:Impl_Schnittstelle}
Die grundlegende Schnittstelle des Snake Env. wird in dem File snake\_env bereitgestellt. In diesem wird die Klasse SnakeEnv definiert, welche durch die Vererbung der Oberklasse gym.Env zentrale Methoden übernimmt. Zu diesen gehören die step, reset, render und close Methode.\\
Weiterhin bietet die selbst definierte Methode post\_init die Möglichkeit zentrale Einstellungen, wie z.B. die Spielfeldgröße und GUI-Aktivierung, zu editieren. 
Die step-Methode startet den Ausführungsprozess der vom Agenten ausgewählten Action, durch Aufrufen weiterer Methoden \ref{sec:Impl_Spiellogik}. Nach der Abarbeitung wird die neue Obs, der Reward, das done-flag und das has\_won-flag übermittelt, wobei letzteres zur Bestimmung des Sieges dient.
Die reset-Methode startet das Spiel von neuem. Zum Schluss wird dann noch die Obs. zurückgegeben.
Die render-Methode ruft die Methoden für die visuelle Darstellung auf.
Die close-Methode terminiert das Programm.\\
Auf Basis der oben genannten Methoden wird ersichtlich, dass es sich bei der Klasse SnakeEnv um eine Wrapper-Klasse handelt, die als Schnittstelle dient.

\subsection{Spiellogik} \label{sec:Impl_Spiellogik}
Die Spiellogik, welche durch die step-Methode \ref{sec:Impl_Schnittstelle} angestoßen wird, befindet sich im snake\_game File, welches eine Klasse namens SnakeGame definiert. Neben einigen Methoden werden im SnakeGame-Objekt auch viele spielbezogene Daten, wie z.B. das Spielfeld (ground), ein Player-Objekt (p),
ein GUI-Objekt (gui) und die Spielfeldgröße (shape), den Schrittzähler (step\_counter) und eine Hilfsvariable (has\_grown), die für die Bestimmung des Reward benötigt wird.\\
Der Weiteren werden die folgenden Methoden implementiert:
\begin{longtable}[h]{|p{4cm}|p{\linewidth - 5cm}|}
	\caption{Methoden der SnakeGame Klasse}
	\label{tab:methods_of_SnakeGame} 
	\endfirsthead
	\endhead
	\hline
	Methode & Erklärung \\
	\hline
	action & action ist für die Ausführung der Aktionen verantwortlich.\\
	\hline
	evaluate & evaluate bestimmt den, durch die Ausführung der Action, zu erhaltenden Reward.\\
	\hline
	observe & observe stößt den Generierungsprozess der Obs an, welcher ausgelagert im observation File liegt. \\
	\hline
	make\_apple & make\_apple erzeugt einen neuen Apfel auf dem Spielfeld. \\
	\hline
	reset\_snake\_game & reset\_snake\_game setzt den Spielfortschritt zurück und startet es von neuem. \\
	\hline
	max\_snake\_length (getter) & Gibt die maximale Länge der Snake zurück. \\
	\hline
	is\_done (getter) & Liefert den Lebensstatus (player.done). \\
	\hline
\end{longtable}

Zum Erhalt eines tieferen Verständnisses über die Spiellogik, soll diese exemplarisch erläutert werden. Bevor dies jedoch geschehen kann muss noch die Datenhaltungsklasse Player erwähnt werden, die ebenfalls im Snake\_game File definiert ist.
Diese speichert spielerbezogen Daten, wie z.B. die Position aller Schwanzglieder inkl. des Kopfes (pos, tail), Blickrichtung (direction), die Anzahl der Schritte seit dem letzten Fressen (inter\_apple\_steps), den Lebensstatus (done) und einige Konstanten, welche für die Visualisierung benötigt werden (id, c\_s und c\_h). Zuzüglich besitzt die Player Klasse noch eine player\_reset Methode und einige getter Methoden.\\

Da Snake ein zweidimensionales Spiels ist, wird eine gleich dimensionale Matrix (ground) zur Spieldarstellung verwendet.
So kann mit dn Zeilen- und Spaltenindexen der Matrix die Position der Snake deutlich gemacht werden. So wird der Schwanz mit der Konstante c\_s, der Kopf mit c\_h, der Apfel mit -2 und das Ende der Snake mit -1, in der Matrix (ground) deutlich gemacht.
Der Einfachheit halber werden die Zeilen- und Spaltenindexe, daher die Position, ebenfalls noch in einer List (tail) festgehalten. Dieser erleichtert später die Feststellung des Lebensstatus (done).\\
\\Mit dem folgenden Aktivitätsdiagramm soll der Fokus weiter auf die action-Methode gelegt werden. Diese wird wie folgt abgearbeitet.
Als erstes wird der inter\_apple\_steps erhöht. Sollte dieser Zähler größer als, die vorher definierte, Obergrenze sein, so wird der Lebensstatus auf tot gesetzt (p.done = True) und die Methode wird terminiert. Im weiteren Verlauf wird diese Unterprozedur Abbruchprozedur genannt.\\
Anderenfalls wird als nächstes überprüft, um welche action es sich handelt, wobei die actions mit den Zahlen von null - zwei kodiert sind.\\
\\Entsprechende der Action wird die direction des Players angepasst. Die vier Himmelsrichtungen werden dabei mit den Zahlen von 0 - 3 dargestellt, wobei null Norden entspricht eins Osten usw.\\
Nach der Manipulation der direction des Players, wird pos, also die vorläufige neue Position, im Player-Objekt angepasst. 
Diese Änderung wird jedoch noch nicht sofort in die Matrix übertrage, da das Eintragen on Positionen außerhalb des Spielfeldes zu Fehlern führen würde. Es muss daher erst überprüft werden, ob die neue Position des Players im Spielfeld liegt. Sollte dies nicht der Fall sein, so wird die Abbruchprozedur aufgerufen.\\
Anderenfalls wird die neue Position in tail eingefügt.\\
Zu diesem Stand der Abarbeitung ist es möglich, dass das Spiel bereits gewonnen ist. Um dies zu überprüfen, wird die Länge der Snake mit der maximal möglichen Länge, welche sich durch die Spielfeldgröße ergibt, verglichen. Entspricht die Länge Snake der maximal mögliche, so wird die Abbruchprozedur aufgerufen.
Ansonsten muss als nächster Schritt die Matrix aktualisiert werden. Jedoch muss vorher festgestellt werden, ob die Snake einen Apfel gefressen hat.\\
Sollte sie dies getan haben, so wird die neue Position des Kopfes in die Matrix eingepflegt, ein neuer Apfel wird auf einer zufälligen freien Stelle generiert, inter\_apple\_steps wird auf null und die Hilfsvariable has\_grown wird auf True gesetzt. Letztere wird von der evaluate Methode verwendet, um di Höhe des rewards zu bestimmen, siehe \ref{sec:Impl_Reward_Function}.\\
Ist die Snake jedoch nicht gewachsen, so wird das letzte Schwanzglied aus Matrix und Liste gelöscht, um den Anschein von Bewegung zu erwecken. Zuzüglich wird  die Hilfsvariable has\_grown auf False gesetzt.\\
Zum jetzigen Zeitpunkt besteht immer noch die Möglichkeit, dass die Snake in sich selber gelaufen ist. Um dies festzustellen, wird tail auf Duplikate überprüft. Sollten sich Duplikate in tail befinden, wird die Abbruchprozedur aufgerufen.\\
Ansonsten wird zum Schluss noch über tail iteriert und die korrespondierenden Einträge der Matrix (ground) werden mit den Positionen von tail aktualisiert, wobei der Kopf und das letzte Schwanzglied mittels eines anderen Zahlenwert dargestellt werden.

\subsection{Graphische Oberfläche} \label{sec:Impl_GUI}
Im gui File wird eine Klasse GUI definiert, welche das Spiel mit Hilfe des Frameworks pygame (\url{https://www.pygame.org/}) darstellt. Dazu wird in der GUI-Klasse ein Oberfläche (screen) erzeugt, welche die Matrix ground darstellt, siehe \ref{fig:Game_of_Snake}. 
Die Methode update\_GUI, welche von der SnakeGame Methode view aufgerufen wird, überschreibt dazu jeden einzelnen Eintrag des pygame-Spielfelds mit dem korrespondierenden Wert von ground. 
Spielfeld (GUI) und Matrix sind daher nicht direkt gekoppelt sondern müssen über update\_GUI angeglichen werden.\\
Die Fenstergröße der Spieloberfläche wird dynamisch berechnet und kann über das Attribut Particle verändert werden, welches die Feldgröße eines einzelnen Matrixeintrags darstellt. 
Die draw Methode ist für das Generieren der einzelnen Spielfeld-Rechtecke zuständig und reset\_GUI versetzt das Spielfeld zurück in den Ursprungszustand.


\section{Agenten}
Dieser Teil der Implementierung soll sich mit den Agenten befassen. Dabei wird näher auf die Netzstruktur, den Aktionsauswahlprozess die Lern-Methode, den Speicher (Replay Buffer) und die Hauptausführungsmethode (main Methode) eingegangen.

\subsection{DQN} \label{sec:Impl:DQN}
Der DQN Algorithmus ist einer der beiden Algorithmus-Arten, welche im Rahmen dieser Ausarbeitung, implementiert wurden. Diese Implementierung findet hauptsächlich in vier Files statt, welche im dqn Package liegen, dass wiederum zum agents Package gehört.
Im dqn File wird die Agentenklasse definiert, welche die Aktionsbestimmungsmethode act und die lern Methode enthält. 
Im memoryDQN File wird die Memory-Klasse definiert, welche den Replay-Buffer darstellt \ref{sec:Q-Learning}. 
Die Files dqn\_train und dqn\_play beinhalten die eigentlichen main-Methoden, welche Agenten und Env. erstellen und den Trainings- bzw. Spielprozess umsetzt.

\subsubsection{Main-Methode} \label{sec:Impl:Main_Methode}
Die Hauptmethode oder auch main Methode genannt implementiert den eigentlichen Spiel und Trainingsablauf. Die main Methde des DQN ist im dqn Package im File dqn\_train und dqn\_play zu finden. Letzteres wird für das reine visuelle Spielen genutzt und wird daher nicht weiter betrachtet.\\
Um den Spiel- und Trainingsablauf durchzuführen, wird in dem File dqn\_train eine Methode train\_dqn definiert, welche wichtige Hyperparameter des Ablaufen, wie z.B. die Anzahl der zum Training zu absolvierenden Spiele (N\_ITERATIONS), die Lernrate (LR) und die Spielfeldgröße (BOARD\_SIZE), die Batch Size (BATCH\_SIZE), die Maximale Größe des Speichers (MAX\_MEM\_SIZE), Epsilon Decrement (EPS\_DEC) und Epsilon End (EPS\_END) übergeben bekommt.\\
Zu Beginn werden Datenhaltungslisten apples, wins, dtime, eps, steps\_list initialisiert, welche für jedes absolvierte Spiel die, dem Namen der List entsprechenden, Werte speichert. Nach der Erstellung des Agenten und Environments startet der Spielverlauf.\\
Dabei wird wie in \ref{sec:Funktionsweise} vorgegangen. Der Agent erhält eine Obs, bestimmt seine Action und diese wird sogleich im Env ausgeführt. Danach werden die neue Obs sowie Reward und weitere Statusinformationen, wei z.B. das done-Flag usw., ausgegeben. Diese Daten werden im Memory für das sich anschließende Training gespeichert. Dieses wird wie in \ref{sub:Trainingsprozess} durchgeführt. Danach werden die oben genannten Datenhaltungslisten aktualisiert und die Prozedur beginnt von neuem. Wenn diese N\_ITERATIONS Spiele alle absolviert wurden werden die Erhobenen Daten aus den Listen in einem CSV-File gespeichert und die Methode wird terminiert. 

\subsection{PPO}
Der PPO Algorithmus stellt die zweite Algorithmus-Art dar. Seine Implementierung findet im ppo Package, welches die Files actor, actor\_critic, critic, memoryPPO, ppo, ppo\_play und ppo\_train beinhaltet. actor und critic implementieren die entsprechenden NN, actor\_critic stellt eine Verwaltungs-Klasse dar, welche den Optimizer und die act (choose\_action) und die evaluate Methode beinhaltet, wobei letztere für den Trainingsprozess benötigt wird. MemoryPPO definiert das Memory (Replay Buffer), welches Erfahrungen speichert und im ppo File wird die Agentenklasse mit der learn Methode erstellt. ppo\_train und ppo\_play besitzen analoge Funktionalitäten wie in \ref{sec:Impl:DQN} eingangs erwähnt.

\subsubsection{Aktionsauswahlprozess} \label{sec:Impl_PPO_Aktionsauswahlprozess}
Der Aktionsauswahlprozess wird im actor\_critc File implementiert. Von diesem existieren zwei Instanzen, welche die alte und neue Policies darstellen. Der act Methode der alten Policy wird die momentane Obs, welche aus AV (around\_view) und SO (scalar\_obs) besteht, übergeben und zu PyTorch Tensoren umgewandelt. Danach wird diese durch das Actor-NN (Policy-NN) durch propagiert, sodass eine Policy übergeben wird. Auf dieser Wahrscheinlichkeitsverteilung wird dann die nächste Action bestimmt, welche sogleich mit der logarithmierten Wahrscheinlichkeiten der ausgewählten Action und der zu Tensoren umgewandelten Obs, ausgegeben wird.

\subsubsection{Trainingsprozess}
Der Trainingsprozess ist in der Agentenklasse implementiert, welche sich im ppo Package befindet. Zu Beginn wird aus dem Memory ein oder mehrere Mini-Batch/es generiert. Um den Return \ref{sec:Return} zu erhalten, werden die einzelnen in Rewards diskontiert. Sollte ein terminalen Zustand vorhanden sein, werden die discounted Rewards auf null gesetzt, da diese dem zu erwarteten Reward bis zum Ende der Spielepisode entsprechen. Da die Episode terminiert, können keine weiteren Rewards mehr gesammelt werden.\\
Um ein gleichmäßigeres Lernen zu unterstützen, werden die Rewards nach dem sie zu einem Tensoren umgewandelt worden sind, normalisiert. Danach wird die folgende Prozedur (K\_epochs) mal ausgeführt, um das NN zu trainieren. Danach terminiert die learn Methode\\
Zuerst wird die evaluate Methde der ActorCritic-Klasse aufgerufen welche sich im actor\_critic File befindet. Dieser werden die Obs aus dem Mini-Batch bzw. Mini-Batches übergeben. Zusätzlich werden noch die alten korrespondierenden Actions übergeben, welche zur Bestimmung von $\pi_{\theta}(a_{t}|s_{t})$ \ref{sec:Probability_Ratio} dient. Evaluate gibt neben $\pi_{\theta}(a_{t}|s_{t})$ auch noch die State Values \ref{sec:Baseline_Estimate} und die Entropie der neuen Policy $\pi_{\theta}$ aus, siehe \ref{sec:PPO_Training_Objective_Function}.\\
\ref{sec:Probability_Ratio} entsprechend werden als nächstes die Probability Ratios über alle Erfahrungen des Mini-Batch/es bestimmt. Daraufhin folgt die Bestimmung der Advantages, welches entsprechend zu \ref{sec:Advantages} geschieht. Nach der Berechnung beider Surrogates Loss wird die Surrogate Objective Loss gebildet \ref{sec:Surrogate_Objectives}.\\
Als nächstes erfolgt die Bestimmung des Value Loss \ref{eq:Value_Loss} und die des Entropy Loss, wobei letzterer durch die evaluate Methode bereits bestimmt wurde.\\
Zum Schluss werden alle Losses entsprechend \ref{sec:PPO_Training_Objective_Function} zusammengefügt. Die NN's werden daraufhin durch mit Hilfe des PyTorch Frameworks durch Backpropagation und Gradient Desent \ref{Backprop_GD} angepasst.\\
Da der PPO mit unterschiedlichen Policy arbeitet, wird vor der Terminierung der Trainingsmethode die alte Policy $\pi_{\theta_{\text{old}}}$ durch die neu erzeugte $\pi_{\theta}$ ersetzt. Trainiert wird immer auf der neuen Policy $\pi_{\theta}$ und die Daten werden stets auf der alten $\pi_{\theta_{\text{old}}}$ erhoben.

\subsubsection{Main-Methode}
Die main Methode des PPO ist bis auf kleinere Unterschiede analog zu der des DQN Algorithmus, siehe \ref{sec:Impl:Main_Methode}. Sie befindet sich einzig im ppo\_train File, in ihr werden keine DQN spezifischen Daten, wie z.B. epsilon, abgespeichert und es existieren zwei Lernraten für Actor und Critic. 
