\chapter{Implementierung} \label{chap:Implementierung}
\graphicspath{{Abbildungen/Implementierung/}}
Für eine Umsetzung eines solchen Vergleiches, wie er im Kapitel \ref{chap:Konzept} erwähnt wurde, wird eine Implementierung des Spiels Snake, der beiden Algorithmen, der Ablaufroutinen sowie der Statistik-Erzeugung durchzuführen. Als Programmiersprache wurde Python (3.7) gewählt.
Python bietet im Bereich des Maschine Learning eine Vielzahl an Frameworks, welche nicht nur bei der Implementierung des Envs. helfen, sondern auch die Funktionalität der Neuronalen Netzwerke bereitstellt.
In dieser Implementierung wurde sich für das Maschine Learning Framework PyTorch (\url{https://pytorch.org/}) entschieden. Dieses erlaubt das Konzipieren der im Abschnitt \ref{sec:Konzept_Netzstruktur} vorgestellten NNs.

\section{Package Struktur}
Für eine bessere Abtrennung der einzelnen Komponenten, wurde sich für die folgende Projektstruktur entschieden.
\begin{figure}[H]
	\centering
	\def\svgscale{0.095}
	\input{Abbildungen/Implementierung/Package-Structur.pdf_tex}
	\caption[Package Struktur]{Darstellung der Package Struktur.}
	\label{fig:Package_Struktur}
\end{figure}
Alle Ordner werden in src aufbewahrt. Dieser besitzt die vier Unterordner resources, snakeAI, statistic common.\\
In dem Ordner resources werden alle erzeugten Daten der Trainings- und Testläufe sowie einige weitere images für die Projektpräsentation aufbewahrt.\\
Der Ordner statistic beinhaltet die Dateien, zur Auswertung der Trainings- und Testdaten.\\
Der snakeAI Ordner dient als Sammelordner zur Lagerung des Env und der Agenten (siehe \ref{fig:AV-Network}).

\section{Snake Environment} \label{sec:Implementierung_Environment}
Zur Implementierung des Spiels Snake wurde das Framework gym von OpenAI genutzt (siehe \url{https://gym.openai.com/}). Das Snake Environment, welches im snake\_env Package liegt (siehe \ref{fig:Package_Struktur}), besteht aus den wesentlichen Dateien gui, observation, reward, snake\_env und snake\_game.
\\Die Spiellogik ist hauptsächlich in der action Methode implementiert, welche sich in der snake\_game Datei befindet. Aus diesem Grund wird sie im weiteren Verlauf besonders erklärt.\\
\\Die action Methode bekommt zuerst die Aktion als Ganzzahl übergeben. Nach der Überprüfung, ob die maximale Schrittanzahl bereits überschritten worden ist, wird diese umgesetzt. Dabei wird die direktion der Snake, welche mit den Zahlen von eins bis vier codiert ist, entsprechend der Aktion manipuliert. Anschließend wird der Schritt theoretisch ausgeführt, in dem die Position, welche durch ein eindimensionales Array der Länge zwei dargestellt wird, (p.pos) angepasst wird.\\
Danach muss die theoretischen Position überprüft werden, ob sie außerhalb des Spielfelds liegt und damit der Tod eingetreten ist. Sollte dies der Fall sein, so wird die Methode terminiert.
Anschließend wird die neue Position in die Positionsliste, welche über Positionen aller Schwanzglieder verfügt und eine der beiden grundlegenden Datenstrukturen des Env darstellt, eingepflegt. 
Danach wird das Spiel auf einen Sieg überprüft. Sollte dieser eingetreten sein, so wird die Methode terminiert. Anderenfalls wird die neue Position auf dem Spielfeld (zweidimensionales Array), welches die zweite grundlegende Datenstruktur darstellt, aktualisiert. Nach der Überprüfung, ob die Snake nicht in sich selbst gelaufen ist, wird die Methode terminiert. Sollte die Snake nicht in sich selbst gelaufen sein, so wird das Spiel nicht terminiert anderenfalls schon.\\
\\Die Observation wird in von der make\_obs Methode, welche in der observation Datei definiert wurde, erstellt. 
Diese wird dabei nach der Aktionsbestimmung, durch die observe Methode, aufgerufen. Diese observe Methode wird jedoch wiederum durch die Schnittstellenmethode step aufgerufen. Dabei wird wie im Konzept beschrieben vorgegangen (siehe \ref{sec:Konzept_Observation}).\\
\\Ähnlich verhält es sich mit der evaluate Methode, die ebenfalls nach Ausführung der action Methode aufgerufen wird. Diese ruft die jeweilige Reward Funktion auf, welche in der reward Datei definiert ist (siehe \ref{sec:Konzept_Reward}).\\
\\Die reset\_snake\_game Methode, welche in der SnakeGame Klasse definiert wurde, wird immer nach einem Sieg oder Verlust eines Spiels, durch die Schnittstellenmethode reset, aufgerufen. Sie setzt den Spielfortschritt zurück und liefert, durch Aufrufen der observe Methode, eine neue Obs, um das nächste Spiel zu starten.\\
\\Um die Anforderung einer graphischen Oberfläche zu beachten, wurde die view Methode in der SnakeGame Klasse erstellt, welche in der GUI Klasse, die in der gui Datei definiert wurde, wiederum Methoden zur Erstellung einer graphische Oberfläche aufruft. Die view Methode wird dabei durch die Schnittstellenmethode render aufgerufen.

\section{Agenten}
Die Agenten befinden sich im agent Unterordner, wie in Abbildung \ref{fig:Package_Struktur} dargestellt.

\subsection{AV-Network} \label{sec:Implementierung_AV_Network}
Das AV-Network stellt die Netzstruktur dar, welche die AV (around\_view) verarbeitet. Um dies bewerkstelligen zu können, wird das Netz mithilfe des PyTorch Frameworks erstellt. Dabei wird so verfahren, wie es im Konzept (siehe \ref{sec:Konzept_Netzstruktur}) gefordert ist. Eine genauere Darstellung der Implementierung des AV-Networks findet sich im Anhang (siehe \ref{sec:Anhang_AV_Network}).

\subsection{DQN}
Der DQN Algorithmus, aus welchen die DQN Agenten hervorgehen, befindet sich im gleichnamigen Unterordner. Er beinhaltet die folgenden Dateien:\\
Die dqn Datei ist die Hauptdatei. In ihr wird die Agenten Klasse definiert, welche die act und learn Methoden implementiert. Die Agent Klasse ist dabei als DQN-Komponente zu interpretieren.\\
Die memoryDQN Datei implementiert den Replay Buffer bzw. die MemoryDQN Klasse, in welcher die gesammelten Erfahrungen gespeichert werden.\\
Die q\_net Datei beinhaltet die Netzstruktur des Q-Networks.\\
Die dqn\_train Datei implementiert die Trainingsroutine, welche den Agenten trainiert und die Trainingsdaten generiert und speichert.\\
Die dqn\_test Datei implementiert die Testroutine, welche die Testdaten generiert und abspeichert.

\subsection{Q-Network} \label{sec:Implementierung_Q-Network}
Das Q-Network ist in der q\_net Datei definiert und besteht aus dem AV-Network (AV\_NET) (siehe \ref{sec:Implementierung_AV_Network}) und dem Q-Network-Tail (Q-net).
\begin{python}
class QNetwork(nn.Module):
	def __init__(self, OUTPUT, LR, SCALAR_INPUT=41, DEVICE="cuda:0"):
		super(QNetwork, self).__init__()
		T.set_default_dtype(T.float64)
		T.manual_seed(10)
		self.AV_NET = AV_NET()
		self.Q_net = nn.Sequential(
			nn.Linear(128 + SCALAR_INPUT, 64),
			nn.ReLU(),
			nn.Linear(64, OUTPUT)
		)
		self.OPTIMIZER = Adam(self.parameters(), lr=LR)
		self.DEVICE = DEVICE
		self.to(self.DEVICE)
	
	def forward(self, av, scalar_obs):
		av_out = self.AV_NET(av)
		cat = T.cat((av_out, scalar_obs), dim=-1)
		q_values = self.Q_net(cat)
		return q_values
\end{python}
\begin{lstlisting}[caption=PyTorch Implementierung des QNetwork]
\end{lstlisting}
Das Q-net besteht dabei aus den, im Konzept erwähnten, Elementen (siehe \ref{sec:Konzept_Netzstruktur}).
Die forward Methode leitet die AV (av) durch das AV\_NET (siehe \ref{sec:Anhang_AV_Network}). Danach wird der Output des AV\_Net mit der SO (scalar\_obs) verbunden und durch das Q\_net geleitet. Das Ergebnis wird zurückgeliefert.

\subsubsection{Memory} \label{sec:Implementierung_Memory}
Das Memory oder auch Replay Buffer genannt, befindet sich in der memoryDQN Datei. Dort wird die Klasse Memory definiert. Dabei besteht das Memory aus einer Reihe von Tensoren, welche die von Spiel und Agenten generierten Daten, mittels einer zusätzlichen Dimension, speichern.\\
Sollte die AV also die Form (6x13x13) besitzen, so besitzt der Speicher-Tensor (AV) die Form (MEM\_SIZEx6x13x13).
Das Memory verfügt über eine get\_data Methode, welche einen zufallsbasierten Batch zurückliefert. Die Implementierung des Memory richtet sich nach den Ausführungen in Abschnitt \ref{sec:Konzept_DQN}.

\subsubsection{DQN-Agent} \label{sec:Implementierung_DQN_Agent}
Der DQN-Agent besteht aus den act Methoden, welche die Aktionen bestimmen und aus der learn Methode, die für das Training des Agenten zuständig ist.\\
\\\textbf{Aktionsbestimmung} \label{sec:Implementierung_act_DQN}\\
In der Agenten Klasse werden die zwei Methoden act und act\_test definiert. Die act Methode wird dabei analog zur Beschreibung im Konzept (siehe \ref{sec:Konzept_Aktionsauswahlprozess_DQN})).
Die act\_test Methode ist für die Aktionsbestimmung während der Testläufe zuständig und ermittelt seine Aktionen ausschließlich mittels des Q-Networks (siehe Code Listing \ref{code:Aktionsbestimmung}).
\begin{python}
@T.no_grad()
def act_test(self, av, scalar_obs):
	av = T.from_numpy(av).to(self.Q_NET.DEVICE)
	scalar_obs = T.from_numpy(scalar_obs).to(self.Q_NET.DEVICE)
	q_values = self.Q_NET(av, scalar_obs)
	return av, scalar_obs, T.argmax(q_values).item()
\end{python}
\begin{lstlisting}[caption=Aktionsbestimmung, label=code:Aktionsbestimmung]
\end{lstlisting}
\textbf{Trainingsroutine} \label{sec:Implementierung_learn_DQN}\\
Die learn Methode überprüft als erstes, ob sich aus dem Memory genügend Daten für einen Mini-Batch entnehmen kann. Sollte dies nicht der Fall sein, so terminiert die Methode.
Danach wird die get\_data Methode des Memory aufgerufen, welche einen Mini-Batch liefert. Danach werden die Schritt, entsprechend der Beschreibung im Konzept (siehe \ref{sec:Konzept_Lernprozess_DQN}), durchgeführt.\\
Zu Beginn werden die Q-Values der gespeicherten Aktion für die gegenwärtigen Zustände bestimmt. Danach folgen Q-Values aller Aktionen der Nachfolgezustände (siehe Code Listing \ref{code:Bestimmung_Q-Values}).
\begin{python}
av, scalar_obs, actions, rewards, is_terminal, av_, scalar_obs_, batch_index = self.MEM.get_data()
q_eval = self.Q_NET(av, scalar_obs)[batch_index, actions]
q_next = self.Q_NET(av_, scalar_obs_)
\end{python}
\begin{lstlisting}[caption=Bestimmung der Q-Values, label=code:Bestimmung_Q-Values]
\end{lstlisting}
Daraufhin wird, wie in den Abschnitten \ref{eq:DQN_Loss} und \ref{sec:Konzept_Lernprozess_DQN} dargestellt, Q-Target ($r(s,a) +\gamma \max_{a'}Q(s',a';\theta_{i-1})$) bestimmt, siehe auch Code Listing \ref{code:Bestimmung_Q-Target}.
\begin{python}
q_next[is_terminal] = 0.0
q_target = rewards + self.GAMMA * T.max(q_next, dim=1)[0]
\end{python}
\begin{lstlisting}[caption=Bestimmung von Q-Target, label=code:Bestimmung_Q-Target]
\end{lstlisting}
Zum Schluss wird der Loss des DQN bestimmt und das Q-Net aktualisiert.
\begin{python}
loss = self.LOSS(q_target, q_eval)
self.Q_NET.OPTIMIZER.zero_grad()
loss.backward()
self.Q_NET.OPTIMIZER.step()
\end{python}
\begin{lstlisting}[caption=Bestimmung des DQN Loss \& Update des Q-Networks, label=code:Bestimmung_DQN-Loss]
\end{lstlisting}
Zusätzlich wird noch Epsilon verringert, um die Anzahl an Zufallsaktionen während des Trainings zu senken.

\subsection{PPO}
Der PPO Algorithmus, aus welchen die PPO Agenten hervorgehen, befindet sich im gleichnamigen Unterordner. Er beinhaltet die folgenden Dateien:\\
Die ppo Datei stellt die Hauptdatei dar. In dieser wird die Agenten Klasse definiert, welche die learn Methoden implementiert. Die Agenten Klasse ist dabei als PPO-Komponente zu interpretieren (siehe \ref{sec:Konzept_PPO}).\\
Die memoryPPO Datei implementiert den Replay Buffer bzw. das Memory, in welches die gesammelten Erfahrungen gespeichert werden.\\
Die actor bzw. critic Datei beinhaltet die Netzstruktur des Actors bzw. Critics.\\
In der actor\_critic Datei befindet sich die ActorCritic Klasse, welche Actor-NN und Critic-NN miteinander verbindet.\\
Die ppo\_train bzw. ppo\_test Datei implementiert die Trainingsroutine bzw. Testroutine.

\subsubsection{Actor und Critic}
Der Actor bzw. Critic wird durch die Klasse ActorNetwork bzw. CriticNetwork aufgespannt, welche von der Module Klasse des PyTorch Frameworks erbt. Damit lassen sich ActorNetwork und CriticNetwork als NN-Baustein benutzen.\\
Neben dem AV-Network (siehe \ref{sec:Implementierung_AV_Network}) wird noch der Actor-Tail bzw. Critic-Tail definiert. Dabei handelt es sich um das NN, welches die Ausgabe vom AV-Network mit der SO verbindet und aus diesem Ergebnis eine Wahrscheinlichkeitsverteilung über alle Aktionen bzw. einen Value Wert generiert. 
Exemplarisch wird die Klasse des ActorNetworks im Code Listing \ref{code:ActorNetwork} dargestellt. Analog verhält es sich beim Critic, mit der Ausnahme, dass dieser nicht Tensoren der Länge drei sondern der Länge eins ausgibt. Diese stellen die Baseline Estimates bzw. Value Werte (siehe \ref{sec:Baseline_Estimate}) dar.
\begin{python}
class ActorNetwork(nn.Module):
	def __init__(self, OUTPUT=3, SCALAR_IN=41):
		super(ActorNetwork, self).__init__()
		self.AV_NET = AV_NET()
			
		self.ACTOR_TAIL = nn.Sequential(
			nn.Linear(128 + SCALAR_IN, 64),
			nn.ReLU(),
			nn.Linear(64, OUTPUT),
			nn.Softmax(dim=-1)
		)
		
	def forward(self, AV, SCALAR_OBS):
		av_out = self.AV_NET(AV)
		cat = T.cat((av_out, SCALAR_OBS), dim=-1)
		actor_out = self.ACTOR_TAIL(cat)
		return actor_out
\end{python}
\begin{lstlisting}[caption=ActorNetwork, label=code:ActorNetwork]
\end{lstlisting}

Die forward Methoden von Actor und Critic sind dabei analog zu der, welche im Abschnitt \ref{sec:Implementierung_Q-Network} vorgestellt wurde.

\subsubsection{ActorCritic} \label{sec:Implementierung_ActorCritic}
Die ActorCritic Klasse ist im actor\_critic File definiert und verbindet Actor und Critic miteinander.
Wie auch schon die ActorNetwork und CriticNetwork Klassen, erbt die ActorCritc Klasse von Module. Sie kann daher aus den NNs ein gesamt Network aufspannen, welches alle Parameter der beiden NNs besitzt.
Die ActorCritic Klasse implementiert drei Methoden, zu diesen gehören act, act\_test und die evaluate Methode.\\
\\\textbf{Act Methoden} \label{Implementierung_act_PPO}\\
Die PPO Agenten verfügen, wie die DQN Agenten, über zwei act Methoden. Die erste leitet die AV und SO durch das ActorNetwork und erhält eine Wahrscheinlichkeitsverteilung über alle Aktionen. Diese wird auch als Policy bezeichnet.
\begin{python}
@T.no_grad()
def act(self, av, scalar_obs):
	av = T.from_numpy(av).to(self.DEVICE)
	scalar_obs = T.from_numpy(scalar_obs).to(self.DEVICE)
	policy = self.ACTOR(av, scalar_obs)
	dist = Categorical(policy)
	action = dist.sample()
	return av, scalar_obs, action.item(), dist.log_prob(action)
\end{python}
\begin{lstlisting}[caption=Darstellung der act Methode, label=code:act_Methode]
\end{lstlisting}
Daraufhin wird eine Aktion entsprechend der Wahrscheinlichkeitsverteilung bestimmt und zusammen mit ihrer logarithmierten Wahrscheinlichkeit und den Tensor AV und SO zurückgegeben.\\
\\Die act\_test Methode verzichtet, wie beim DQN, wieder auf Zufallselemente. 
Die Vorgehensweise ist größtenteils analog zur act Methode (siehe \ref{code:act_Methode}).
\begin{python}
@T.no_grad()
def act_test(self, av, scalar_obs):
	av = T.from_numpy(av).to(self.DEVICE)
	scalar_obs = T.from_numpy(scalar_obs).to(self.DEVICE)
	policy = self.ACTOR(av, scalar_obs)
	return av, scalar_obs, T.argmax(policy).item()
\end{python}
\begin{lstlisting}[caption=Darstellung der act\_test Methode, label=code:act_test_Methode]
\end{lstlisting}
Am Ende der act\_test Methode (siehe \ref{code:act_test_Methode}) wird jedoch lediglich die Aktion ausgewählt, welche die größte Wahrscheinlichkeit vorweist.\\
\\\textbf{Evaluate Methode}\\
Die evaluate Methode, bestimmt die logarithmierte Wahrscheinlichkeit einer Aktion unter einer anderen bzw. älteren Policy (siehe \ref{sec:Policy}). Neben dieser, bestimmt sie auch noch den Value-Wert des Critics und ermittelt die Entropy der Policy. All diese Werte sind essentiell für die learn Methode.
\begin{python}
def evaluate(self, av, scalar_obs, action):
	policy, value = self.forward(av, scalar_obs)
	dist = Categorical(policy)
	action_probs = dist.log_prob(action)
	dist_entropy = dist.entropy()
	return action_probs, T.squeeze(value), dist_entropy
\end{python}
\begin{lstlisting}[caption=Darstellung der evaluate Methode, label=code:evaluate_Methode]
\end{lstlisting}

\subsubsection{Memory}
Das Memory oder auch Replay Buffer genannt, befindet sich in der memoryPPO Datei und
besteht aus einer Reihe von Tensoren, welche die von Spiel und Agenten generierten Daten mittels einer zusätzlichen Dimension speichert (siehe \ref{sec:Implementierung_Memory}). Die Rewards und Terminals (is\_terminal) werden jedoch in Listen eingepflegt, da dies das spätere Diskontieren (Abzinsen) erleichtert.\\
Mit der get\_data Methode werden die gesamten Erfahrungen der letzten Spielepisode zurückgegeben. Sobald diese zum Lernen herangezogen worden sind, werden die gelöscht bzw. überschreiben.

\subsubsection{PPO-Agent} \label{sec:Implementierung_PPO_Agent}
Die Agent Klasse wird in der ppo Datei definiert.
Im PPO wird der Memory und zwei ActorCritic Networks erstellt. Diese stellen die alte und neue Policy dar. Während mit der alten Policy immer die Trainingsdaten generiert werden, wird mit der neuen Policy trainieren. Nach dem Lernen wird die alte Policy mit der neuen aktualisieren.\\
\\\textbf{Learn Methode} \label{sec:Implementierung_learn_PPO}\\
Als erstes wird überprüft, ob das Memory mehr als 64 Erfahrungen besitzt. Sollte dies nicht der Fall sein, so wird die Methode terminiert. Die bereits gespeicherten Erfahrungen bleiben erhalten.
Ansonsten werden die Daten aus dem Memory mit der get\_data Methode erhalten.
Danach werden die Rewards mit der generate\_rewards Methode diskontiert (abgezinst), (siehe Code Listing \ref{code:Diskontierung_Rewards} und Abschnitt \ref{sec:Konzept_Lernprozess_PPO}). Diese implementiert die Funktionalität, welche im Abschnitt \ref{sec:Return} dargestellt wurde.
\begin{python}
def generate_reward(self, rewards_in, terminals_in):
	rewards = []
	discounted_reward = 0
	for reward, is_terminal in zip(reversed(rewards_in), reversed(terminals_in)):
		if is_terminal:
			discounted_reward = 0
		discounted_reward = reward + self.GAMMA * discounted_reward
		rewards.insert(0, discounted_reward)
	return rewards
\end{python}
\begin{lstlisting}[caption=Diskontierung der Rewards, label=code:Diskontierung_Rewards]
\end{lstlisting}
Diese diskontierten Rewards (Return) werden im Anschluss noch normalisiert, um ein stetigeres Lernen zu ermöglichen.
\begin{python}
rewards = self.generate_reward(reward_list, dones_list)
rewards = T.tensor(rewards, dtype=T.float64, device=self.DEVICE)
rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)
\end{python}
\begin{lstlisting}[caption=Normalisierung der diskontierten Rewards, label=code:Normalisierung_Diskontierung_Rewards]
\end{lstlisting}
Im Anschluss wird die folgende Prozedur K\_Epochs mal wiederholt.\\
Die Erfahrungen einer bzw. mehrerer Spielepisode/n werden zufallsbasiert durchmischt, um ein stabileres Lernen zu ermöglichen. 
Danach wird die evaluate Methode mit den Erfahrungen aufgerufen. Diese bestimmt die neuen logarithmierten Wahrscheinlichkeiten (log\_probs) aller Aktionen. Mit diesen und den gespeicherten alten (old\_log\_probs) werden daraufhin die ratios (siehe \ref{sec:Probability_Ratio}) gebildet.\\
Zuzüglich wird mithilfe der Values, welche die evaluate Methode zurückgibt, die Advantages aller Erfahrungen erstellt (siehe \ref{sec:Advantages}).
\begin{python}		
probs, state_values, dist_entropy = self.POLICY.evaluate(old_av_b, old_scalar_b, old_action_b)	
ratios = T.exp(probs - probs_old_b)	
advantages = rewards_b - state_values.detach()
\end{python}
\begin{lstlisting}[caption=Bestimmung der Ratios und Advantages, label=code:Bestimmung_Ratio_Advantages]
\end{lstlisting}
Mit diesen Werten ist es nun möglich die Surrogate Losses zu bestimmten (siehe \ref{sec:Surrogate_Objectives}) und anschließend den Actor-Loss bzw. Clip-PPO-Loss (siehe \ref{eq:clip_loss_ppo}).
\begin{python}
	surr1 = ratios * advantages
	surr2 = T.clamp(ratios, 1 - self.EPS_CLIP, 1 + self.EPS_CLIP) * advantages
	loss_actor = -(T.min(surr1, surr2) + dist_entropy * self.ENT_COEFFICIENT).mean()
\end{python}
\begin{lstlisting}[caption=Bestimmung der Surrogate Losses, label=code:Bestimmung_Surrogate_Losses]
\end{lstlisting}
In diesem befindet sich, zur einfacheren Handhabung, noch der Entropy-Loss mit integriert (siehe \ref{sec:PPO_Training_Objective_Function}).
Zur Bestimmung des gesamt Loss fehlt noch der Critic-Loss, welcher mit dem MSE (Mean Squared Error) bestimmt wird. Dabei wird der quadrierte Fehler zwischen den Returns und den Values des Critics ermittelt und im Anschluss gemittelt.
Danach können Actor- und Critic-Loss zusammenaddiert und mit diesem entstandenen PPO-Loss dann das Actor\_Critic NN aktualisiert werden.
\begin{python}
loss_critic = self.CRITIC_COEFFICIENT * self.LOSS(rewards_b, state_values)
loss = loss_actor + loss_critic 
self.POLICY.OPTIMIZER.zero_grad()
loss.backward()
self.POLICY.OPTIMIZER.step()
\end{python}
\begin{lstlisting}[caption=Bestimmung des PPO-Losses \& Update der Netze, label=code:Bestimmung_PPO_Losses_update_NN]
\end{lstlisting}
Zu Schluss wird, nachdem die Prozedur K\_Epochs-mal durchgeführt wurde, die alte Policy mit der neuen aktualisiert. Zuzüglich wird das Memory geleert bzw. mit den nächsten Erfahrungen überschrieben.

\section{Train Methoden} \label{sec:Implementierung_train_Methode}
Zu Beginn werden Agent und Env zusammen mit den Datenhaltungslisten (siehe \ref{sec:Konzept_Datenerhebung}) erstellt. Ein Scheduler wird, für die Optimierung B (siehe \ref{sec:Konzept_Optimierung02}), ebenfalls erzeugt.
Danach wird die Lernprozedur durchgeführt.\\
Zu Beginn dieser, wird eine Obs bestehend aus AV (around\_view) und SO (scalar\_obs) generiert. Diese Obs wird dem Agenten übergeben, welcher eine Aktion und die weiteren benötigten Daten für den jeweiligen Algorithmus zurückgibt, beim PPO z.B. die logarithmierte Wahrscheinlichkeit für die ausgewählte Aktion (log\_probability).
Stellvertretend für alle Agenten wird hier die Trainingsmethode des PPO für Beispiele herangezogen. Analog wird beim DQN verfahren.
Danach wird diese bestimmte Aktion im Env ausgeführt. Dabei wird mit dem OPTIMIZATION Hyperparameter (siehe \ref{sec:Anleitung_PPO_Train_Startargumente}) gesteuert, welche Reward Funktion verwendet wird 
(siehe \ref{sec:Konzept_Optimierung01}).
\begin{python}
for i in range(1, N_ITERATIONS + 1):
	av, scalar_obs = game.reset(get_random_game_size() if RAND_GAME_SIZE else None)
	while not game.has_ended:
		av, scalar_obs, action, log_probability = agent.OLD_POLICY.act(av, scalar_obs)	
		av_, scalac_obs_, reward, is_terminal, won = game.step(action, OPTIMIZATION)
		agent.MEM.store(av, scalar_obs, action, log_probability, reward, is_terminal)
		
		av = av_
		scalar_obs = scalac_obs_
		
	agent.learn()
		
	wins.append(won)
	apples.append(game.apple_count)
	steps_list.append(game.game.step_counter)
\end{python}
\begin{lstlisting}[caption=Spielablauf \& Datenspeicherung, label=code:Bestimmung_Spielablauf_Datenspeicherung]
\end{lstlisting}
Danach wird das Memory mit den entstandenen Erfahrungen aktualisiert.
Sollte die Spielepisode terminieren, so wird die learn Methode des Agent aufgerufen und die generierten Episodendaten werden in die Datenhaltungslisten eingefügt (siehe \ref{sec:Konzept_Datenerhebung}). Beim DQN wird die learn Methode nach jedem fünften Schritt aufgerufen, um eine die Trainingsdatenmenge zu erhöhen und damit das Lernen zu stabilisieren. Ansonsten ist das Prozedere analog zum PPO.\\
Damit der Lernprozess, wie im Konzept dargestellt, bei einer Siegrate von 60\% stoppt, wird die Siegrate der letzten 100 Spiele ermittelt und sollte diese größer als 60\% sein, so wird der Lernprozess terminiert und die Daten in den Datenhaltungslisten werden gespeichert.
\begin{python}
if sum(wins[-100:]) / 100 > 0.6:
	save("PPO", AGENT_NUMBER, STATISTIC_RUN_NUMBER, "train",RUN_TYPE, RAND_GAME_SIZE, agent, dtime, steps_list, apples, scores, wins)
	return
\end{python}
\begin{lstlisting}[caption=Abbruchbedingung, label=code:Abbruchbedingung]
\end{lstlisting}
Wird die Optimierung B (siehe \ref{sec:Konzept_Optimierung02}) verwendet, so wird alle 100 Spielepisoden die Steigung der Performance der letzten 100 Trainingsspiele bzw. Epochs ermittelt. Sollte diese nicht größer als null sein, so wird die Lernrate mit 0.95 multipliziert und damit gesenkt. Dies soll jedoch erst in der Endphase des Lernen umgesetzt werden, um die Lernrate nicht zu Beginn zu stark zu senken. Daher wird diese erst bei Überschreiten von 15.000 Epochs (Trainingsspielen) angepasst.
\begin{python}
if i > 15000 and i % 100 == 0 and SCHEDULED_LR:
	m, b, _, _, _ = linregress(list(range(100)), apples[-100:])
	if m <= 0:
		scheduler.step()
\end{python}
\begin{lstlisting}[caption=Anwendung von Optimierung B - Scheduler, label=code:Optimierung_B]
\end{lstlisting}
Sind alle 30.000 Epochs abgeschlossen, wird das NN und die Trainingsdaten gespeichert und die Methode terminiert.\\
Sollte man sich dazu entscheiden den Trainingsprozess vorzeitig abbrechen zu wollten, so wird man vom System gefragt, ob die Daten und das NN gespeichert werden sollen. Mit der Betätigung von der Taste "`y"' werden NN und Daten gespeichert mit "`n"' werden diese nicht gespeichert. Danach terminiert die Methode.

\subsection{Test Methoden}
Die Test Methoden sind bis auf wenige Ausnahmen mit den Train Methoden übereinstimmend. Es werden in diesem Abschnitt daher nur die Unterschiede aufgezeigt. Einer dieser besteht in dem Hyperparameter MODEL\_PATH, welche den Speicherort repräsentiert, unter dem sich das NN befindet. Mit diesem wird dann der Test durchgeführt. Alle Element des Lernens sind aus der test Methode entfernt, zu diesen gehören der Scheduler, das Aufrufen der learn Methode und das Speichern des NNs.\\
Hinzukommt die Prozedur, um die Spielfeldgröße zu ändern, für die Bestimmung der Robustheit (siehe \ref{sec:Konzept_Datenverarbeitung} und \ref{sec:Anforderungen_an_die_Evaluation}).\\
Des Weiteren wird für die Aktionsbestimmung nun die act\_test Methode verwendet und es kommt die Funktionalität der graphischen Umsetzung hinzu, indem die render Methode des Env aufgerufen wird, sofern die GUI in den Hyperparametern nicht ausgeschaltet worden ist.
Ansonsten treten keine unterschied zwischen den Train und der Test Methoden auf.

\section{Speicherung}
Sämtliche Daten und Networks werden in den baseline-run-n Unterordnern gespeichert. Man findet daher in den Hyperparametern die STATISTIC\_RUN\_NUMBER, welche für die Benennung des Unterordners zuständig ist. Sollte die save Methode, welche für das Speichern der Daten und des NNs zuständig ist, mit einer STATISTIC\_RUN\_NUMBER aufgerufen werde, für die noch kein Ordner erstellt wurde, so wird dies automatisch geschehen. Der Parameter RUN\_TYPE ist für die Benennung des Unterordners zuständig. Es gibt die Möglichkeit zwischen "baseline" und "optimized". Der USE\_CASE gibt an, ob es sich um einen Test- oder Trainingslauf handelt. Die AGENT\_NUMBER bestimmt unter welchen Namen das NN und die Daten abgespeichert werden soll. Wird eine eins übergeben so werden die Dateien "PPO-01-train.csv" und "PPO-01-train.model" gespeichert. Sollte ein Training bzw. Test mit der zufallsverteilten Spielfeldgröße durchgeführt werden, so wird dies, in der Datei, mit dem Namenszusatz "rgs" (random game size) signalisiert. Die Trainingsdateien würden dann "PPO-01-rgs-train.csv" und "PPO-01-rgs-train.model" heißen. Mit dem Parameter OPTIMIZATION lässt sich die Optimierung auswählen, welche im Namen der Dateien deutlich gemacht wird. Eine NN, welches mit einer Optimierung trainiert wurde, würde dann unter dem Namen "PPO-01-opt-a-train.model" gespeichert werden usw.

\section{Statistik} \label{sec:Implementierung_Statistiken}
Die Statistiken werden mit der generate\_statistic Funktion erstellt, welche sich in der statisticTool Datei befindet. Diese ist wiederum im statistic Ordner definiert (siehe \ref{fig:Package_Struktur}). Ihr wird die STATISTIC\_RUN\_NUMBER, der RUN\_TYPE die OPTIMIZATION und eine Agenten-Liste übergeben. Die STATISTIC\_RUN\_NUMBER gibt an, welcher Run ausgewertet werden soll. Der RUN\_TYPE differenziert zwischen der Auswertung von baseline und optimized Daten. Der USE\_CASE unterscheidet die Auswertung von Test- und Trainingsdaten. Mit der Agenten-Liste können gezielt Statistiken zu einzelnen Agenten angefertigt werden. Wird eine leere List übergeben, so wird jeder Agent, welcher über Daten im Run-Ordner verfügt, untersucht.\\
Zuerst wird der, durch die übergebenen Parameter aufgespannte, Path bestimmt. Danach werden alle CSV-Datein, welche die Test- und Trainingsdaten enthalten, eingelesen und deren Daten gespeichert. Sollte die Agenten List nicht leer sein, so werden die Daten, der Agenten, die sich nicht in der Liste befinden, gelöscht. Die CSV-Datei bleibt erhalten.
Danach werden die Daten entsprechend der Darstellungen im Abschnitt \ref{sec:Konzept_Datenerhebung_Verarbeitung} bereitgestellt. Dazu werden Dataframes des Framework Pandas (siehe \url{https://pandas.pydata.org/}) genutzt. In diesen befinden sich die Anzahl der gefressenen Äpfel (apples), der gegangenen Schritte (steps) und der Siege (wins). Diese Werte werden mithilfe des Pandas Frameworks für die Statistik aufbereitet. In diesem Fall wird der Durchschnitt über die letzten 100 Werte pro Epoch (Spiel) gebildet. Dieser Vorgang wird für alle Agenten durchgeführt.
\begin{python}
performance_lists = [value["apples"].rolling(100).mean().fillna(0) for value in agent_dict.values()]
efficiency_lists = [(value["apples"] / value["steps"]).rolling(100).mean().fillna(0) for value in agent_dict.values()]
robustness_lists = [(value["apples"]).rolling(100).mean().fillna(0) for value in agent_dict_rgs.values()]
win_rate = [value["wins"].rolling(100).mean().fillna(0) for value in agent_dict.values()]
\end{python}
\begin{lstlisting}[caption=Erstellung der Daten entsprechende der Evaluationskriterien, label=code:Datenerstellung]
\end{lstlisting}
Danach werden die Statistiken mit der make\_statistics Unterfunktion erstellt.
Diese generiert, mithilfe des Matplotlib Frameworks, die Graphiken. Dafür wird die Größe und Achsenbeschriftung definiert, sowie die Farben der Kurven und das Vorhandensein eine Gitters in der Grafik.\\
Daraufhin wird iterativ jeder übergebenen Datensätze geplotted. Zum Schluss wird noch eine Legende hinzugefügt, damit die Agenten besser unterschieden werden können. Die erzeugte Statistik wird als PNG Datei unter dem übergebenen Path gespeichert.