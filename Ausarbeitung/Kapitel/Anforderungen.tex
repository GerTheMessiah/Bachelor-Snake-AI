\chapter{Anforderungen} \label{chap:Anforderungen}
In dem letzten Kapitel \ref{sec:Grundlagen}, wurden die Grundlagen für den weiteren Vergleich, der Deep Reinforcement Learning Algorithmen, gelegt. Es wurde daher entschieden, dass der Vergleich exemplarisch an zwei RL Agenten unterschiedlicher Gattungen durchgeführt werden soll. Dies erlaubt eine Abstraktion des Vorgehens auch für andere Agenten.\\
Problematisch an einem solchen Vergleich ist die Objektivität. Wie kann also sicher gestellt werden, dass die Ergebnisse vergleichbar sind?\\
Die Antwort auf diese Frage sind feste Anforderungen bzw. Rahmenbedingungen, die bei den einzelnen, am Vergleich beteiligten, Teilen gelten müssen. Dabei gibt es verschiedene Anforderungen an unterschiedliche Systembereiche. In dieser Ausarbeitung existieren es vier Anforderungsbereiche, welche Anforderungen an die folgenden Bereiche stellen:
\begin{itemize}
	\item Spielimplementierung
	\item Agenten
	\item statistische Datenerhebung
	\item Evaluation
\end{itemize}

\section{Anforderungen an das Environment}
Die Anforderungen, welche an eine Reinforcement Learning Environment gestellt werden, sind vielseitiger Natur. Sie stammen unter anderem aus der Softwaretechnik, aus dem RL und aus dem gesundem Menschenverstand.

\subsection{Kommunikation}
Damit keine vollkommene Abtrennung des Environments jegliches Interagieren unmöglich macht, sollen nach dem Vorbild aus \ref{sec:Environment} drei Kommunikationskanäle implementiert werden. Das Env. soll in der Lage sein, Actions zu empfangen und Observation und Rewards zu senden.

\subsection{Funktionalität} \label{sec:funktionalität_Env}
Die Funktionalität des Environments ist in drei Unterfunktionalitäten aufzuteilen.
\begin{longtable}[h]{|p{4cm}|p{\linewidth - 5cm}|}
	\caption{Funktionalität des Environment}
	\label{tab:Funktionalität_Env} 
	\endfirsthead
	\endhead
	\hline
	Funktionalität & Erklärung \\
	\hline
	Step & Step steht im Sachzusammenhang für das Gehen eines Schrittes im Env. Es beschreibt daher die Umsetzung der von Agenten bestimmten Actions. \\
	\hline
	Reset & Um den Fortschritt einer Spielepisode wieder zu löschen, ist das Vorhandensein einer Reset-Funktionalität erforderlich. \\
	\hline
	Render & Render beschreibt die Darstellung des Spielgeschehens auf visueller Basis. Näheres dazu unter \ref{sec:visualisierung_Env} \\
	\hline
\end{longtable}

\subsection{Visualisierung} \label{sec:visualisierung_Env}
Um den Spielfortschritt besser evaluieren zu können und aus Gründen der Ästhetik soll eine graphische Oberfläche implementiert werden. Diese soll das Spielgeschehen graphisch wiedergeben.

\subsection{Test}
Um zu garantieren, dass das Env. voll funktionsfähig ist sollen dieses mit Tests überprüft werden. Dies dient der Sicherheit, da Fehler im Env. zu Fehlern im Entscheidungsfindungsprozess führen können, wie das Hide and Seek Environment von OpenAI zeigt \cite{DBLP:journals/corr/abs-1909-07528}.

\section{Anforderungen an die Agenten}
Neben dem Environment gelten für die Agenten ebenfalls spezielle Anforderungen, welche zum einem objektiven Vergleich führen sollen. Auch sollen Anforderungen an die Implementierung gestellt werden, um Standards einzuführen.

\subsection{Funktionalität}
Reinforcement Learning Agenten können ein großes Repertoire an Funktionen besitzen. Zu den wichtigsten Funktionalitäten gehören das Spielen, daher das Bestimmen von Actions und das Lernen, daher die Anpassung des NN, um bessere Actions zu wählen, was dann zu optimalen Resultaten führt.

\subsection{Parametrisierung}
Um den Vergleich der verschiedenen Systeme mit dem größtmöglichen Maß an Objektivität zu vollziehen, ist es wichtig, dass die Einzigartigkeit eines Agenten nicht alleine durch die Art (PPO oder DQN) definiert wird, sondern auch durch die dem Agenten zugehörigen Parameter. Siehe \ref{chap:Agenten}

\subsection{Test}
Zur Prüfung der Funktionalität der einzelnen Agenten, sollen Tests implementiert werden, welche im besonderen das Lernen und die Aktionsbestimmung abdecken sollen. Dies dient zum Ausschluss von Fehlern.

\section{Anforderungen an die Datenerhebung} \label{sec:Anforderungen_an_die Datenerhebung}
Auch die statistische Datenerhebung soll durch Anforderungen konkretisiert und definiert werden. Um die Objektivität sicherzustellen, werden dafür Anforderungen bezüglich der Durchführung, der Datenspeicherung und den Spezifikationen der Datenerhebung erhoben.

\subsection{Mehrfache Datenerhebung}
Um die Validität der zu erzeugenden Trainingsdaten zu garantieren, soll für jeden Agenten die Datenerhebung mehrmalig durchgeführt werden. Dies trägt nicht nur zur Objektivität bei, sondern sichert das Ergebnis ebenfalls auch vor Schwankungen ab, welche beim Reinforcement Learning natürlicherweise auftreten, bedingt durch die zufälligen Spielverläufe des Environment.

\subsection{Datenspeicherung}
Damit aus den erzeugten Daten statistische Schlüsse gezogen werden können ist es wichtig, dass die erzeugten Spieldaten gespeichert werden. Da jedoch die Menge an Daten die schnell riesige Dimensionen annimmt, sollen stellvertretend nur die Daten ganzer Spiele gespeichert werden. Dies ist ein Kompromiss zwischen Vollständigkeit und effizientem Speicherplatzmanagement.\\
Folgenden Daten sollen für die Auswertung erhoben und gespeichert werden:
\begin{longtable}[h]{|p{4cm}|p{\linewidth - 5cm}|}
	\caption{Zu erhebende Daten}
	\label{tab:Datenerhebung} 
	\endfirsthead
	\endhead
	\hline
	Daten & Erklärung \\
	\hline
	time & Die Uhrzeit. Dies dient später dem Geschwindigkeitsvergleich. \\
	\hline
	steps & Die in einem Spiel durchgeführten Züge. Diese geben in der Evaluation später Aufschluss über den Lernerfolge und weisen auf Lernfehler der Agenten, wie beispielsweise das Laufen im Kreis, hin. \\
	\hline
	apples & Die Anzahl der gefressenen Äpfel in einem Spiel. Maßgeblicher Evaluationsfaktor zur Einschätzung des Lernerfolges. \\
	\hline
	scores & Die gesammelten und aufsummierten Rewards, welche der Agent in einem Spiel gesammelt hat. Auch dieser Datenwert gibt Aufschluss über den Lernerfolg. Gleichzeitig lässt sich den Scores auch noch die Effizienz der Lösung und damit des Lernens entnehmen. \\
	\hline
	wins & Hat der Agent das Spiel gewonnen. Dieser Wert stellt die Endkontrolle des Agenten dar. \\
	\hline
	epsilon (nur beim Deep Q-Learning Algorithmus) & Gibt die Wahrscheinlichkeit für das Ziehen einer zufälligen Aktion wieder.  \\
	\hline
	Learning Rate (lr) & einmalig zu speichernder skalarer Wert, welcher ein Maß für die Anpassung der Gewichte des bzw. der NN wiedergibt. \\
	\hline
	board\_size &  einmalige zu speichernder vektorieller Wert. Dieser gibt die Feldgröße des Environments an. Dieser ist wichtig für die Evaluation der Robustheit der Agenten. \\
	\hline
	andere Hyperparameter der Agenten & Die Agenten werden maßgeblich durch die ihnen zugrundeliegenden Paramente gesteuert. Um einen Vergleich verschiedenen Agenten durchführen zu können, sind daher die Hyperparameter von größter Wichtigkeit. \\
	\hline
\end{longtable}

\subsection{Variation der Datenerhebungsparameter}
Um die Möglichkeit auszuschließen, dass die momentan ausgewählten Parameter, wie z.B. Feldgröße, Lernrate, Reward-Funktion usw., für einzelne Agenten vorteilhafte Bedingungen hervorbringen können, sollen die einzelnen Agenten unter verschiedenen Modifikationen getestet werden. Dazu soll die Datenerhebung unter der Anwendung einer Modifikation durchgeführt werden. Zu den Modifikationen zählen:

\subsubsection{Variation der Reward-Funktion}
Durch das Verändern der Reward-Funktion soll überprüft werden, ob die Agenten unterschiedlich gut auf eine andere Reward-Funktion reagieren. Sollte die Reward-Funktion eins $R_{1}()$ für den Agenten eins $A_{1}$ überdurchschnittlich bessere Resultate liefern, so wird dies erkannt und in der Evaluation berücksichtigt.

\section{Anforderungen an die Evaluation} \label{sec:Anforderungen_an_die_Evaluation}
Bei der Evaluation soll der optimale Agent bestimmt werden. Dabei stellt sich jedoch die Frage, was optimal im Sachzusammenhang bedeutet. 
Daher sollen die Agenten unter verschiedenen Gesichtspunkten evaluiert werden, um ein möglichst großes Spektrum an Kriterien abzudecken.
Einige dieser Kriterien stammen aus der verwendeten Literatur \ref{chap:Verwandte_Arbeiten} wie z.B. die Performance und Spielzeit die anderen ergeben sich aus den zu speichernden Daten.
Die einzelnen Kriterien lauten:
\begin{longtable}[h]{|p{4cm}|p{\linewidth - 5cm}|}
	\caption{Evaluationskriterien}
	\label{tab:Kriterien} 
	\endfirsthead
	\endhead
	\hline
	Kriterium & Erläuterung \\
	\hline
	Performance & Welcher Agent erreicht, nach einer festen Anzahl an absolvierten Spielen, das beste Ergebnis? Im Sachzusammenhang mit dem Spiel Snake bedeutet dies: Welcher Agent frisst die meisten Äpfel, nach dem er über eine feste Anzahl an Spielen trainiert wurde? \\
	\hline
	Effizienz & Welcher Agent löst das Spiel mit der größten Effizienz. Bezogen auf das Spiel Snake bedeutet dies, welches Agent ist in der Lage die Äpfel mit möglichst wenig Schritten zu erreichen und zu fressen? Hierbei werden beispielsweise Probleme, wie das im Kreis laufen sichtbar. \\
	\hline
	Robustheit & Welcher Agent ist in der Lage in einer modifizierten Umgebung das Spielziel am besten zu erreichen? In Bezug auf Snake bedeutet dies: Welcher Agent ist in der Lage auf einem größeren oder kleineren Spielfeld die meisten Äpfel zu fressen? \\
	\hline
	Trainingszeit & Welcher Agent schafft es ein festes Ziel in der geringstmöglichen Zeit zu erreichen oder welcher Agent ist als erstes in der Lage durchschnittlich 10 Äpfel zu fressen. \\
	\hline
	Spielzeit & Welcher Agent schafft es am längsten ein durchzuhalten bevor ein terminaler Zustand erreicht wird? Auf Snake bezogen: Welcher Agent schafft es am längsten am Leben zu bleiben. \\
	\hline
	Multiprocessing fähig (boolscher Wert) & Welche Agenten sind multiprocessing fähig und können damit schneller und ausgiebiger trainiert werden. \\
	\hline
\end{longtable}