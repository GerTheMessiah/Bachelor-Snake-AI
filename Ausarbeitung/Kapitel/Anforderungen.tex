\chapter{Anforderungen} \label{chap:Anforderungen}
In Kapitel \ref{sec:Grundlagen}, wurden die Grundlagen für den weiteren Vergleich der Reinforcement Learning Agenten gelegt, welche auf zwei unterschiedlichen Algorithmus-Arten (DQN und PPO) basieren.\\
Um diesem Vergleich durchzuführen soll ein System erstellt werden, welches diese geplanten Vergleich durchführen, festhalten und auswerten kann. Dieses soll aus einem Environment, aus mehreren Agenten beider Algorithmus-Arten, sowie aus statistischen Analysekomponenten zur Leistungsbestimmung, bestehen. Zuzüglich sollen weitere Anforderungen an die Evaluation gestellt werden, um die Vergleichbarkeit sicherstellen.

\section{Anforderungen an das Environment} \label{sec:Anforderungen_Env}
In diesem Abschnitt werden die Anforderungen an das Env dargestellt. Neben der Hauptanforderung, dass das Env das Spiel Snake implementieren soll, ergeben sich weitere zusätzliche Anforderungen.

\subsection{Standardisierte Schnittstelle} \label{sec:Anforderungen_Schnittstelle}
Das Env soll eine standardisierte Schnittstelle besitzen, sodass nach \ref{sec:Environment} drei Kommunikationskanäle implementiert werden. Das Env soll in der Lage sein, Actions zu empfangen. Des Weiteren soll es Observation und Rewards an den Agenten übergeben können.

\subsection{Funktionalitäten} \label{sec:funktionalität_Env}
Das Env soll die folgenden Funktionalitäten implementieren.

\subsubsection{Aktionsausführung}
Das Env muss eine Funktionalität beinhalten, welche die vom Agenten bestimmt Aktion ausführen kann. Diese Aktionsausführung muss sich nach den Regeln des Spiels Snake richten \ref{sec:Snake}.

\subsubsection{Reset} \label{sec:Anforderung_Reset}
Das Env muss eine Reset Methode implementieren, um einen erbrachten Spielfortschritt zu löschen. Dies ist für ein stetiges Lernen unentbehrlich.

\subsubsection{Render} \label{sec:visualisierung_Env}
Das Env muss eine Render Funktionalität implementieren, um eine Visualisierung des Spiels Snake zu ermöglichen. Diese dient der besseren Evaluation.

\section{Anforderungen an die Agenten}
In diesem Abschnitt werden die Anforderungen an die Agenten, welche auf den beide Algorithmus-Arten (PPO, DQN) basieren, dargestellt.

\subsection{Funktionalitäten} \label{sec:Agent_Funktionalitäten}
Die Agenten müssen folgende Funktionalitäten implementieren.

\subsubsection{Aktionsbestimmung}
Die Agenten müssen in der Lage sein aus der Obs Aktionen zu bestimmen, welche dem Env übergeben werden.

\subsubsection{Lernen}
Die Agenten müssen fähig sein, auf Grundlage vergangener Spielepisoden zu lernen und damit ihre Spielergebnisse zu verbessern.

\subsection{Parametrisierung} \label{sec:Parametrisierung}
Das System muss die Möglichkeit besitzen, mehrere Agenten der gleichen Algorithmus-Art zu erstellen, welche sich jedoch durch verwendeten Hyperparameter und Algorithmus-Art unterscheiden. Diese Definition von Agenten ist in der Evaluation zu berücksichtigen.

\subsection{Diversität der RL Algorithmen}
Um nicht nur Agenten einer Algorithmus-Art untereinander zu vergleichen, sondern auch den Vergleich zu anderen Algorithmus-Arten zu erbringen, sollen ein DQN- und PPO-Algorithmus miteinander verglichen werden. Diese bieten sich wie in \ref{sec:PPO} und \ref{sec:Q-Learning} für den Vergleich an.

\section{Anforderungen an die Datenerhebung} \label{sec:Anforderungen_an_die_Datenerhebung}
In diesem Teil sollen Anforderungen an die statistische Datenerhebung und an die damit verbundenen Analysekomponenten gestellt werden.

\subsection{Mehrfache Datenerhebung}
Die Datenermittlung der muss für jeden einzelnen Agenten mehrfach durchgeführt werden, um die Validität der Messung zu gewährleisten.

\subsection{Datenspeicherung}
Damit aus den erzeugten Daten statistische Schlüsse gezogen werden können ist es wichtig, dass die erzeugten Spieldaten gespeichert werden. Da jedoch die Menge an Daten schnell riesige Dimensionen annimmt, sollen stellvertretend nur die Daten ganzer Spiele gespeichert werden. Dies diese Strategie stellt einen Kompromiss zwischen Vollständigkeit und effizientem Speicherplatzmanagement dar.\\
Das System muss daher folgende Daten speichern:
\begin{longtable}[h]{|p{4cm}|p{\linewidth - 5cm}|}
	\caption{Zu erhebende Daten}
	\label{tab:Datenerhebung} 
	\endfirsthead
	\endhead
	\hline
	Daten & Erklärung \\
	\hline
	time & Die Uhrzeit. Dies dient später dem Geschwindigkeitsvergleich. \\
	\hline
	steps & Die in einem Spiel durchgeführten Züge. Diese geben in der Evaluation später Aufschluss über den Lernerfolge und weisen auf Lernfehler der Agenten, wie beispielsweise das Laufen im Kreis, hin. \\
	\hline
	apples & Die Anzahl der gefressenen Äpfel in einem Spiel. Maßgeblicher Evaluationsfaktor zur Einschätzung des Lernerfolges. \\
	\hline
	wins & Hat der Agent das Spiel gewonnen. Dieser Wert stellt die Endkontrolle des Agenten dar. \\
	\hline
	epsilon (nur beim Deep Q-Learning Algorithmus) & Gibt die Wahrscheinlichkeit für das Ziehen einer zufälligen Aktion wieder. \\
	\hline
	board\_size &  einmalige zu speichernder vektorieller Wert. Dieser gibt die Feldgröße des Environments an. Dieser ist wichtig für die Evaluation der Robustheit der Agenten. \\
	\hline
\end{longtable}

\section{Anforderungen an die Statistiken}
Das System muss in der Lage sein, Statistiken zu generieren und zu speichern, welche für die Evaluation verwendet werden. Diese Statistiken sollen auf den Werten der Evaluationskriterien \ref{tab:Kriterien} basieren.

\section{Anforderungen an die Evaluation} \label{sec:Anforderungen_an_die_Evaluation}
Bei der Evaluation soll der möglichst optimale Agent entsprechend verschiedener Gesichtspunkten ermittelt werden.
Einige dieser Kriterien stammen aus der verwendeten Literatur \cite{Autonomous_Agents_in_Snake_Game_via_DRL} und \cite{UAV} wie z.B. die Performance und Spielzeit. Die anderen ergeben sich aus den zu speichernden Daten.
Die einzelnen Kriterien lauten:
\begin{longtable}[h]{|p{4cm}|p{\linewidth - 5cm}|}
	\caption{Evaluationskriterien}
	\label{tab:Kriterien} 
	\endfirsthead
	\endhead
	\hline
	Kriterium & Erläuterung \\
	\hline
	Performance & Welcher Agent erreicht, nach einer festen Anzahl an absolvierten Spielen, das beste Ergebnis? Im Sachzusammenhang mit dem Spiel Snake bedeutet dies: Welcher Agent frisst die meisten Äpfel, nach dem er über eine feste Anzahl an Spielen trainiert wurde? \\
	\hline
	Effizienz & Welcher Agent löst das Spiel mit der größten Effizienz. Bezogen auf das Spiel Snake bedeutet dies, welches Agent ist in der Lage die Äpfel mit möglichst wenig Schritten zu fressen? \\
	\hline
	Robustheit & Welcher Agent ist in der Lage in einer modifizierten Umgebung das Spielziel am besten zu erreichen? In Bezug auf Snake bedeutet dies: Welcher Agent ist in der Lage auf einem größeren oder kleineren Spielfeld die meisten Äpfel zu fressen? \\
	\hline
	Trainingszeit & Welcher Agent schafft es ein festes Ziel in der geringstmöglichen Zeit zu erreichen bzw. welcher Agent ist als erstes in der Lage durchschnittlich $x$ Äpfel zu fressen. \\
	\hline
	Spielzeit & Welcher Agent schafft es im Durchschnitt am längsten einen terminalen Zustand zu vermeiden? Auf Snake bezogen: Welcher Agent schafft es am längsten nicht zu sterben. \\
	\hline
\end{longtable}