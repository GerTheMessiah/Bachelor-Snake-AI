\babel@toc {ngerman}{}
\contentsline {chapter}{\nonumberline Abbildungsverzeichnis}{v}{chapter*.2}%
\contentsline {chapter}{\nonumberline Tabellenverzeichnis}{vi}{chapter*.3}%
\contentsline {chapter}{\nonumberline Abk체rzungsverzeichnis}{vii}{chapter*.4}%
\contentsline {chapter}{\numberline {1}Einleitung}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Zielsetzung}{2}{section.1.2}%
\contentsline {chapter}{\numberline {2}Grundlagen}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Game of Snake}{3}{section.2.1}%
\contentsline {section}{\numberline {2.2}Reinforcement Learning}{4}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Vokabular}{5}{subsection.2.2.1}%
\contentsline {subsubsection}{\nonumberline Agent}{5}{subsubsection*.6}%
\contentsline {subsubsection}{\nonumberline Environment}{5}{subsubsection*.8}%
\contentsline {subsubsection}{\nonumberline Action}{5}{subsubsection*.10}%
\contentsline {subsubsection}{\nonumberline Observation}{6}{subsubsection*.12}%
\contentsline {subsubsection}{\nonumberline Reward}{6}{subsubsection*.14}%
\contentsline {subsubsection}{\nonumberline State}{7}{subsubsection*.16}%
\contentsline {subsubsection}{\nonumberline Policy}{7}{subsubsection*.18}%
\contentsline {subsubsection}{\nonumberline Value}{7}{subsubsection*.20}%
\contentsline {subsection}{\numberline {2.2.2}Funktionsweise}{8}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Arten von RL-Verfahren}{8}{subsection.2.2.3}%
\contentsline {subsubsection}{\nonumberline Model-free und Model-based}{9}{subsubsection*.22}%
\contentsline {subsubsection}{\nonumberline Policy-Based und Value-Based Verfahren}{9}{subsubsection*.24}%
\contentsline {subsubsection}{\nonumberline On-Policy und Off-Policy Verfahren}{10}{subsubsection*.26}%
\contentsline {section}{\numberline {2.3}Proximal Policy Optimization}{10}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Actor-Critic Modell}{11}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}PPO Training Objective Function}{11}{subsection.2.3.2}%
\contentsline {subsubsection}{\nonumberline Formelelemente}{12}{subsubsection*.28}%
\contentsline {subsubsection}{\nonumberline Return}{13}{subsubsection*.30}%
\contentsline {subsubsection}{\nonumberline Baseline Estimate}{13}{subsubsection*.32}%
\contentsline {subsubsection}{\nonumberline Advantage}{13}{subsubsection*.34}%
\contentsline {subsubsection}{\nonumberline Probability Ratio}{14}{subsubsection*.36}%
\contentsline {subsubsection}{\nonumberline Surrogate Objectives}{15}{subsubsection*.38}%
\contentsline {subsubsection}{\nonumberline Zusammenfassung der PPO Training Objective Function}{15}{subsubsection*.40}%
\contentsline {subsection}{\numberline {2.3.3}PPO - Algorithmus}{16}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Deep Q-Network}{17}{section.2.4}%
\contentsline {chapter}{\numberline {3}Anforderungen}{20}{chapter.3}%
\contentsline {section}{\numberline {3.1}Anforderungen an das Environment}{20}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Kommunikation}{20}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Funktionalit채t}{21}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Visualisierung}{21}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Test}{21}{subsection.3.1.4}%
\contentsline {section}{\numberline {3.2}Anforderungen an die Agenten}{21}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Funktionalit채t}{21}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Parametrisierung}{22}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Test}{22}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Anforderungen an die Datenerhebung}{22}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Mehrfache Datenerhebung}{22}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Datenspeicherung}{22}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Variation der Datenerhebungsparameter}{23}{subsection.3.3.3}%
\contentsline {subsubsection}{\nonumberline Variation der Reward-Funktion}{23}{subsubsection*.42}%
\contentsline {section}{\numberline {3.4}Anforderungen an die Evaluation}{24}{section.3.4}%
\contentsline {chapter}{\numberline {4}Verwandte Arbeiten}{25}{chapter.4}%
\contentsline {section}{\numberline {4.1}Autonomous Agents in Snake Game via Deep Reinforcement Learning}{25}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Diskussion}{26}{subsection.4.1.1}%
\contentsline {section}{\numberline {4.2}UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene}{27}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Diskussion}{28}{subsection.4.2.1}%
\contentsline {section}{\numberline {4.3}Zusammenfassung}{28}{section.4.3}%
\contentsline {chapter}{\numberline {5}Agenten}{30}{chapter.5}%
\contentsline {section}{\numberline {5.1}Agenten}{30}{section.5.1}%
\contentsline {chapter}{\numberline {6}Vorgehen}{32}{chapter.6}%
\contentsline {section}{\numberline {6.1}Bemerkung zur Netzstruktur}{32}{section.6.1}%
\contentsline {section}{\numberline {6.2}Baseline Datenerhebung, Vergleich und Evaluation}{32}{section.6.2}%
\contentsline {section}{\numberline {6.3}Anwendung der Optimierungen}{33}{section.6.3}%
\contentsline {chapter}{\numberline {7}Implementierung}{34}{chapter.7}%
\contentsline {section}{\numberline {7.1}Snake Environment}{34}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Schnittstelle}{34}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}Spiellogik}{35}{subsection.7.1.2}%
\contentsline {subsection}{\numberline {7.1.3}Reward Function}{37}{subsection.7.1.3}%
\contentsline {subsection}{\numberline {7.1.4}Observation}{38}{subsection.7.1.4}%
\contentsline {subsection}{\numberline {7.1.5}Graphische Oberfl채che}{40}{subsection.7.1.5}%
\contentsline {section}{\numberline {7.2}Agenten}{40}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Netzstruktur}{41}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}DQN}{42}{subsection.7.2.2}%
\contentsline {subsubsection}{\nonumberline Aktionsauswahlprozess}{43}{subsubsection*.44}%
\contentsline {subsubsection}{\nonumberline Trainingsprozess}{43}{subsubsection*.46}%
\contentsline {subsubsection}{\nonumberline Main-Methode}{44}{subsubsection*.48}%
\contentsline {subsection}{\numberline {7.2.3}PPO}{44}{subsection.7.2.3}%
\contentsline {subsubsection}{\nonumberline Aktionsauswahlprozess}{44}{subsubsection*.50}%
\contentsline {subsubsection}{\nonumberline Trainingsprozess}{45}{subsubsection*.52}%
\contentsline {subsubsection}{\nonumberline Main-Methode}{46}{subsubsection*.54}%
\contentsline {chapter}{\numberline {8}Optimierungen}{47}{chapter.8}%
\contentsline {section}{\numberline {8.1}Optimierung 1 - Dual Experience Replay}{47}{section.8.1}%
\contentsline {section}{\numberline {8.2}Optimierung 2 - Joined Reward Function}{48}{section.8.2}%
\contentsline {section}{\numberline {8.3}Optimierung 3 - Odor Reward Function \& Loop Storm Strategy}{49}{section.8.3}%
\contentsline {section}{\numberline {8.4}Optimierung 4 - Dynamische Lernrate}{49}{section.8.4}%
\contentsline {chapter}{\numberline {9}Evaluation}{50}{chapter.9}%
\contentsline {chapter}{Literaturverzeichnis}{51}{section*.55}%
\contentsline {chapter}{\numberline {A}Anhang}{53}{appendix.A}%
\contentsline {section}{\numberline {A.1}Backpropagation und das Gradientenverfahren}{53}{section.A.1}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
