\babel@toc {ngerman}{}\relax 
\contentsline {chapter}{\nonumberline Abbildungsverzeichnis}{v}{chapter*.2}%
\contentsline {chapter}{\nonumberline Tabellenverzeichnis}{vi}{chapter*.3}%
\contentsline {chapter}{\nonumberline Abkürzungsverzeichnis}{vii}{chapter*.4}%
\contentsline {chapter}{\numberline {1}Einleitung}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Zielsetzung}{2}{section.1.2}%
\contentsline {chapter}{\numberline {2}Grundlagen}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Game of Snake}{3}{section.2.1}%
\contentsline {section}{\numberline {2.2}Reinforcement Learning}{4}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Vokabular}{5}{subsection.2.2.1}%
\contentsline {subsubsection}{\nonumberline Agent}{5}{subsubsection*.6}%
\contentsline {subsubsection}{\nonumberline Environment}{5}{subsubsection*.8}%
\contentsline {subsubsection}{\nonumberline Action}{5}{subsubsection*.10}%
\contentsline {subsubsection}{\nonumberline Observation}{6}{subsubsection*.12}%
\contentsline {subsubsection}{\nonumberline Reward}{6}{subsubsection*.14}%
\contentsline {subsubsection}{\nonumberline State}{7}{subsubsection*.16}%
\contentsline {subsubsection}{\nonumberline Policy}{7}{subsubsection*.18}%
\contentsline {subsubsection}{\nonumberline Value}{7}{subsubsection*.20}%
\contentsline {subsection}{\numberline {2.2.2}Funktionsweise}{8}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Arten von RL-Verfahren}{8}{subsection.2.2.3}%
\contentsline {subsubsection}{\nonumberline Model-free und Model-based}{9}{subsubsection*.22}%
\contentsline {subsubsection}{\nonumberline Policy-Based und Value-Based Verfahren}{9}{subsubsection*.24}%
\contentsline {subsubsection}{\nonumberline On-Policy und Off-Policy Verfahren}{10}{subsubsection*.26}%
\contentsline {section}{\numberline {2.3}Proximal Policy Optimization}{10}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Actor-Critic Modell}{11}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}PPO Training Objective Function}{11}{subsection.2.3.2}%
\contentsline {subsubsection}{\nonumberline Formelelemente}{12}{subsubsection*.28}%
\contentsline {subsubsection}{\nonumberline Return}{13}{subsubsection*.30}%
\contentsline {subsubsection}{\nonumberline Baseline Estimate}{13}{subsubsection*.32}%
\contentsline {subsubsection}{\nonumberline Advantage}{13}{subsubsection*.34}%
\contentsline {subsubsection}{\nonumberline Probability Ratio}{14}{subsubsection*.36}%
\contentsline {subsubsection}{\nonumberline Surrogate Objectives}{15}{subsubsection*.38}%
\contentsline {subsubsection}{\nonumberline Zusammenfassung der PPO Training Objective Function}{15}{subsubsection*.40}%
\contentsline {subsection}{\numberline {2.3.3}PPO - Algorithmus}{16}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Deep Q-Network}{17}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}DQN - Algorithmus}{18}{subsection.2.4.1}%
\contentsline {chapter}{\numberline {3}Anforderungen}{20}{chapter.3}%
\contentsline {section}{\numberline {3.1}Anforderungen an das Environment}{20}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Standardisierte Schnittstelle}{20}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Funktionalitäten}{20}{subsection.3.1.2}%
\contentsline {subsubsection}{\nonumberline Aktionsausführung}{21}{subsubsection*.42}%
\contentsline {subsubsection}{\nonumberline Reset}{21}{subsubsection*.44}%
\contentsline {subsubsection}{\nonumberline Render}{21}{subsubsection*.46}%
\contentsline {section}{\numberline {3.2}Anforderungen an die Agenten}{21}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Funktionalitäten}{21}{subsection.3.2.1}%
\contentsline {subsubsection}{\nonumberline Aktionsbestimmung}{21}{subsubsection*.48}%
\contentsline {subsubsection}{\nonumberline Lernen}{21}{subsubsection*.50}%
\contentsline {subsection}{\numberline {3.2.2}Parametrisierung}{21}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Diversität der RL Algorithmen}{22}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Anforderungen an die Datenerhebung}{22}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Mehrfache Datenerhebung}{22}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Datenspeicherung}{22}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Anforderungen an die Statistiken}{23}{section.3.4}%
\contentsline {section}{\numberline {3.5}Anforderungen an die Evaluation}{23}{section.3.5}%
\contentsline {chapter}{\numberline {4}Verwandte Arbeiten}{25}{chapter.4}%
\contentsline {section}{\numberline {4.1}Autonomous Agents in Snake Game via Deep Reinforcement Learning}{25}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Diskussion}{26}{subsection.4.1.1}%
\contentsline {section}{\numberline {4.2}UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene}{28}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Diskussion}{29}{subsection.4.2.1}%
\contentsline {section}{\numberline {4.3}Zusammenfassung}{30}{section.4.3}%
\contentsline {chapter}{\numberline {5}Konzept}{31}{chapter.5}%
\contentsline {section}{\numberline {5.1}Vorgehen}{31}{section.5.1}%
\contentsline {section}{\numberline {5.2}Environment}{33}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Spiellogik}{33}{subsection.5.2.1}%
\contentsline {subsubsection}{\nonumberline Spielablauf}{34}{subsubsection*.52}%
\contentsline {subsubsection}{\nonumberline Reward}{36}{subsubsection*.54}%
\contentsline {subsubsection}{\nonumberline Observation}{36}{subsubsection*.56}%
\contentsline {subsubsection}{\nonumberline GUI}{39}{subsubsection*.58}%
\contentsline {subsection}{\numberline {5.2.2}Schnittstelle}{39}{subsection.5.2.2}%
\contentsline {section}{\numberline {5.3}Agenten}{41}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Netzstruktur}{41}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}DQN}{43}{subsection.5.3.2}%
\contentsline {subsubsection}{\nonumberline Aktionsauswahlprozess}{44}{subsubsection*.60}%
\contentsline {subsubsection}{\nonumberline Lernprozess}{45}{subsubsection*.62}%
\contentsline {subsection}{\numberline {5.3.3}PPO}{46}{subsection.5.3.3}%
\contentsline {subsubsection}{\nonumberline Aktionsauswahlprozess}{47}{subsubsection*.64}%
\contentsline {subsubsection}{\nonumberline Lernprozess}{47}{subsubsection*.66}%
\contentsline {subsection}{\numberline {5.3.4}Vorstellung der zu untersuchenden Agenten}{48}{subsection.5.3.4}%
\contentsline {section}{\numberline {5.4}Optimierungen}{50}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Optimierung A - Joined Reward Function}{50}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Optimierung B - Anpassung der Lernrate}{51}{subsection.5.4.2}%
\contentsline {section}{\numberline {5.5}Datenerhebung und Verarbeitung}{51}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Datenerhebung}{51}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Datenverarbeitung und Erzeugung von Statistiken}{52}{subsection.5.5.2}%
\contentsline {chapter}{\numberline {6}Implementierung}{54}{chapter.6}%
\contentsline {section}{\numberline {6.1}Package Struktur}{54}{section.6.1}%
\contentsline {section}{\numberline {6.2}Snake Environment}{55}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Spiellogik}{55}{subsection.6.2.1}%
\contentsline {subsubsection}{\nonumberline Action}{56}{subsubsection*.68}%
\contentsline {subsubsection}{\nonumberline Observe}{59}{subsubsection*.70}%
\contentsline {subsubsection}{\nonumberline Evaluate}{59}{subsubsection*.72}%
\contentsline {subsubsection}{\nonumberline Reset}{59}{subsubsection*.74}%
\contentsline {subsubsection}{\nonumberline View}{60}{subsubsection*.76}%
\contentsline {subsection}{\numberline {6.2.2}Player}{60}{subsection.6.2.2}%
\contentsline {subsection}{\numberline {6.2.3}Observation}{60}{subsection.6.2.3}%
\contentsline {subsection}{\numberline {6.2.4}Reward}{62}{subsection.6.2.4}%
\contentsline {subsection}{\numberline {6.2.5}GUI}{63}{subsection.6.2.5}%
\contentsline {subsection}{\numberline {6.2.6}Schnittstelle}{65}{subsection.6.2.6}%
\contentsline {section}{\numberline {6.3}Agenten}{66}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}AV-Network}{66}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}DQN}{66}{subsection.6.3.2}%
\contentsline {subsubsection}{\nonumberline Q-Network}{66}{subsubsection*.78}%
\contentsline {subsubsection}{\nonumberline Memory}{67}{subsubsection*.80}%
\contentsline {subsubsection}{\nonumberline DQN-Agent}{68}{subsubsection*.82}%
\contentsline {subsection}{\numberline {6.3.3}PPO}{70}{subsection.6.3.3}%
\contentsline {subsubsection}{\nonumberline Actor}{70}{subsubsection*.84}%
\contentsline {subsubsection}{\nonumberline Critic}{71}{subsubsection*.86}%
\contentsline {subsubsection}{\nonumberline ActorCritic}{72}{subsubsection*.88}%
\contentsline {subsubsection}{\nonumberline Memory}{73}{subsubsection*.90}%
\contentsline {subsubsection}{\nonumberline PPO}{74}{subsubsection*.92}%
\contentsline {section}{\numberline {6.4}Main Methoden}{77}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Train Methode}{77}{subsection.6.4.1}%
\contentsline {subsection}{\numberline {6.4.2}Test Methoden}{80}{subsection.6.4.2}%
\contentsline {section}{\numberline {6.5}Speicherung}{80}{section.6.5}%
\contentsline {section}{\numberline {6.6}Statistik}{81}{section.6.6}%
\contentsline {chapter}{\numberline {7}Evaluation}{85}{chapter.7}%
\contentsline {chapter}{Literaturverzeichnis}{86}{section*.93}%
\contentsline {chapter}{\numberline {A}Anhang}{88}{appendix.A}%
\contentsline {section}{\numberline {A.1}Backpropagation und das Gradientenverfahren}{88}{section.A.1}%
\contentsline {section}{\numberline {A.2}Tensoren}{89}{section.A.2}%
\contentsline {section}{\numberline {A.3}Convolution Neural Networks}{89}{section.A.3}%
\contentsline {subsection}{\numberline {A.3.1}Convolutional Layer}{89}{subsection.A.3.1}%
\contentsline {subsection}{\numberline {A.3.2}Pooling Layer}{91}{subsection.A.3.2}%
\contentsline {subsection}{\numberline {A.3.3}Fully Connected Layer}{91}{subsection.A.3.3}%
\contentsline {chapter}{\numberline {B}Anhang zur Implementierung}{92}{appendix.B}%
\contentsline {section}{\numberline {B.1}Around-View}{92}{section.B.1}%
\contentsline {section}{\numberline {B.2}Distanzbestimmung}{93}{section.B.2}%
\contentsline {section}{\numberline {B.3}AV-Network}{94}{section.B.3}%
\contentsline {section}{\numberline {B.4}DQN-Memory}{95}{section.B.4}%
\contentsline {chapter}{\numberline {C}Anleitung}{96}{appendix.C}%
\contentsline {section}{\numberline {C.1}PPO Train Startargumente}{96}{section.C.1}%
\contentsline {section}{\numberline {C.2}DQN Train Startargumente}{97}{section.C.2}%
\contentsline {section}{\numberline {C.3}Test Startargumente}{98}{section.C.3}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
