\babel@toc {ngerman}{}\relax 
\contentsline {chapter}{\nonumberline Abbildungsverzeichnis}{v}{chapter*.2}%
\contentsline {chapter}{\nonumberline Tabellenverzeichnis}{vi}{chapter*.3}%
\contentsline {chapter}{\nonumberline Abkürzungsverzeichnis}{vii}{chapter*.4}%
\contentsline {chapter}{\numberline {1}Einleitung}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Zielsetzung}{2}{section.1.2}%
\contentsline {chapter}{\numberline {2}Grundlagen}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Game of Snake}{3}{section.2.1}%
\contentsline {section}{\numberline {2.2}Reinforcement Learning}{4}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Vokabular}{4}{subsection.2.2.1}%
\contentsline {subsubsection}{\nonumberline Agent}{5}{subsubsection*.6}%
\contentsline {subsubsection}{\nonumberline Environment}{5}{subsubsection*.8}%
\contentsline {subsubsection}{\nonumberline Action}{5}{subsubsection*.10}%
\contentsline {subsubsection}{\nonumberline Observation}{5}{subsubsection*.12}%
\contentsline {subsubsection}{\nonumberline Reward}{6}{subsubsection*.14}%
\contentsline {subsubsection}{\nonumberline State}{6}{subsubsection*.16}%
\contentsline {subsubsection}{\nonumberline Policy}{6}{subsubsection*.18}%
\contentsline {subsubsection}{\nonumberline Value}{6}{subsubsection*.20}%
\contentsline {subsection}{\numberline {2.2.2}Funktionsweise}{7}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Arten von RL Verfahren}{8}{subsection.2.2.3}%
\contentsline {subsubsection}{\nonumberline Policy-Based und Value-Based Verfahren}{8}{subsubsection*.22}%
\contentsline {subsubsection}{\nonumberline On-Policy und Off-Policy Verfahren}{8}{subsubsection*.24}%
\contentsline {section}{\numberline {2.3}Proximal Policy Optimization}{9}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Actor Critic Modell}{9}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}PPO Formelelemente}{10}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}PPO Fehlerfunktion}{11}{subsection.2.3.3}%
\contentsline {subsubsection}{\nonumberline Return}{11}{subsubsection*.26}%
\contentsline {subsubsection}{\nonumberline Baseline Estimate}{12}{subsubsection*.28}%
\contentsline {subsubsection}{\nonumberline Advantage}{12}{subsubsection*.30}%
\contentsline {subsubsection}{\nonumberline Probability Ratio}{12}{subsubsection*.32}%
\contentsline {subsubsection}{\nonumberline Surrogate Fehlerfunktion}{13}{subsubsection*.34}%
\contentsline {subsubsection}{\nonumberline Zusammenfassung der Actor Fehlerfunktion}{14}{subsubsection*.36}%
\contentsline {subsubsection}{\nonumberline Zusammenfassung der PPO Fehlerfunktion}{14}{subsubsection*.38}%
\contentsline {subsection}{\numberline {2.3.4}PPO - Algorithmus}{15}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Deep Q-Network}{16}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}DQN -- Algorithmus}{17}{subsection.2.4.1}%
\contentsline {chapter}{\numberline {3}Anforderungen}{19}{chapter.3}%
\contentsline {section}{\numberline {3.1}Anforderungen an das Environment}{19}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Standardisierte Schnittstelle}{19}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Funktionalitäten}{19}{subsection.3.1.2}%
\contentsline {subsubsection}{\nonumberline Aktionsausführung}{20}{subsubsection*.40}%
\contentsline {subsubsection}{\nonumberline Reset}{20}{subsubsection*.42}%
\contentsline {subsubsection}{\nonumberline Render}{20}{subsubsection*.44}%
\contentsline {section}{\numberline {3.2}Anforderungen an die Agenten}{20}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Funktionalitäten}{20}{subsection.3.2.1}%
\contentsline {subsubsection}{\nonumberline Aktionsbestimmung}{20}{subsubsection*.46}%
\contentsline {subsubsection}{\nonumberline Lernen}{20}{subsubsection*.48}%
\contentsline {subsubsection}{\nonumberline Parametrisierung}{21}{subsubsection*.50}%
\contentsline {subsection}{\numberline {3.2.2}Diversität der RL Algorithmen}{21}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Anforderungen an die Datenerhebung}{21}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Mehrfache Datenerhebung}{21}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Datenspeicherung}{21}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Anforderungen an die Evaluation}{22}{section.3.4}%
\contentsline {chapter}{\numberline {4}Verwandte Arbeiten}{24}{chapter.4}%
\contentsline {section}{\numberline {4.1}Autonomous Agents in Snake Game via Deep Reinforcement Learning}{24}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Diskussion}{26}{subsection.4.1.1}%
\contentsline {section}{\numberline {4.2}UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene}{27}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Diskussion}{28}{subsection.4.2.1}%
\contentsline {section}{\numberline {4.3}Zusammenfassung}{29}{section.4.3}%
\contentsline {chapter}{\numberline {5}Konzept}{30}{chapter.5}%
\contentsline {section}{\numberline {5.1}Vorgehen}{30}{section.5.1}%
\contentsline {section}{\numberline {5.2}Environment}{32}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Spiellogik}{32}{subsection.5.2.1}%
\contentsline {subsubsection}{\nonumberline Spielablauf}{33}{subsubsection*.52}%
\contentsline {subsubsection}{\nonumberline Reward}{34}{subsubsection*.54}%
\contentsline {subsubsection}{\nonumberline Observation}{35}{subsubsection*.56}%
\contentsline {subsubsection}{\nonumberline GUI}{37}{subsubsection*.58}%
\contentsline {subsection}{\numberline {5.2.2}Schnittstelle}{37}{subsection.5.2.2}%
\contentsline {section}{\numberline {5.3}Agenten}{38}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Netzstruktur}{38}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}DQN}{39}{subsection.5.3.2}%
\contentsline {subsubsection}{\nonumberline Aktionsauswahlprozess}{40}{subsubsection*.60}%
\contentsline {subsubsection}{\nonumberline Lernprozess}{41}{subsubsection*.62}%
\contentsline {subsection}{\numberline {5.3.3}PPO}{42}{subsection.5.3.3}%
\contentsline {subsubsection}{\nonumberline Aktionsauswahlprozess}{42}{subsubsection*.64}%
\contentsline {subsubsection}{\nonumberline Lernprozess}{42}{subsubsection*.66}%
\contentsline {subsection}{\numberline {5.3.4}Vorstellung der zu untersuchenden Agenten}{43}{subsection.5.3.4}%
\contentsline {section}{\numberline {5.4}Optimierungen}{45}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Optimierung A -- Joined Reward Function}{45}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Optimierung B -- Anpassung der Lernrate}{46}{subsection.5.4.2}%
\contentsline {section}{\numberline {5.5}Datenerhebung und Verarbeitung}{46}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Datenerhebung}{47}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Datenverarbeitung und Erzeugung von Statistiken}{48}{subsection.5.5.2}%
\contentsline {chapter}{\numberline {6}Implementierung}{49}{chapter.6}%
\contentsline {section}{\numberline {6.1}Snake Environment}{49}{section.6.1}%
\contentsline {section}{\numberline {6.2}AV-Network}{51}{section.6.2}%
\contentsline {section}{\numberline {6.3}DQN}{51}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Q-Network}{52}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Memory}{52}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Agent}{52}{subsection.6.3.3}%
\contentsline {subsubsection}{\nonumberline Aktionsbestimmung}{52}{subsubsection*.68}%
\contentsline {subsubsection}{\nonumberline Trainingsroutine}{52}{subsubsection*.70}%
\contentsline {section}{\numberline {6.4}PPO}{53}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Actor und Critic}{54}{subsection.6.4.1}%
\contentsline {subsection}{\numberline {6.4.2}ActorCritic}{54}{subsection.6.4.2}%
\contentsline {subsubsection}{\nonumberline Aktionsbestimmung}{54}{subsubsection*.72}%
\contentsline {subsection}{\numberline {6.4.3}Memory}{54}{subsection.6.4.3}%
\contentsline {subsection}{\numberline {6.4.4}Agent}{55}{subsection.6.4.4}%
\contentsline {subsubsection}{\nonumberline Trainingsroutine}{55}{subsubsection*.74}%
\contentsline {section}{\numberline {6.5}Train Methoden}{56}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Test Methoden}{57}{subsection.6.5.1}%
\contentsline {section}{\numberline {6.6}Statistik}{58}{section.6.6}%
\contentsline {chapter}{\numberline {7}Evaluation}{59}{chapter.7}%
\contentsline {section}{\numberline {7.1}Ergebnisevaluation der Vergleiche}{59}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Evaluation der Baseline Vergleiche}{59}{subsection.7.1.1}%
\contentsline {subsubsection}{\nonumberline Performance}{59}{subsubsection*.76}%
\contentsline {subsubsection}{\nonumberline Siegrate}{61}{subsubsection*.78}%
\contentsline {subsubsection}{\nonumberline Robustheit}{62}{subsubsection*.80}%
\contentsline {subsubsection}{\nonumberline Effizienz}{63}{subsubsection*.82}%
\contentsline {subsection}{\numberline {7.1.2}Evaluation der Optimized Vergleiche}{64}{subsection.7.1.2}%
\contentsline {subsubsection}{\nonumberline Performance}{64}{subsubsection*.84}%
\contentsline {subsubsection}{\nonumberline Siegrate}{66}{subsubsection*.86}%
\contentsline {subsubsection}{\nonumberline Robustheit}{67}{subsubsection*.88}%
\contentsline {subsubsection}{\nonumberline Effizienz}{68}{subsubsection*.90}%
\contentsline {subsection}{\numberline {7.1.3}Bestimmung des optimalen Agenten}{69}{subsection.7.1.3}%
\contentsline {section}{\numberline {7.2}Anforderungsevaluation}{69}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Anforderungsevaluation der Environment}{69}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Anforderungsevaluation der Agenten}{70}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Anforderungsevaluation an die Datenerhebung}{70}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}Anforderungen an die Evaluation}{71}{subsection.7.2.4}%
\contentsline {chapter}{\numberline {8}Fazit}{72}{chapter.8}%
\contentsline {section}{\numberline {8.1}Beantwortung der Forschungsfrage}{72}{section.8.1}%
\contentsline {section}{\numberline {8.2}Ausblick}{73}{section.8.2}%
\contentsline {chapter}{Literaturverzeichnis}{75}{section*.91}%
\contentsline {chapter}{\numberline {A}Anhang}{78}{appendix.A}%
\contentsline {section}{\numberline {A.1}Implementierung}{78}{section.A.1}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
