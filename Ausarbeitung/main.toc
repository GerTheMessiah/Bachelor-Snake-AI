\babel@toc {ngerman}{}
\contentsline {chapter}{\nonumberline Abbildungsverzeichnis}{iv}{chapter*.2}%
\contentsline {chapter}{\nonumberline Tabellenverzeichnis}{v}{chapter*.3}%
\contentsline {chapter}{\nonumberline Abkürzungsverzeichnis}{vi}{chapter*.4}%
\contentsline {chapter}{\numberline {1}Einleitung}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}%
\contentsline {chapter}{\numberline {2}Grundlagen}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Game of Snake}{3}{section.2.1}%
\contentsline {section}{\numberline {2.2}Reinforcement Learning}{4}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Klassifikation von RL-Verfahren}{5}{subsection.2.2.1}%
\contentsline {subsubsection}{\nonumberline Model-free und Model-based}{5}{subsubsection*.6}%
\contentsline {subsubsection}{\nonumberline Policy-based und value-based Verfahren}{5}{subsubsection*.8}%
\contentsline {subsubsection}{\nonumberline On-Policy und Off-Policy Verfahren}{6}{subsubsection*.10}%
\contentsline {subsection}{\numberline {2.2.2}Vokabular}{6}{subsection.2.2.2}%
\contentsline {subsubsection}{\nonumberline Agent}{6}{subsubsection*.12}%
\contentsline {subsubsection}{\nonumberline Environment}{7}{subsubsection*.14}%
\contentsline {subsubsection}{\nonumberline Action}{7}{subsubsection*.16}%
\contentsline {subsubsection}{\nonumberline Observation}{8}{subsubsection*.18}%
\contentsline {subsubsection}{\nonumberline Reward}{8}{subsubsection*.20}%
\contentsline {subsubsection}{\nonumberline State}{8}{subsubsection*.22}%
\contentsline {subsection}{\numberline {2.2.3}Funktionsweise}{9}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Proximal Policy Optimization}{9}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Actor-Critic Modell}{10}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}PPO Training Objective Function}{10}{subsection.2.3.2}%
\contentsline {subsubsection}{\nonumberline Formelelemente}{11}{subsubsection*.24}%
\contentsline {subsubsection}{\nonumberline Advantage}{12}{subsubsection*.26}%
\contentsline {subsubsection}{\nonumberline Return}{12}{subsubsection*.28}%
\contentsline {subsubsection}{\nonumberline Baseline Estimate}{12}{subsubsection*.30}%
\contentsline {subsubsection}{\nonumberline Probability Ratio}{13}{subsubsection*.32}%
\contentsline {subsubsection}{\nonumberline Surrogate Objectives}{13}{subsubsection*.34}%
\contentsline {subsubsection}{\nonumberline Zusammenfassung der PPO Training Objective Function}{14}{subsubsection*.36}%
\contentsline {section}{\numberline {2.4}Deep Q-Learning}{14}{section.2.4}%
\contentsline {section}{\numberline {2.5}Backpropagation}{16}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Verfahren}{16}{subsection.2.5.1}%
\contentsline {subsubsection}{\nonumberline Berechnung der Ausgabe des NN}{16}{subsubsection*.38}%
\contentsline {subsubsection}{\nonumberline Bestimmung der Fehler}{16}{subsubsection*.40}%
\contentsline {subsubsection}{\nonumberline Berechnen der Änderungen des NN-Gewichte}{17}{subsubsection*.42}%
\contentsline {chapter}{\numberline {3}Verwandte Arbeiten}{19}{chapter.3}%
\contentsline {section}{\numberline {3.1}Autonomous Agents in Snake Game via Deep Reinforcement Learning}{19}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Vorstellung}{19}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Diskussion}{20}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Deep Reinforcement Learning for Snake}{21}{section.3.2}%
\contentsline {section}{\numberline {3.3}UAV Autonomous Target Search Based onDeep Reinforcement Learning in Complex Disaster Scene}{22}{section.3.3}%
\contentsline {chapter}{\numberline {4}Anforderungen}{23}{chapter.4}%
\contentsline {section}{\numberline {4.1}Anforderungen an das Environment}{23}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Abtrennung}{23}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Kommunikation}{24}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Funktionalität}{24}{subsection.4.1.3}%
\contentsline {subsection}{\numberline {4.1.4}Visualisierung}{24}{subsection.4.1.4}%
\contentsline {subsection}{\numberline {4.1.5}Effizienz}{24}{subsection.4.1.5}%
\contentsline {subsection}{\numberline {4.1.6}Test}{25}{subsection.4.1.6}%
\contentsline {section}{\numberline {4.2}Anforderungen an die Agenten}{25}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Abtrennung}{25}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Funktionalität}{25}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Effizienz}{25}{subsection.4.2.3}%
\contentsline {subsection}{\numberline {4.2.4}Einzigartigkeit}{26}{subsection.4.2.4}%
\contentsline {subsection}{\numberline {4.2.5}Test}{26}{subsection.4.2.5}%
\contentsline {section}{\numberline {4.3}Anforderungen an die Datenerhebung}{26}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Mehrfache Datenerhebung}{26}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Datenspeicherung}{26}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Variation der Datenerhebungsparameter}{27}{subsection.4.3.3}%
\contentsline {subsubsection}{\nonumberline Variation der Reward-Funktion}{28}{subsubsection*.44}%
\contentsline {subsubsection}{\nonumberline Variation der Netzstruktur}{28}{subsubsection*.46}%
\contentsline {subsubsection}{\nonumberline Variation der Observation}{28}{subsubsection*.48}%
\contentsline {section}{\numberline {4.4}Anforderungen an die Evaluation}{28}{section.4.4}%
\contentsline {chapter}{\numberline {5}Agenten}{30}{chapter.5}%
\contentsline {section}{\numberline {5.1}Agenten}{31}{section.5.1}%
\contentsline {chapter}{Literaturverzeichnis}{33}{section*.49}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
